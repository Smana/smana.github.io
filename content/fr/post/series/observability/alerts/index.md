+++
author = "Smaine Kahlouch"
title = "`VictoriaMetrics` : Des alertes efficaces, de la th√©orie √† la pratique üõ†Ô∏è"
date = "2025-04-21"
summary = "Des `Core Web Vitals` aux `Golden Signals`, en passant par la configuration de notifications Slack, d√©couvrez comment mettre en place des alertes efficaces avec l'op√©rateur VictoriaMetrics."
featured = true
codeMaxLines = 21
usePageBundles = true
toc = true
series = [
  "observability"
]
tags = [
    "observability"
]
thumbnail= "thumbnail.png"
+++

{{% notice info "Mieux inspecter nos applications üëÅÔ∏è" %}}
Une fois que notre application est d√©ploy√©e, il est primordial de disposer d'indicateurs permettant d'identifier d'√©ventuels probl√®mes ainsi que suivre les √©volutions de performance. Parmi ces √©l√©ments, les **m√©triques** et les **logs** jouent un r√¥le essentiel en fournissant des informations pr√©cieuses sur le fonctionnement de l'application. En compl√©ment, il est souvent utile de mettre en place un **tracing** d√©taill√© pour suivre pr√©cis√©ment toutes les actions r√©alis√©es par l'application.

Dans cette [s√©rie d'articles](http://localhost:1313/fr/series/observability/), nous allons explorer les diff√©rents aspects li√©s √† la supervision applicative. L'objectif √©tant d'analyser en d√©tail l'√©tat de nos applications, afin d'am√©liorer leur **disponibilit√©** et leurs **performances**, tout en garantissant une exp√©rience utilisateur optimale.
{{% /notice %}}

Lors d'un [pr√©c√©dent article](https://blog.ogenki.io/fr/post/series/observability/metrics/), nous avons vu comment collecter et visualiser des m√©triques. Celles-ci permettent d'analyser le comportement et les performances de nos applications. Il est tout aussi primordial de configurer des **alertes** afin d'√™tre notifi√© en cas d'anomalies sur notre plateforme.

## üéØ Objectifs

* üìä Comprendre les approches standards pour d√©finir des alertes efficaces : Les "**Core Web Vitals**" et les "**Golden Signals**"
* üîç D√©couvrir les langages **PromQL** et **MetricsQL** pour l'√©criture de r√®gles d'alerte
* ‚öôÔ∏è Configurer des alertes de fa√ßon d√©clarative avec **VictoriaMetrics Operator**
* üì± **Router ces alertes** vers diff√©rents canaux Slack

## üìã Pr√©requis

La suite de cet article suppose que vous avez d√©j√† :

* Une instance VictoriaMetrics fonctionnelle d√©ploy√©e sur Kubernetes
* Un acc√®s √† un workspace Slack pour les notifications

La mise en place d'alertes pertinentes est un √©l√©ment crucial de toute strat√©gie d'observabilit√©. Cependant, d√©finir des seuils appropri√©s et √©viter la fatigue li√©e aux alertes n√©cessite une approche r√©fl√©chie et m√©thodique.


Nous allons voir dans cet article qu'il est tr√®s simple de positionner des seuils au del√† desquels nous serions notifi√©s. Cependant faire en sorte que ces alertes soient **pertinentes** n'est pas toujours √©vident.

## üîç Qu'est ce qu'une bonne alerte?

<center><img src="chasseurs-chasseur.gif" width=300 alt=""></center>

Une alerte correctement configur√©e permet d'identifier et de r√©soudre les probl√®mes au sein de notre syst√®me de mani√®re **proactive**, avant qu'ils ne s'aggravent. Des alertes efficaces doivent:

- Signaler des probl√®mes n√©cessitant une **intervention imm√©diate**.
- Etre d√©clench√©es **au bon moment**: suffisamment t√¥t pour pr√©venir un impact sur les utilisateurs, sans toutefois √™tre trop fr√©quentes au point de provoquer une fatigue li√©e aux alertes.
- Indiquer la **cause racine** ou la zone n√©cessitant une investigation. Pour ce faire il est recommand√© d'effectuer une analyse permettant la priorisation d'indicateurs pertinents, avec un r√©el impact sur le m√©tier ([SLIs](https://sre.google/sre-book/service-level-objectives/)).

Il est donc important de se focaliser sur un **nombre maitris√©** d'indicateurs √† surveiller. Il existe pour cela des approches qui permettent de mettre en oeuvre une supervision efficace de nos syst√®mes.
Ici nous allons nous pencher sur 2 mod√®les d'alerte reconnus: Les **Core Web Vitals** et les **Golden Signals**.

### üåê Les "Core Web Vitals"

Les Core Web Vitals sont des m√©triques d√©velopp√©es par Google pour √©valuer l'**exp√©rience utilisateur** sur les applications web. Ils mettent en √©vidence des indicateurs li√©s √† la satisfaction des utilisateurs finaux et permettent de garantir que notre application offre de bonnes performances pour les utilisateurs r√©els. Ces m√©triques se concentrent sur trois aspects principaux :

<center><img src="core_web_vitals.png" width=800 alt="Core Web Vitals"></center>

- **Largest Contentful Paint** (LCP), *Temps de chargement de la page* : Le LCP mesure le temps n√©cessaire pour que le plus grand √©l√©ment de contenu visible sur une page web (par exemple, une image, une vid√©o ou un large bloc de texte) soit **enti√®rement rendu** dans la fen√™tre d'affichage. Un bon LCP se situe en dessous de **2,5 secondes**.

- **Interaction to Next Paint** (INP), *R√©activit√©* : L'INP √©value la r√©activit√© d'une page web en mesurant la **latence de toutes les interactions** utilisateur, telles que les clics, les taps et les entr√©es clavier, etc... Elle refl√®te le temps n√©cessaire pour qu'une page r√©agisse visuellement √† une interaction, c'est-√†-dire le d√©lai avant que le navigateur affiche le prochain rendu apr√®s une action de l'utilisateur. un bon INP doit √™tre inf√©rieur √† **200 millisecondes**

- **Cumulative Layout Shift** (CLS), *Stabilit√© visuelle* : Le CLS √©value la **stabilit√© visuelle** en quantifiant les d√©calages de mise en page inattendus sur une page, lorsque des √©l√©ments se d√©placent pendant le chargement ou l'interaction. Un bon score CLS est inf√©rieur ou √©gal √† **0,1**.

La performance d'un site web est consid√©r√©e satisfaisante si elle atteint les seuils d√©crits ci-dessus au **75·µâ percentile**, favorisant ainsi une bonne exp√©rience utilisateur et, par cons√©quent, une meilleure r√©tention et un meilleur r√©f√©rencement ([SEO](https://en.wikipedia.org/wiki/Search_engine_optimization)).

{{% notice note "Attention aux alertes sur les Core Web Vitals" %}}
L'ajout d'alertes sp√©cifiques sur ces m√©triques doit √™tre m√ªrement r√©fl√©chi. Contrairement aux indicateurs op√©rationnels classiques, tels que la disponibilit√© ou le taux d'erreurs, qui refl√®tent directement la stabilit√© du syst√®me, les *Web Vitals* d√©pendent de **nombreux facteurs externes**, comme les conditions r√©seau des utilisateurs ou leurs appareils, rendant les seuils plus complexes √† surveiller efficacement.

Pour √©viter une surcharge d'alertes inutiles, ces alertes doivent uniquement cibler des **d√©gradations significatives**. Par exemple, une augmentation soudaine du **CLS** (stabilit√© visuelle) ou une d√©t√©rioration continue du **LCP** (temps de chargement) sur plusieurs jours peuvent indiquer des probl√®mes importants n√©cessitant une intervention.
{{% /notice %}}

Enfin, ces alertes n√©cessitent des outils adapt√©s, comme le *RUM (Real User Monitoring)* pour les donn√©es r√©elles ou le *Synthetic Monitoring* pour des tests simul√©s, qui requi√®rent une solution sp√©cifique non abord√©e dans cet article.

### ‚ú® Les "Golden Signals"

<center><img src="golden_signals.png" width=300 alt="Golden Signals"></center>

Les _Golden Signals_ sont un ensemble de **quatre indicateurs cl√©s**, largement utilis√©s dans le domaine de la supervision des syst√®mes et des applications, notamment avec des outils comme Prometheus. Ces signaux permettent de surveiller la sant√© et la performance des applications de mani√®re efficace. Ils sont particuli√®rement appropri√©s dans le contexte d'une architecture distribu√©e:

* **La Latence** ‚è≥: Elle inclut √† la fois le temps des requ√™tes r√©ussies et le temps des requ√™tes √©chou√©es. La latence est cruciale car une augmentation du temps de r√©ponse peut indiquer des probl√®mes de performance.

* **Le Trafic** üì∂: Il peut √™tre mesur√©e en termes de nombre de requ√™tes par seconde, de d√©bit de donn√©es, ou d'autres m√©triques qui expriment la charge du syst√®me.

* **Les erreurs** ‚ùå: Il s'agit du taux d'√©chec des requ√™tes ou des transactions. Cela peut inclure des erreurs d'application, des erreurs d'infrastructure ou toute situation o√π une requ√™te ne s'est pas termin√©e correctement (par exemple, des r√©ponses HTTP 5xx ou des requ√™tes rejet√©es).

* **La saturation** üìà: C'est une mesure de l'utilisation des ressources du syst√®me, comme le CPU, la m√©moire ou la bande passante r√©seau. La saturation indique √† quel point le syst√®me est proche de ses limites. Un syst√®me satur√© peut entra√Æner des ralentissements ou des pannes.

Ces Golden Signals sont essentiels car ils permettent de **concentrer la surveillance sur les aspects critiques** qui peuvent rapidement affecter l'exp√©rience utilisateur ou la performance globale du syst√®me. Avec Prometheus, ces signaux sont souvent surveill√©s via des m√©triques sp√©cifiques pour d√©clencher des alertes lorsque certains seuils sont d√©pass√©s.

{{% notice info "D'autres m√©thodes et indicateurs" %}}
J'ai √©voqu√© ici 2 m√©thodologies qui, je trouve, sont un bon point de d√©part pour ajuster au mieux notre syst√®me d'alerting. Ceci-dit il en existe d'autres, chacune avec leurs sp√©cificit√©s. On peut ainsi citer [USE](https://www.brendangregg.com/usemethod.html) ou [RED](https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/).

De m√™me, au-del√† des Core Web Vitals pr√©sent√©s plus haut, d'autres m√©triques web comme **[FCP](https://web.dev/articles/fcp)** (First Contentful Paint) ou **[TTFB](https://web.dev/articles/ttfb)** (Time To First Byte) peuvent s'av√©rer utiles selon vos besoins sp√©cifiques.

Le choix des m√©triques √† surveiller d√©pendra de votre contexte et de vos objectifs. L'essentiel est de garder √† l'esprit qu'une bonne strat√©gie d'alerting repose sur un ensemble cibl√© d'indicateurs pertinents üéØ
{{% /notice %}}

Vous l'aurez compris: Definir des alertes √ßa se r√©fl√©chit! Maintenant entrons dans le concret et voyons **comment d√©finir des seuils √† partir de nos m√©triques**.

## üíª Exprimer des requ√™tes avec PromQL/MetricsQL

Les m√©triques collect√©es avec Prometheus peuvent √™tre requet√©es avec un langage sp√©cifique appel√© `PromQL` (Prometheus Query Language). Ce langage permet d'extraire des donn√©es de supervision, d'effectuer des **calculs**, d'**agr√©ger** les r√©sultats, d'appliquer des **filtres**, mais aussi de configurer des **alertes**.

(‚ÑπÔ∏è Se r√©f√©rer au [pr√©c√©dent article](https://blog.ogenki.io/fr/post/series/observability/metrics/#-quest-ce-quune-m%C3%A9trique) pour comprendre ce que l'on entend par m√©trique.)

PromQL est un langage puissant dont voici quelques exemples simples appliqu√©s aux m√©triques expos√©es par un serveur web Nginx :

* Nombre total de requ√™tes trait√©es (`nginx_http_requests_total`) :
  ```promql
  nginx_http_requests_total
  ```

* Nombre moyen de requ√™tes par seconde sur une fen√™tre de 5 minutes.
  ```promql
  rate(nginx_http_requests_total[5m])
  ```

* Nombre de requ√™tes HTTP par seconde retournant un code d'erreur 5xx sur les 5 derni√®res minutes.
  ```promql
  rate(nginx_http_requests_total{status=~"5.."}[5m])
  ```

* Nombre de requ√™tes par seconde, agr√©g√© par pod et filtr√© sur le namespace "myns" sur les 5 derni√®res minutes.
  ```promql
  sum(rate(nginx_http_requests_total{namespace="myns"}[5m])) by (pod)
  ```

üí° Dans les exemples ci-dessus, nous avons mis en √©vidence deux _Golden Signals_ : le trafic üì∂ et les erreurs ‚ùå.

`MetricsQL` est le langage utilis√© avec VictoriaMetrics. Il se veut compatible avec PromQL avec de l√©g√®res diff√©rences qui permettent de faciliter l'√©criture de requ√™tes complexes.</br>
Il apporte aussi de nouvelles fonctions dont voici quelques exemples:

* `histogram(q)`: Cette fonction calcule un histogramme pour chaque groupe de points ayant le m√™me horodatage, ce qui est utile pour visualiser un grand nombre de s√©ries temporelles (timeseries) via un heatmap. </br>
  Pour cr√©er un histogramme des requ√™tes HTTP
  ```promql
  histogram(rate(vm_http_requests_total[5m]))
  ```

* `quantiles("phiLabel", phi1, ..., phiN, q)`: Utilis√© pour extraire plusieurs quantiles (ou percentiles) d'une m√©trique donn√©e . </br>
  Pour calculer les 50e, 90e et 99e percentiles du taux de requ√™tes HTTP
  ```promql
  quantiles("percentile", 0.5, 0.9, 0.99, rate(vm_http_requests_total[5m]))
  ```

Afin de pouvoir tester ses requ√™tes, vous pouvez utiliser la d√©mo fournie par VictoriaMetrics: https://play.victoriametrics.com

<center><img src="vmplay.png" width=700 alt=""></center>

## üõ†Ô∏è Configurer des alertes avec VictoriaMetrics Operator

VictoriaMetrics propose deux composants essentiels pour la gestion des alertes :
- **VMAlert** : responsable de l'√©valuation des r√®gles d'alerte
- **AlertManager** : g√®re le routage et la distribution des notifications

### VMAlert : Le moteur d'√©valuation des r√®gles

VMAlert est le composant qui √©value en continu les r√®gles d'alerte d√©finies. Il supporte deux types de r√®gles :

* **Recording Rules** üìä
   Les recording rules permettent de pr√©-calculer des expressions PromQL complexes et de les stocker comme nouvelles m√©triques pour optimiser les performances.

* **Alerting Rules** üö®
   Les alerting rules d√©finissent les conditions qui d√©clenchent des alertes lorsque certains seuils sont d√©pass√©s.

Dans la suite de cet article, nous allons nous concentrer sur les alerting rules qui sont essentielles pour la d√©tection proactive des probl√®mes.

{{% notice tip "Des exemples concrets" %}}
<table>
  <tr>
        <td>
          <img src="repo_gift.png" style="width:80%;">
        </td>
        <td style="vertical-align:middle; padding-left:10px;" width="70%">

Le reste de cet article est issu d'un ensemble de configurations que vous pouvez retrouver dans le repository <strong><a href="https://github.com/Smana/cloud-native-ref">Cloud Native Ref</a></strong>.</br>
Il y est fait usage de nombreux op√©rateurs et notamment celui pour [VictoriaMetrics](https://github.com/VictoriaMetrics/operator).

L'ambition de ce projet est de pouvoir <strong>d√©marrer rapidement une plateforme compl√®te</strong> qui applique les bonnes pratiques en terme d'automatisation, de supervision, de s√©curit√© etc. </br>
Les commentaires et contributions sont les bienvenues üôè
        </td>
  </tr>
</table>
{{% /notice %}}


### D√©clarer une r√®gle d'alerting avec `VMRule`

Nous avons vu pr√©c√©demment que VictoriaMetrics fournit un op√©rateur Kubernetes qui permet de g√©rer les diff√©rents composants de mani√®re d√©clarative. Parmi les ressources personnalis√©es (Custom Resources) disponibles, la `VMRule` permet de d√©finir des r√®gles d'alertes et d'enregistrement (recording rules).

Si vous avez d√©j√† utilis√© l'[op√©rateur Prometheus](https://github.com/prometheus-operator/prometheus-operator), vous retrouverez une syntaxe tr√®s similaire car l'op√©rateur VictoriaMetrics est compatible avec les custom resources de Prometheus. (Ce qui nous permet de faciliter la migration üòâ).

Prenons un exemple concret avec une `VMRule` qui surveille l'√©tat de sant√© des ressources Flux :

[flux/observability/vmrule.yaml](https://github.com/Smana/cloud-native-ref/blob/main/flux/observability/vmrule.yaml)

```yaml
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMRule
metadata:
  labels:
    prometheus-instance: main
  name: flux-system
  namespace: flux-system
spec:
  groups:
    - name: flux-system
      rules:
        - alert: FluxReconciliationFailure
          annotations:
            message: Flux resource has been unhealthy for more than 5m
            description: "{{ $labels.kind }} {{ $labels.exported_namespace }}/{{ $labels.name }} reconciliation has been failing for more than ten minutes."
            runbook_url: "https://fluxcd.io/flux/cheatsheets/troubleshooting/"
            dashboard: "https://grafana.priv.${domain_name}/dashboards"
          expr: max(gotk_reconcile_condition{status="False",type="Ready"}) by (exported_namespace, name, kind) + on(exported_namespace, name, kind) (max(gotk_reconcile_condition{status="Deleted"}) by (exported_namespace, name, kind)) * 2 == 1
          for: 10m
          labels:
            severity: warning
```

Il est recommand√© de suivre quelques **bonnes pratiques** pour donner le maximum de contexte afin d'identifier rapidement la cause racine.

1. **Nommage et Organisation** üìù
   - Utiliser des noms descriptifs pour les r√®gles, comme `FluxReconciliationFailure`
   - Grouper les r√®gles par composant Flux (ex: `flux-system`, `flux-controllers`)
   - Documenter les conditions de r√©conciliation dans les annotations

2. **Seuils et Dur√©es** ‚è±Ô∏è
   - Ajuster la dur√©e d'√©valuation de l'alerte `for: 10m` pour √©viter les faux positifs
   - Adapter les seuils selon le type de ressource supervis√©es.
   - Consid√©rer des dur√©es diff√©rentes selon l'environnement (prod/staging)

3. **Labels et Routing** üè∑Ô∏è
   - Ajouter des labels pour le routage selon le contexte. Mon exemple n'est pas tr√®s pouss√© car il s'agit d'une configuration de d√©mo. Mais nous pourrions tr√®s bien ajouter un label `team` afin de router vers la bonne √©quipe, ou avoir une politique de routage diff√©rente selon l'environnement.
     ```yaml
     labels:
       severity: [critical|warning|info]
       team: [sre|dev|ops]
       environment: [prod|staging|dev]
     ```

4. **L'importance des annotations** üìö

  Les annotations permettent d'ajouter divers informations sur le contexte de l'alerte
  - Une **description** claire du probl√®me de r√©conciliation
  - Le lien vers le **runbook** de troubleshooting Flux
  - Le lien vers le **dashboard Grafana** d√©di√©

5. **Requ√™te PromQL** üîç
   ```yaml
   expr: |
     max(gotk_reconcile_condition{status="False",type="Ready"}) by (exported_namespace, name, kind)
     + on(exported_namespace, name, kind)
     (max(gotk_reconcile_condition{status="Deleted"}) by (exported_namespace, name, kind)) * 2 == 1
   ```
   Cette alerte se d√©clenchera si Flux n'arrive pas √† r√©concilier une ressource ou si une ressource est supprim√©e alors qu'elle ne devrait pas l'√™tre. Dans le d√©tail:
   - La m√©trique `gotk_reconcile_condition` expose l'√©tat de sant√© des ressources Flux
   - Le filtre `status="False",type="Ready"` identifie les ressources qui ne sont pas dans l'√©tat "Ready"
   - La deuxi√®me partie de l'expression (`status="Deleted"`) d√©tecte les ressources qui ont √©t√© supprim√©es
   - L'op√©ration `+ on(...) (...) * 2 == 1` combine ces conditions pour d√©clencher une alerte quand :
     - Une ressource n'est pas "Ready" (premi√®re partie = 1)
     - OU une ressource a √©t√© supprim√©e de fa√ßon inattendue (deuxi√®me partie = 1)
   - Le `max` et le `by` permettent de regrouper les alertes par namespace, nom et type de ressource

## üí¨ Int√©gration avec Slack

Nous pouvons envoyer ces alertes au travers de diff√©rents canaux ou outils. Nous pouvons citer Grafana OnCall, Opsg√©nie, Pagerduty ou simplement des emails et j'en passe...

Dans notre exemple nous envoyons des notifications vers un canal Slack. Nous allons donc d'abord cr√©er une application Slack et r√©cup√©rer le token g√©n√©r√© avant de configurer VictoriaMetrics.

### Configuration de l'Application Slack

1. **Cr√©ation de l'Application** üîß
   - Cela se fait sur [https://api.slack.com/apps](https://api.slack.com/apps)
   - Cliquer sur "Create New App"
   - Choisir "From scratch"
   - Nommer l'application (ex: "AlertManager")
   - S√©lectionner le workspace cible

2. **Configuration des Permissions** üîë
   Dans "OAuth & Permissions", ajouter les scopes suivants :
   - `chat:write` (Requis)
   - `chat:write.public` (Pour poster dans les canaux publics)
   - `channels:read` (Pour lister les canaux)
   - `groups:read` (Pour les groupes priv√©s)

<center>
  <video id="SlackPermissions" controls width="700" autoplay loop muted>
    <source src="slack-permissions.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
</center>

3. **Installation et Token** üéüÔ∏è
   - Installer l'application dans le workspace
   - Copier le "Bot User OAuth Token" (commence par `xoxb-`)
   - Stocker le token de mani√®re s√©curis√©e. Dans notre exemple, le secret est r√©cup√©r√© depuis AWS¬†Secrets Manager en utilisant l'[op√©rateur External Secrets](https://external-secrets.io).

### Configuration d'AlertManager pour Slack

Le reste de la configuration se fait gr√¢ce √† des values **Helm** afin de param√®trer AlertManager

[observability/base/victoria-metrics-k8s-stack/vm-common-helm-values-configmap.yaml](https://github.com/Smana/cloud-native-ref/blob/main/observability/base/victoria-metrics-k8s-stack/vm-common-helm-values-configmap.yaml)

1. R√©f√©rencer le point de montage du secret contenant le token

```yaml
    alertmanager:
      enabled: true
      spec:
        externalURL: "https://vmalertmanager-${cluster_name}.priv.${domain_name}"
        secrets:
          - "victoria-metrics-k8s-stack-alertmanager-slack-app"
      config:
        global:
          slack_api_url: "https://slack.com/api/chat.postMessage"
          http_config:
            authorization:
              credentials_file: /etc/vm/secrets/victoria-metrics-k8s-stack-alertmanager-slack-app/token
```

Le secret `victoria-metrics-k8s-stack-alertmanager-slack-app` contenant le token est r√©cup√©r√© depuis AWS Secrets Manager. Dans la configuration il faut r√©f√©rencer le point de montage de ce secret (`config.globl.http_config.authorization`)

2. Explication du routage

```yaml
        route:
          group_by:
            - cluster
            - alertname
            - severity
            - namespace
          group_interval: 5m
          group_wait: 30s
          repeat_interval: 3h
          receiver: "slack-monitoring"
          routes:
            - matchers:
                - alertname =~ "InfoInhibitor|Watchdog|KubeCPUOvercommit"
              receiver: "blackhole"
        receivers:
          - name: "blackhole"
          - name: "slack-monitoring"
```

* **Groupement des alertes** : Le groupement des alertes est essentiel pour r√©duire le bruit et am√©liorer la lisibilit√© des notifications. Sans groupement, chaque alerte serait envoy√©e individuellement, ce qui pourrait rapidement devenir ing√©rable. Les crit√®res de groupement choisis permettent une organisation logique:
  * `group_by` d√©fini les labels sur lesquels grouper les alertes.
  * `group_wait`: D√©lai de 30s avant l'envoi initial d'une notification pour permettre le groupement
  * `group_interval`: Intervalle de 5m entre les notifications pour un m√™me groupe
  * `repeat_interval`: Les alertes ne sont r√©p√©t√©es que toutes les 3h pour √©viter le spam

* **Receivers**: Les receivers sont des composants d'AlertManager qui d√©finissent comment et o√π envoyer les notifications d'alerte. Ils peuvent √™tre configur√©s pour diff√©rents canaux de communication comme Slack, Email, PagerDuty, etc. Dans notre configuration:
  * `slack-monitoring`: Receiver principal qui envoie les alertes vers un canal Slack sp√©cifique avec un formatage personnalis√©
  * `blackhole`: Receiver sp√©cial qui "absorbe" les alertes sans les transmettre nulle part, utile pour filtrer les alertes non pertinentes ou purement techniques

{{% notice tip "Exemple de routage" %}}
Selon l'organisation et les proc√©dures en vigueur dans l'entreprise, nous pouvons d√©finir un routage cibl√© des alertes.
Supposons, par exemple, que nous souhaitons router les alertes critiques des environnements de production et s√©curit√© vers l'√©quipe d'astreinte :

```yaml
        - matchers:
            - environment =~ "prod|security"
            - team = "oncall"
          receiver: "pagerduty"
```
{{% /notice %}}

3. **Templates Personnalis√©s** üìù

Ce bloc de configuration d√©finit un receiver Slack pour AlertManager qui utilise les templates Monzo. [Les templates Monzo](https://gist.github.com/milesbxf/e2744fc90e9c41b47aa47925f8ff6512) sont un ensemble de templates de notification qui permettent de formater les alertes Slack de mani√®re √©l√©gante et informative.

```yaml
    alertmanager:
      config:
        receivers:
          - name: "slack-monitoring"
            slack_configs:
              - channel: "#alerts"
                send_resolved: true
                title: '{{ template "slack.monzo.title" . }}'
                icon_emoji: '{{ template "slack.monzo.icon_emoji" . }}'
                color: '{{ template "slack.monzo.color" . }}'
                text: '{{ template "slack.monzo.text" . }}'
                actions:
                  - type: button
                    text: "Runbook :green_book:"
                    url: "{{ (index .Alerts 0).Annotations.runbook_url }}"
                  - type: button
                    text: "Query :mag:"
                    url: "{{ (index .Alerts 0).GeneratorURL }}"
                  - type: button
                    text: "Dashboard :grafana:"
                    url: "{{ (index .Alerts 0).Annotations.dashboard }}"
                  - type: button
                    text: "Silence :no_bell:"
                    url: '{{ template "__alert_silence_link" . }}'
                  - type: button
                    text: '{{ template "slack.monzo.link_button_text" . }}'
                    url: "{{ .CommonAnnotations.link_url }}"
```
Voici un exemple de notification g√©n√©r√©e avec ce format. Il permet notamment d'ajouter des boutons d'action pour visualiser le dashboard Grafana üìä, afficher le runbook üìö ou mettre en silence l'alerte üîï.

<center><img src="alert.png" width=650 alt="Slack alert example"></center>


## üëÄ Visualiser et interagir avec les alertes

La visualisation et la gestion des alertes sont des aspects essentiels d'un syst√®me d'alerting efficace. VictoriaMetrics et son √©cosyst√®me offrent plusieurs options pour interagir avec vos alertes :

### Alertmanager : La solution standard

`Alertmanager` est le composant standard qui permet de :
- Visualiser l'√©tat actuel des alertes
- Configurer le routage des notifications
- G√©rer les silences (mise en pause temporaire d'alertes)
- Consulter l'historique des alertes

<center><img src="alertmanager.png" width=750 alt="Alertmanager"></center>

### VMUI : L'interface native de VictoriaMetrics

`VMUI` offre une interface simplifi√©e pour :
- Consulter les alertes actives
- Visualiser les r√®gles d'alertes
- Afficher les m√©triques associ√©es

<center><img src="vmalert.png" width=800 alt="VMAlert"></center>

### Grafana Alerting : Une solution compl√®te

Bien que nous utilisions Alertmanager pour la d√©finition et le routage des alertes, `Grafana Alerting` offre une solution alternative compl√®te qui permet de :
- Centraliser la gestion des alertes
- Visualiser les alertes dans le contexte des dashboards
- Configurer des r√®gles d'alertes directement depuis l'interface
- G√©rer les silences et les notifications

<center><img src="grafana-alerts.png" width=800 alt="Grafana Alerting"></center>

{{% notice tip "Choisir la bonne interface" %}}
Le choix de l'interface d√©pend de vos besoins sp√©cifiques :
- Alertmanager est id√©al pour la gestion op√©rationnelle des alertes
- VMUI est parfait pour une vue rapide et simple
- Grafana Alerting est recommand√© si vous souhaitez une solution int√©gr√©e avec vos dashboards
{{% /notice %}}

## üéØ Conclusion

La d√©finition d'alertes pertinentes est un √©l√©ment cl√© de toute strat√©gie d'observabilit√©. L'op√©rateur VictoriaMetrics, avec ses ressources personnalis√©es Kubernetes comme `VMRule`, simplifie grandement la mise en place d'un syst√®me d'alerting efficace. La configuration d√©clarative permet de d√©finir rapidement des r√®gles d'alerte complexes tout en maintenant une excellente lisibilit√© et maintenabilit√© du code.

Cependant, la configuration technique des alertes, m√™me avec des outils aussi puissants que VictoriaMetrics, ne suffit pas √† elle seule. Une strat√©gie d'alerting efficace doit s'int√©grer dans un cadre organisationnel plus large :
- D√©finition claire des proc√©dures d'astreinte
- Identification des √©quipes responsables de la surveillance
- Mise en place de runbooks et proc√©dures de r√©ponse aux incidents
- Adaptation des canaux de notification selon la criticit√© et le contexte

{{% notice tip "Pour aller plus loin üöÄ" %}}
D√©couvrez comment int√©grer ces alertes avec d'autres composants de votre stack d'observabilit√© dans les prochains articles de cette s√©rie, notamment la corr√©lation avec les logs et le tracing distribu√©.
{{% /notice %}}

## üîñ References

* https://web.dev/articles/vitals
* https://medium.com/@romanhavronenko/victoriametrics-promql-compliance-d4318203f51e
* https://victoriametrics.com/blog/alerting-recording-rules-alertmanager/
* https://docs.victoriametrics.com/vmalert/
