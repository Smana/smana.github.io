+++
author = "Smaine Kahlouch"
title = "`VictoriaMetrics` : Des alertes efficaces, de la th√©orie √† la pratique üõ†Ô∏è"
date = "2024-10-12"
summary = "Des `Core Web Vitals` aux `Golden Signals`, en passant par la configuration de notifications Slack, d√©couvrez comment mettre en place des alertes efficaces avec VictoriaMetrics."
featured = true
codeMaxLines = 21
usePageBundles = true
toc = true
series = [
  "observability"
]
tags = [
    "observability",
    "monitoring",
    "alerting"
]
thumbnail= "thumbnail.png"
+++

{{% notice info "Mieux inspecter nos applications üëÅÔ∏è" %}}
Une fois que notre application est d√©ploy√©e, il est primordial de disposer d'indicateurs permettant d'identifier d'√©ventuels probl√®mes ainsi que suivre les √©volutions de performance. Parmi ces √©l√©ments, les **m√©triques** et les **logs** jouent un r√¥le essentiel en fournissant des informations pr√©cieuses sur le fonctionnement de l'application. En compl√©ment, il est souvent utile de mettre en place un **tracing** d√©taill√© pour suivre pr√©cis√©ment toutes les actions r√©alis√©es par l'application.

Dans cette [s√©rie d'articles](http://localhost:1313/fr/series/observability/), nous allons explorer les diff√©rents aspects li√©s √† la supervision applicative. L'objectif √©tant d'analyser en d√©tail l'√©tat de nos applications, afin d'am√©liorer leur **disponibilit√©** et leurs **performances**, tout en garantissant une exp√©rience utilisateur optimale.
{{% /notice %}}

Lors d'un [pr√©c√©dent article](https://blog.ogenki.io/fr/post/series/observability/metrics/), nous avons vu comment collecter et visualiser des m√©triques. Celles-ci permettent d'analyser le comportement et les performances de nos applications. Il est tout aussi primordial de configurer des **alertes** afin d'√™tre notifi√© en cas d'anomalies sur notre plateforme.

## üéØ Objectifs

* üìä Comprendre les approches standards pour d√©finir des alertes efficaces : Les "**Core Web Vitals**" et les "**Golden Signals**"
* üîç D√©couvrir les langages **PromQL** et **MetricsQL** pour l'√©criture de r√®gles d'alerte
* ‚öôÔ∏è Configurer des alertes de fa√ßon d√©clarative avec **VictoriaMetrics Operator**
* üì± **Router ces alertes** vers diff√©rents canaux Slack

## üìã Pr√©requis

La suite de cet article suppose que vous avez d√©j√† :

* Une instance VictoriaMetrics fonctionnelle
* Un cluster Kubernetes configur√©
* Un acc√®s √† un workspace Slack pour les notifications
* Les permissions n√©cessaires pour configurer les alertes

La mise en place d'alertes pertinentes est un √©l√©ment crucial de toute strat√©gie d'observabilit√©. Cependant, d√©finir des seuils appropri√©s et √©viter la fatigue li√©e aux alertes n√©cessite une approche r√©fl√©chie et m√©thodique.


Nous allons voir dans cet article qu'il est tr√®s simple de positionner des seuils au del√† desquels nous serions notifi√©s. Cependant faire en sorte que ces alertes soient **pertinentes** n'est pas toujours √©vident.

## üîç Qu'est ce qu'une bonne alerte?

<center><img src="chasseurs-chasseur.gif" width=300 alt=""></center>

Une alerte correctement configur√©e permet d'identifier et de r√©soudre les probl√®mes au sein de notre syst√®me de mani√®re **proactive**, avant qu'ils ne s'aggravent. Des alertes efficaces doivent:

- Signaler des probl√®mes n√©cessitant une **intervention imm√©diate**.
- Etre d√©clench√©es **au bon moment**: suffisamment t√¥t pour pr√©venir un impact sur les utilisateurs, sans toutefois √™tre trop fr√©quentes au point de provoquer une fatigue li√©e aux alertes.
- Indiquer la **cause racine** ou la zone n√©cessitant une investigation. Pour ce faire il est recommand√© d'effectuer une analyse permettant la priorisation d'indicateurs pertinents, avec un r√©el impact sur le m√©tier ([SLIs](https://sre.google/sre-book/service-level-objectives/)).

Il est donc important de se focaliser sur un **nombre maitris√©** d'indicateurs √† surveiller. Il existe pour cela des approches qui permettent de mettre en oeuvre une supervision efficace de nos syst√®mes.
Ici nous allons nous pencher sur 2 mod√®les d'alerte reconnus: Les **Core Web Vitals** et les **Golden Signals**.

### üåê Les "Core Web Vitals"

Les Core Web Vitals sont des m√©triques d√©velopp√©es par Google pour √©valuer l'**exp√©rience utilisateur** sur les applications web. Ils mettent en √©vidence des indicateurs li√©s √† la satisfaction des utilisateurs finaux et permettent de garantir que notre application offre de bonnes performances pour les utilisateurs r√©els. Ces m√©triques se concentrent sur trois aspects principaux :

<center><img src="core_web_vitals.png" width=800 alt="Core Web Vitals"></center>

- **Largest Contentful Paint** (LCP), *Temps de chargement de la page* : Le LCP mesure le temps n√©cessaire pour que le plus grand √©l√©ment de contenu visible sur une page web (par exemple, une image, une vid√©o ou un large bloc de texte) soit **enti√®rement rendu** dans la fen√™tre d'affichage. Un bon LCP se situe en dessous de **2,5 secondes**.

- **Interaction to Next Paint** (INP), *R√©activit√©* : L'INP √©value la r√©activit√© d'une page web en mesurant la **latence de toutes les interactions** utilisateur, telles que les clics, les taps et les entr√©es clavier, etc... Elle refl√®te le temps n√©cessaire pour qu'une page r√©agisse visuellement √† une interaction, c'est-√†-dire le d√©lai avant que le navigateur affiche le prochain rendu apr√®s une action de l'utilisateur. un bon INP doit √™tre inf√©rieur √† **200 millisecondes**

- **Cumulative Layout Shift** (CLS), *Stabilit√© visuelle* : Le CLS √©value la **stabilit√© visuelle** en quantifiant les d√©calages de mise en page inattendus sur une page, lorsque des √©l√©ments se d√©placent pendant le chargement ou l'interaction. Un bon score CLS est inf√©rieur ou √©gal √† **0,1**.

La performance d'un site web est consid√©r√©e satisfaisante si elle atteint les seuils d√©crits ci-dessus au **75·µâ percentile**, favorisant ainsi une bonne exp√©rience utilisateur et, par cons√©quent, une meilleure r√©tention et un meilleur r√©f√©rencement ([SEO](https://en.wikipedia.org/wiki/Search_engine_optimization)).

Cependant, l'ajout d'alertes sp√©cifiques sur ces m√©triques doit √™tre m√ªrement r√©fl√©chi. Contrairement aux indicateurs op√©rationnels classiques, tels que la disponibilit√© ou le taux d'erreurs, qui refl√®tent directement la stabilit√© du syst√®me, Les *Web Vitals* d√©pendent de **nombreux facteurs externes**, comme les conditions r√©seau des utilisateurs ou leurs appareils, rendant les seuils plus complexes √† surveiller efficacement.

Pour √©viter une surcharge d'alertes inutiles, ces alertes doivent uniquemement cibler des **d√©gradations significatives**. Par exemple, une augmentation soudaine du **CLS** (stabilit√© visuelle) ou une d√©t√©rioration continue du **LCP** (temps de chargement) sur plusieurs jours peuvent indiquer des probl√®mes importants n√©cessitant une intervention.

Enfin, ces alertes n√©cessitent des outils adapt√©s, comme le *RUM (Real User Monitoring)* pour les donn√©es r√©elles ou le *Synthetic Monitoring* pour des tests simul√©s, qui requi√®rent une solution sp√©cifique non abord√©e dans cet article.

### ‚ú® Les "Golden Signals"

<center><img src="golden_signals.png" width=300 alt="Golden Signals"></center>

Les _Golden Signals_ sont un ensemble de **quatre indicateurs cl√©s**, largement utilis√©s dans le domaine de la supervision des syst√®mes et des applications, notamment avec des outils comme Prometheus. Ces signaux permettent de surveiller la sant√© et la performance des applications de mani√®re efficace. Ils sont particuli√®rement appropri√©s dans le contexte d'une architecture distribu√©e:

* **La Latence** ‚è≥: Elle inclut √† la fois le temps des requ√™tes r√©ussies et le temps des requ√™tes √©chou√©es. La latence est cruciale car une augmentation du temps de r√©ponse peut indiquer des probl√®mes de performance.

* **Le Trafic** üì∂: Il peut √™tre mesur√©e en termes de nombre de requ√™tes par seconde, de d√©bit de donn√©es, ou d'autres m√©triques qui expriment la charge du syst√®me.

* **Les erreurs** ‚ùå: Il s'agit du taux d'√©chec des requ√™tes ou des transactions. Cela peut inclure des erreurs d'application, des erreurs d'infrastructure ou toute situation o√π une requ√™te ne s'est pas termin√©e correctement (par exemple, des r√©ponses HTTP 5xx ou des requ√™tes rejet√©es).

* **La saturation** üìà: C'est une mesure de l'utilisation des ressources du syst√®me, comme le CPU, la m√©moire ou la bande passante r√©seau. La saturation indique √† quel point le syst√®me est proche de ses limites. Un syst√®me satur√© peut entra√Æner des ralentissements ou des pannes.

Ces Golden Signals sont essentiels car ils permettent de **concentrer la surveillance sur les aspects critiques** qui peuvent rapidement affecter l'exp√©rience utilisateur ou la performance globale du syst√®me. Avec Prometheus, ces signaux sont souvent surveill√©s via des m√©triques sp√©cifiques pour d√©clencher des alertes lorsque certains seuils sont d√©pass√©s.

Vous l'aurez compris: Definir des alertes √ßa se r√©fl√©chit! Maintenant entrons dans le concret et voyons **comment d√©finir des seuils √† partir de nos m√©triques**.

{{% notice info "D'autres m√©thodes et indicateurs" %}}
J'ai √©voqu√© ici 2 m√©thodologies qui, je trouve, sont un bon point de d√©part pour ajuster au mieux notre syst√®me d'alerting. Ceci-dit il en existe d'autres, chacune avec leurs sp√©cificit√©s. On peut ainsi citer [USE](https://www.brendangregg.com/usemethod.html) ou [RED](https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services/).

De m√™me, au-del√† des Core Web Vitals pr√©sent√©s plus haut, d'autres m√©triques web comme **[FCP](https://web.dev/articles/fcp)** (First Contentful Paint) ou **[TTFB](https://web.dev/articles/ttfb)** (Time To First Byte) peuvent s'av√©rer utiles selon vos besoins sp√©cifiques.

Le choix des m√©triques √† surveiller d√©pendra de votre contexte et de vos objectifs. L'essentiel est de garder √† l'esprit qu'une bonne strat√©gie d'alerting repose sur un ensemble cibl√© d'indicateurs pertinents üéØ
{{% /notice %}}

## üíª Exprimer des requ√™tes avec PromQL/MetricsQL

Les m√©triques collect√©es avec Prometheus peuvent √™tre requet√©es avec un langage sp√©cifique appel√© `PromQL` (Prometheus Query Language). Ce langage permet d'extraire des donn√©es de supervision, d'effectuer des **calculs**, d'**agr√©ger** les r√©sultats, d'appliquer des **filtres**, mais aussi de configurer des **alertes**.

(‚ÑπÔ∏è Se r√©f√©rer au [pr√©c√©dent article](https://blog.ogenki.io/fr/post/series/observability/metrics/#-quest-ce-quune-m%C3%A9trique) pour comprendre ce que l'on entend par m√©trique.)

PromQL est un langage puissant dont voici quelques exemples simples appliqu√©s aux m√©triques expos√©es par un serveur web Nginx :

* Nombre total de requ√™tes trait√©es (`nginx_http_requests_total`) :
  ```promql
  nginx_http_requests_total
  ```

* Nombre moyen de requ√™tes par seconde sur une fen√™tre de 5 minutes.
  ```promql
  rate(nginx_http_requests_total[5m])
  ```

* Nombre de requ√™tes HTTP par seconde retournant un code d'erreur 5xx sur les 5 derni√®res minutes.
  ```promql
  rate(nginx_http_requests_total{status=~"5.."}[5m])
  ```

* Nombre de requ√™tes par seconde, agr√©g√© par pod et filtr√© sur le namespace "myns" sur les 5 derni√®res minutes.
  ```promql
  sum(rate(nginx_http_requests_total{namespace="myns"}[5m])) by (pod)
  ```

üí° Dans les exemples ci-dessus, nous avons mis en √©vidence deux _Golden Signals_ : le trafic üì∂ et les erreurs ‚ùå.

`MetricsQL` est le langage utilis√© avec VictoriaMetrics. Il se veut compatible avec PromQL avec de l√©g√®res diff√©rences qui permettent de faciliter l'√©criture de requ√™tes complexes.</br>
Il apporte aussi de nouvelles fonctions dont voici quelques exemples:

* `histogram(q)`: Cette fonction calcule un histogramme pour chaque groupe de points ayant le m√™me horodatage, ce qui est utile pour visualiser un grand nombre de s√©ries temporelles (timeseries) via un heatmap. </br>
  Pour cr√©er un histogramme des requ√™tes HTTP
  ```promql
  histogram(rate(vm_http_requests_total[5m]))
  ```

* `quantiles("phiLabel", phi1, ..., phiN, q)`: Utilis√© pour extraire plusieurs quantiles (ou percentiles) d'une m√©trique donn√©e . </br>
  Pour calculer les 50e, 90e et 99e percentiles du taux de requ√™tes HTTP
  ```promql
  quantiles("percentile", 0.5, 0.9, 0.99, rate(vm_http_requests_total[5m]))
  ```

Afin de pouvoir tester ses requ√™tes, vous pouvez utiliser la d√©mo fournie par VictoriaMetrics: https://play.victoriametrics.com

<center><img src="vmplay.png" width=700 alt=""></center>

## üõ†Ô∏è Configurer des alertes avec VictoriaMetrics Operator

VictoriaMetrics propose deux composants essentiels pour la gestion des alertes :
- **VMAlert** : responsable de l'√©valuation des r√®gles d'alerte
- **AlertManager** : g√®re le routage et la distribution des notifications

### VMAlert : Le moteur d'√©valuation des r√®gles

VMAlert est le composant qui √©value en continu les r√®gles d'alerte d√©finies. Il supporte deux types de r√®gles :

1. **Recording Rules** üìä
   - Pr√©-calculent des expressions PromQL complexes
   - Cr√©ent de nouvelles m√©triques (time series)
   - Optimisent les performances des dashboards
   - Exemple de recording rule :
   ```yaml
   groups:
     - name: recording_rules
       rules:
         - record: job:http_requests_total:rate5m
           expr: sum(rate(http_requests_total[5m])) by (job)
   ```

2. **Alerting Rules** üö®
   - D√©finissent les conditions de d√©clenchement des alertes
   - Supportent des annotations pour enrichir les notifications
   - Permettent la classification par labels
   - Exemple d'alerte sur la latence :
   ```yaml
   groups:
     - name: latency_alerts
       rules:
         - alert: HighLatency
           expr: http_request_duration_seconds{quantile="0.9"} > 1
           for: 5m
           labels:
             severity: warning
           annotations:
             summary: "Latence √©lev√©e d√©tect√©e"
             description: "La latence P90 d√©passe 1s depuis 5 minutes"
             runbook_url: "https://wiki.example.com/runbooks/high-latency"
   ```

{{% notice tip "Des exemples concrets" %}}
<table>
  <tr>
        <td>
          <img src="repo_gift.png" style="width:80%;">
        </td>
        <td style="vertical-align:middle; padding-left:10px;" width="70%">

Le reste de cet article est issu d'un ensemble de configurations que vous pouvez retrouver dans le repository <strong><a href="https://github.com/Smana/cloud-native-ref">Cloud Native Ref</a></strong>.</br>
Il y est fait usage de nombreux op√©rateurs et notamment celui pour [VictoriaMetrics](https://github.com/VictoriaMetrics/operator).

L'ambition de ce projet est de pouvoir <strong>d√©marrer rapidement une plateforme compl√®te</strong> qui applique les bonnes pratiques en terme d'automatisation, de supervision, de s√©curit√© etc. </br>
Les commentaires et contributions sont les bienvenues üôè
        </td>
  </tr>
</table>
{{% /notice %}}


### üí° D√©clarer une `VMRule`

Nous avons vu pr√©c√©demment que VictoriaMetrics fournit un op√©rateur Kubernetes qui permet de g√©rer les diff√©rents composants de mani√®re d√©clarative. Parmi les ressources personnalis√©es (Custom Resources) disponibles, la `VMRule` permet de d√©finir des r√®gles d'alertes et d'enregistrement (recording rules).

Si vous avez d√©j√† utilis√© l'[op√©rateur Prometheus](https://github.com/prometheus-operator/prometheus-operator), vous retrouverez une syntaxe tr√®s similaire car l'op√©rateur VictoriaMetrics est compatible avec les custom resources de Prometheus. (Ce qui nous permet de faciliter la migration üòâ).

Prenons un exemple concret avec une `VMRule` qui surveille l'√©tat de sant√© des ressources Flux :

[flux/observability/vmrule.yaml](https://github.com/Smana/cloud-native-ref/blob/main/flux/observability/vmrule.yaml)

```yaml
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMRule
metadata:
  labels:
    prometheus-instance: main
  name: flux-system
  namespace: flux-system
spec:
  groups:
    - name: flux-system
      rules:
        - alert: FluxReconciliationFailure
          annotations:
            message: Flux resource has been unhealthy for more than 5m
            description: "{{ $labels.kind }} {{ $labels.exported_namespace }}/{{ $labels.name }} reconciliation has been failing for more than ten minutes."
            runbook_url: "https://fluxcd.io/flux/cheatsheets/troubleshooting/"
            dashboard: "https://grafana.priv.${domain_name}/dashboards"
          expr: max(gotk_reconcile_condition{status="False",type="Ready"}) by (exported_namespace, name, kind) + on(exported_namespace, name, kind) (max(gotk_reconcile_condition{status="Deleted"}) by (exported_namespace, name, kind)) * 2 == 1
          for: 10m
          labels:
            severity: warning
```

Il est recommand√© de suivre quelques **bonnes pratiques** pour donner le maximum de contexte afin d'identifier rapidement la cause racine.

1. **Nommage et Organisation** üìù
   - Utiliser des noms descriptifs pour les r√®gles, comme `FluxReconciliationFailure`
   - Grouper les r√®gles par composant Flux (ex: `flux-system`, `flux-controllers`)
   - Documenter les conditions de r√©conciliation dans les annotations

2. **Seuils et Dur√©es** ‚è±Ô∏è
   - Ajuster la dur√©e d'√©valuation de l'alerte `for: 10m` pour √©viter les faux positifs
   - Adapter les seuils selon le type de ressource supervis√©es.
   - Consid√©rer des dur√©es diff√©rentes selon l'environnement (prod/staging)

3. **Labels et Routing** üè∑Ô∏è
   - Ajouter des labels pour le routage selon le contexte. Mon exemple n'est pas tr√®s pouss√© car il s'agit d'une configuration de d√©mo. Mais nous pourrions tr√®s bien ajouter un label `team` afin de router vers la bonne √©quipe, ou avoir une politique de routage diff√©rente selon l'environnement.
     ```yaml
     labels:
       severity: [critical|warning|info]
       team: [sre|dev|ops]
       environment: [prod|staging|dev]
     ```

4. **L'importance des annotations** üìö

  Les annotations permettent d'ajouter divers informations sur le contexte de l'alerte
  - Une **description** claire du probl√®me de r√©conciliation
  - Le lien vers le **runbook** de troubleshooting Flux
  - Le lien vers le **dashboard Grafana** d√©di√©

5. **Expression PromQL Optimis√©e** üîç
   ```yaml
   expr: |
     max(gotk_reconcile_condition{status="False",type="Ready"}) by (exported_namespace, name, kind)
     + on(exported_namespace, name, kind)
     (max(gotk_reconcile_condition{status="Deleted"}) by (exported_namespace, name, kind)) * 2 == 1
   ```
   Cette expression :
   - Surveille les conditions de r√©conciliation en √©chec
   - Prend en compte les ressources supprim√©es
   - Agr√®ge par namespace, nom et type de ressource

## üí¨ Int√©gration avec Slack

L'int√©gration avec Slack permet de recevoir les alertes directement dans vos canaux de communication. Voici comment la configurer de mani√®re efficace.

### Configuration de l'Application Slack

1. **Cr√©ation de l'Application** üîß
   - Rendez-vous sur [https://api.slack.com/apps](https://api.slack.com/apps)
   - Cliquez sur "Create New App"
   - Choisissez "From scratch"
   - Nommez votre application (ex: "AlertManager")
   - S√©lectionnez votre workspace

2. **Configuration des Permissions** üîë
   Dans "OAuth & Permissions", ajoutez les scopes suivants :
   - `chat:write` (Requis)
   - `chat:write.public` (Pour poster dans les canaux publics)
   - `channels:read` (Pour lister les canaux)
   - `groups:read` (Pour les groupes priv√©s)

3. **Installation et Token** üéüÔ∏è
   - Installez l'application dans votre workspace
   - Copiez le "Bot User OAuth Token" (commence par `xoxb-`)
   - Stockez ce token de mani√®re s√©curis√©e dans Kubernetes :

   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: alertmanager-slack-token
     namespace: monitoring
   type: Opaque
   stringData:
     token: xoxb-your-token-here
   ```

### Configuration d'AlertManager pour Slack

1. **Configuration de Base** ‚öôÔ∏è
   ```yaml
   alertmanager:
     config:
       global:
         slack_api_url: "https://slack.com/api/chat.postMessage"
         resolve_timeout: 5m

       route:
         group_by: ['alertname', 'job', 'severity']
         group_wait: 30s
         group_interval: 5m
         repeat_interval: 4h
         receiver: 'slack-notifications'

       receivers:
         - name: 'slack-notifications'
           slack_configs:
           - channel: '#alerts'
             send_resolved: true
             icon_emoji: ':bell:'
             title: '{{ template "slack.title" . }}'
             text: '{{ template "slack.text" . }}'
   ```

2. **Templates Personnalis√©s** üìù
   ```yaml
   templates:
     - name: slack.title
       template: |
         [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
     - name: slack.text
       template: |
         {{ range .Alerts }}
         *Alert:* {{ .Labels.alertname }}
         *Description:* {{ .Annotations.description }}
         *Severity:* {{ .Labels.severity }}
         *Started:* {{ .StartsAt | since }}
         {{ if .Annotations.runbook }}*Runbook:* {{ .Annotations.runbook }}{{ end }}
         {{ end }}
   ```

<center><img src="alert.png" width=650 alt="Slack alert example"></center>

### üé® Personnalisation Avanc√©e des Notifications

1. **Boutons d'Action** üîò
   ```yaml
   slack_configs:
     - actions:
         - type: button
           text: "Voir le Runbook üìö"
           url: "{{ .CommonAnnotations.runbook_url }}"
         - type: button
           text: "Voir le Dashboard üìä"
           url: "{{ .CommonAnnotations.dashboard_url }}"
         - type: button
           text: "Silence üîï"
           url: "{{ template "__alert_silence_link" . }}"
   ```

2. **Routage Intelligent** üîÄ
   ```yaml
   route:
     routes:
       - match:
           severity: critical
         receiver: 'slack-critical'
         continue: true
       - match_re:
           service: ^(frontend|backend)$
         receiver: 'slack-apps'
   ```

## ü§ñ Fonctionnalit√©s Avanc√©es

### Int√©gration avec Grafana OnCall

[Grafana OnCall](https://grafana.com/products/oncall/) permet d'am√©liorer la gestion des astreintes et des escalades d'incidents. Voici comment l'int√©grer :

1. **Configuration de l'Int√©gration** üîå
   ```yaml
   receivers:
     - name: 'grafana-oncall'
       webhook_configs:
         - url: 'http://oncall:8080/api/v1/alert'
           send_resolved: true
   ```

2. **D√©finition des Rotations** üìÖ
   - Configurez les √©quipes et les rotations dans Grafana OnCall
   - Associez les alertes aux √©quipes via les labels

### AI Runbooks üìö

Les AI Runbooks permettent d'automatiser la r√©solution des incidents en utilisant l'intelligence artificielle :

1. **Int√©gration avec un LLM** üß†
   ```yaml
   annotations:
     runbook_ai: |
       {
         "model": "gpt-4",
         "context": "Application Java Spring Boot",
         "previous_incidents": "link_to_similar_incidents",
         "suggested_actions": [
           "V√©rifier les logs applicatifs",
           "Analyser l'utilisation m√©moire",
           "Red√©marrer le service si n√©cessaire"
         ]
       }
   ```

2. **Automatisation des Actions** ü§ñ
   ```yaml
   - alert: HighMemoryUsage
     expr: container_memory_usage_bytes > 2e9
     for: 5m
     annotations:
       runbook_ai_action: |
         1. Collecter les dumps m√©moire
         2. Analyser avec AI Memory Analyzer
         3. Sugg√©rer des optimisations
   ```

### M√©triques sur les Alertes üìä

Surveillez la qualit√© de vos alertes avec des m√©triques d√©di√©es :

```yaml
- record: alert_quality_metrics
  expr: |
    sum(rate(alertmanager_notifications_total[24h])) by (integration)
    /
    sum(rate(alertmanager_notifications_failed_total[24h])) by (integration)
```


## üîß Troubleshooting et Maintenance

### Diagnostic des Probl√®mes Courants

1. **Alertes Non D√©clench√©es** ü§î
   - V√©rifiez l'√©tat de VMAlert :
     ```bash
     kubectl get pods -n monitoring -l app=vmalert
     kubectl logs -n monitoring -l app=vmalert
     ```
   - Validez vos expressions PromQL sur VictoriaMetrics UI
   - Contr√¥lez les timestamps des derni√®res m√©triques re√ßues

2. **Notifications Non Re√ßues** üì´
   - V√©rifiez l'√©tat d'AlertManager :
     ```bash
     kubectl get pods -n monitoring -l app=alertmanager
     ```
   - Consultez les logs pour les erreurs de connexion :
     ```bash
     kubectl logs -n monitoring -l app=alertmanager | grep "error"
     ```
   - Testez la connectivit√© Slack avec une alerte de test

3. **Faux Positifs Fr√©quents** ‚ö†Ô∏è
   - Ajustez les seuils et les dur√©es (`for`)
   - Utilisez des expressions plus robustes :
     ```yaml
     - alert: HighErrorRate
       expr: |
         (
           sum(rate(http_requests_total{status=~"5.."}[5m]))
           /
           sum(rate(http_requests_total[5m])) > 0.05
         )
       for: 5m
     ```
   - Impl√©mentez des conditions de pr√©qualification

### Maintenance et Bonnes Pratiques

1. **Revue P√©riodique** üìä
   - Analysez les m√©triques d'alertes mensuellement
   - Identifiez les alertes les plus fr√©quentes
   - Ajustez les seuils selon les retours d'exp√©rience

2. **Documentation** üìù
   - Maintenez un catalogue d'alertes √† jour
   - Documentez les proc√©dures de r√©solution
   - Partagez les retours d'exp√©rience

3. **Tests et Validation** ‚úÖ
   ```yaml
   - alert: TestAlert
     expr: vector(1)
     labels:
       severity: info
       type: test
     annotations:
       summary: "Alerte de test"
       description: "Cette alerte permet de valider la cha√Æne de notification"
   ```

## üéØ Conclusion

La mise en place d'alertes efficaces avec VictoriaMetrics n√©cessite une approche m√©thodique et r√©fl√©chie. Nous avons vu comment :

1. **D√©finir des Alertes Pertinentes** üìã
   - Utiliser les Core Web Vitals pour la performance utilisateur
   - S'appuyer sur les Golden Signals pour la sant√© syst√®me
   - √âviter la fatigue d'alertes avec des seuils appropri√©s

2. **Configurer l'Infrastructure** ‚öôÔ∏è
   - Mettre en place VMAlert pour l'√©valuation des r√®gles
   - Configurer AlertManager pour la gestion des notifications
   - Int√©grer Slack pour la communication d'√©quipe

3. **Optimiser et Maintenir** üîÑ
   - Utiliser les recording rules pour les calculs complexes
   - Impl√©menter des templates personnalis√©s
   - Maintenir une documentation √† jour

### Prochaines √âtapes Sugg√©r√©es

Pour aller plus loin dans votre impl√©mentation :

1. **Automatisation** ü§ñ
   - D√©ploiement des r√®gles via GitOps
   - Int√©gration avec des outils d'IA pour l'analyse
   - Automatisation des tests d'alertes

2. **Monitoring Avanc√©** üìà
   - Impl√©mentation de SLOs bas√©s sur les alertes
   - Corr√©lation avec les logs et le tracing
   - Dashboards d√©di√©s au suivi des alertes

3. **Organisation** üë•
   - D√©finition des processus d'escalade
   - Formation des √©quipes
   - Revues post-incident syst√©matiques

La supervision proactive via des alertes bien configur√©es est un √©l√©ment cl√© de toute strat√©gie d'observabilit√©. En suivant les bonnes pratiques et en utilisant les outils appropri√©s, vous pouvez construire un syst√®me d'alerting robuste et efficace qui vous permettra d'identifier et de r√©soudre les probl√®mes avant qu'ils n'impactent vos utilisateurs.

{{% notice tip "Pour aller plus loin üöÄ" %}}
D√©couvrez comment int√©grer ces alertes avec d'autres composants de votre stack d'observabilit√© dans les prochains articles de cette s√©rie, notamment la corr√©lation avec les logs et le tracing distribu√©.
{{% /notice %}}


## üîñ References

* https://web.dev/articles/vitals
* https://medium.com/@romanhavronenko/victoriametrics-promql-compliance-d4318203f51e
* https://victoriametrics.com/blog/alerting-recording-rules-alertmanager/


* Grafana oncall
* AI runbooks


https://docs.victoriametrics.com/vmalert/
VMAlert, ruler evaluation des alertes en fonction de seuils.
Alertmanage notifications
