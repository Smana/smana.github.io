[{"body":"","link":"https://blog.ogenki.io/fr/","section":"","tags":null,"title":""},{"body":" Mieux inspecter nos applications üëÅÔ∏è Une fois notre application d√©ploy√©e, il est primordial de disposer d'indicateurs permettant d'identifier d'√©ventuels probl√®mes ainsi que de suivre les √©volutions de performance. Parmi ces √©l√©ments, les m√©triques et les logs jouent un r√¥le essentiel en fournissant des informations pr√©cieuses sur le fonctionnement de l'application. En compl√©ment, il est souvent utile de mettre en place un tracing d√©taill√© pour suivre pr√©cis√©ment toutes les actions r√©alis√©es par l'application.\nDans cette s√©rie d'articles, nous allons explorer les diff√©rents aspects li√©s √† la supervision applicative. L'objectif √©tant d'analyser en d√©tail l'√©tat de nos applications, afin d'am√©liorer leur disponibilit√© et leurs performances, tout en garantissant une exp√©rience utilisateur optimale.\nTrop souvent, la gestion des logs signifie solutions complexes et requ√™tes lentes. Pourtant les logs sont un pilier incontournable pour comprendre, diagnostiquer et am√©liorer nos applications.\nEn effet, si les m√©triques nous permettent d'observer l'√©volution d'indicateurs dans le temps et les traces de suivre le cheminement d'une requ√™te au sein de notre plateforme, les logs nous offrent le contexte d√©taill√© indispensable √† la compr√©hension des √©v√©nements.\n‚ùì A quoi servent nos logs? Les logs ne sont pas de simples messages que l'on accumule dans un coin de notre infra: ils constituent la m√©moire vivante de nos syst√®mes. Ils sont essentiels car ils endossent plusieurs r√¥les critiques dont voici quelques mises en situation concr√®tes :\nDiagnostic et D√©pannage : Une application e-commerce rencontre des erreurs 500 lors du paiement, les logs permettent de retracer la s√©quence exacte des appels, d‚Äôidentifier qu‚Äôune d√©pendance externe (ex: API de paiement) est en cause, et de corriger rapidement le probl√®me. S√©curit√© et Conformit√© : Les logs r√©v√®lent des tentatives de connexion suspectes en dehors des horaires habituels‚ÄØ; ils permettent de d√©tecter une attaque par brut force et de renforcer la s√©curit√©. Ils sont aussi indispensables pour r√©pondre aux exigences r√©glementaires (RGPD, PCI DSS, etc.). Monitoring et Alerting Proactif : Des r√®gles d‚Äôalerte d√©tectent automatiquement une augmentation anormale du taux d‚Äôerreurs dans les logs d‚Äôun service critique, permettant d‚Äôintervenir avant que la situation ne s'aggrave. Audit et Tra√ßabilit√© : Lors d‚Äôun audit RGPD, les logs d‚Äôacc√®s permettent de reconstituer pr√©cis√©ment l‚Äôhistorique des actions sur les donn√©es personnelles. Mais pour que ces cas d‚Äôusage r√©v√®lent toute leur valeur, il ne suffit pas de collecter les logs‚ÄØ: il faut pouvoir les rechercher rapidement, formuler des requ√™tes simples, et garantir leur conservation √† long terme sans exploser les co√ªts ni la complexit√©. C'est exactement l√† que VictoriaLogs entre en sc√®ne üîé\nüöÄ VictoriaLogs : Une nouvelle r√©ponse √† la gestion et l'analyse de logs Avec l'adoption des architectures distribu√©es, nos plateformes g√©n√®rent des logs en quantit√© toujours plus importante.\nPour exploiter ces volumes croissants, nous nous sommes traditionnellement tourn√©s vers des solutions comme ELK (Elasticsearch, Logstash, Kibana) ou Grafana Loki, souvent synonymes de complexit√© op√©rationnelle.\nEn 2023, VictoriaLogs est apparu comme une alternative prometteuse qui pourrait bien changer la donne.\nD√©velopp√© par l'√©quipe derri√®re la base de donn√©es time series VictoriaMetrics, dont la popularit√© est grandissante, VictoriaLogs h√©rite des m√™mes qualit√©s. Voici ses principales caract√©ristiques:\nSimplicit√© de d√©ploiement et d'op√©ration : Son installation et sa configuration sont plut√¥t simples et nous allons voir ensemble le mode le plus avanc√© ci-apr√®s (cluster). Haute performance : Optimis√© pour une ingestion massive de logs et des requ√™tes analytiques rapides, m√™me sur de tr√®s grands volumes de donn√©es. Efficacit√© des ressources : Faible empreinte CPU et m√©moire, et compression efficace des donn√©es pour minimiser les co√ªts de stockage en comparaison avec des solutions similaires. Int√©gration √† l'√©cosyst√®me VictoriaMetrics : S'int√®gre naturellement avec VictoriaMetrics pour une solution d'observabilit√© unifi√©e, et avec VMAlert pour l'alerting et Grafana pour la visualisation. Recherche Full-Text Rapide et par label : VictoriaLogs permet des recherches √† la fois des recherches full-text sur le contenu des logs et des filtrages pr√©cis par labels. üÜö Par rapport √† Elasticsearch ou Loki? Plusieurs sources permettent d'attester de la performance de VictoriaLogs, en comparaison √† d'autres solutions de gestion de logs.\nLes √©carts de performance, en comparaison avec ELK ou Loki, sont assez impressionnants, que ce soit en termes d'utilisation m√©moire ou de compression des donn√©es.\nConcernant la recherche de logs, VictoriaLogs se distingue en combinant efficacement la recherche full-text d'Elasticsearch et le filtrage par labels de Loki, offrant ainsi le meilleur des deux approches tout en conservant une rapidit√© d'ex√©cution des requ√™tes.\nüóÉÔ∏è L'ingestion et le stockage Un log dans VictoriaLogs est typiquement un objet JSON. Chaque log contient forc√©ment des champs suivants :\n_msg: Le contenu brut du message de log, tel qu'il est produit par l'application. _time: Le timestamp du log. _stream: Un ensemble de labels (cl√©-valeur) qui identifient de mani√®re unique la source du log. L\u0026#39;importance du Stream Le champs _stream dans VictoriaLogs permet d'optimiser la compression et de garantir une recherche ultra-rapide gr√¢ce au stockage contigu des logs partageant les m√™mes labels.\nL'efficacit√© d√©pend d'un choix pr√©cautionneux : seuls les champs constants, qui identifient de fa√ßon unique une instance d'application (container, namespace, pod), doivent faire partie du stream. Les champs dynamiques (IP, user_id, trace_id) doivent rester dans le message afin d'√©viter une cardinalit√© trop √©lev√©e.\nIl est possible de stocker un log simplement via la commande curl, ou en utilisant diff√©rents agents de collecte et de transport de logs tels que Promtail, FluentBit, OpenTelemetry et j'en passe.\nJ'ai choisi Vector car il s'agit d'une solution tr√®s performante mais aussi car il est propos√© par d√©faut dans le chart Helm que nous allons utiliser üòâ.\nParmi les √©l√©ments de configuration requis, il faut indiquer la destination mais aussi les champs indispensables dont nous avons parl√© pr√©c√©demment, qui sont ici configur√©s en utilisant des headers HTTP.\n1 sinks: 2 vlogs-0: 3 compression: gzip 4 endpoints: 5 - http://\u0026lt;victorialogs_host\u0026gt;:9428/insert/elasticsearch 6 healthcheck: 7 enabled: false 8 inputs: 9 - parser 10 mode: bulk 11 request: 12 headers: 13 AccountID: \u0026#34;0\u0026#34; 14 ProjectID: \u0026#34;0\u0026#34; 15 VL-Msg-Field: message,msg,_msg,log.msg,log.message,log 16 VL-Stream-Fields: stream,kubernetes.pod_name,kubernetes.container_name,kubernetes.pod_namespace 17 VL-Time-Field: timestamp 18 type: elasticsearch Les logs sont collect√©s sur un cluster Kubernetes, et Vector l'enrichit avec de nombreux champs permettant d'identifier de fa√ßon pr√©cise la source. Voici un exemple concret de log enrichi tel qu'il est stock√© dans VictoriaLogs (Ce log a √©t√© volontairement tronqu√© pour le besoin de cet article) :\n1 { 2 \u0026#34;_time\u0026#34;: \u0026#34;2025-07-29T07:25:49.870820279Z\u0026#34;, 3 \u0026#34;_stream_id\u0026#34;: \u0026#34;00000000000000006a98e166d58afc9efc6ea35a22d87f1b\u0026#34;, 4 \u0026#34;_stream\u0026#34;: \u0026#34;{kubernetes.container_name=\\\u0026#34;loggen\\\u0026#34;,kubernetes.pod_name=\\\u0026#34;loggen-loggen-68dc4f9b8b-6mrqj\\\u0026#34;,kubernetes.pod_namespace=\\\u0026#34;observability\\\u0026#34;,stream=\\\u0026#34;stdout\\\u0026#34;}\u0026#34;, 5 \u0026#34;_msg\u0026#34;: \u0026#34;236.161.251.196 - [07/Jul/2025:08:13:41 ] \\\u0026#34;GET /homepage HTTP/2\\\u0026#34; 204 4367 \\\u0026#34;http://localhost/\\\u0026#34; \\\u0026#34;curl/7.68.0\\\u0026#34; \\\u0026#34;DE\\\u0026#34; 0.83\u0026#34;, 6 \u0026#34;file\u0026#34;: \u0026#34;/var/log/pods/observability_loggen-loggen-68dc4f9b8b-6mrqj_33076791-133a-490f-bd44-97717d242a61/loggen/0.log\u0026#34;, 7 \u0026#34;kubernetes.container_name\u0026#34;: \u0026#34;loggen\u0026#34;, 8 \u0026#34;kubernetes.node_labels.beta.kubernetes.io/instance-type\u0026#34;: \u0026#34;c5.xlarge\u0026#34;, 9 \u0026#34;kubernetes.node_labels.beta.kubernetes.io/os\u0026#34;: \u0026#34;linux\u0026#34;, 10 \u0026#34;kubernetes.node_labels.eks.amazonaws.com/capacityType\u0026#34;: \u0026#34;SPOT\u0026#34;, 11 \u0026#34;kubernetes.pod_ip\u0026#34;: \u0026#34;10.0.33.16\u0026#34;, 12 \u0026#34;kubernetes.pod_labels.app.kubernetes.io/name\u0026#34;: \u0026#34;loggen\u0026#34;, 13 \u0026#34;kubernetes.pod_name\u0026#34;: \u0026#34;loggen-loggen-68dc4f9b8b-6mrqj\u0026#34;, 14 \u0026#34;kubernetes.pod_namespace\u0026#34;: \u0026#34;observability\u0026#34;, 15 \u0026#34;kubernetes.pod_node_name\u0026#34;: \u0026#34;ip-10-0-47-231.eu-west-3.compute.internal\u0026#34;, 16 \u0026#34;source_type\u0026#34;: \u0026#34;kubernetes_logs\u0026#34;, 17 \u0026#34;stream\u0026#34;: \u0026#34;stdout\u0026#34; 18 \u0026lt;REDACTED\u0026gt; 19 } Maintenant que nous avons une vue d'ensemble du fonctionnement de VictoriaLogs, je vous propose ici une m√©thode d'installation et de configuration qui peut √™tre envisag√© pour de la prodution.\nüèóÔ∏è Installation et configuration VictoriaLogs peut √™tre install√© de 2 fa√ßons:\nUn mode Single qui a l'avantage d'√™tre tr√®s simple car un seul binaire se charge de toutes les op√©rations. C'est le mode √† privil√©gier car il est simple √† op√©rer. Si vous disposez d'une machine puissante, dont les ressources permettent de r√©pondre √† votre besoin, ce mode sera toujours plus performant car il ne n√©cessite pas de transferts r√©seau entre les diff√©rents composants du mode cluster.\nLe mode Cluster sera utilis√© pour les tr√®s fortes charges et un besoin de scaling horizontal (lorsqu'une seule machine n'est pas suffisante pour r√©pondre au besoin). S'agissant du mode qui donnera le plus de flexibilit√© pour scaler, nous allons l'explorer dans cet article.\nSi vous avez parcouru le pr√©c√©dent article sur VictoriaMetrics, vous remarquerez que l'architecture du mode cluster est tr√®s ressemblante:\nVLStorage: C'est le composant responsable de la persistence des logs sur disque. Il s'agit donc d'un Statefulset et chaque pod dispose d'un volume d√©di√© (Persistent Volume).\nVLInsert: Ce composant re√ßoit les logs √† partir de diff√©rentes sources et diff√©rents protocoles et se charge de les r√©partir sur les VLStorages.\nVector: D√©ploy√© en DaemonSet Vector se charge de transferer les logs stock√©s sur les noeuds Kubernetes vers le service VLInsert.\nVLSelect: Il s'agit du service qui expose l'API nous permettant d'ex√©cuter des requ√™tes. Les donn√©es sont extraites √† partir des VLStorages.\nVMAlert: Afin de pouvoir √©mettre des alertes bas√©s sur les logs, une instance VMAlert d√©di√©e est d√©ploy√©e.\nL'installation se fait en utilisant le chart Helm fournit par VictoriaMetrics, en param√®trant quelques variables. Voici un exemple appropri√© pour EKS que nous allons d√©crire ci-apr√®s:\nobservability/base/victoria-logs/helmrelease-vlcluster.yaml\n1 printNotes: false 2 3 vlselect: 4 horizontalPodAutoscaler: 5 enabled: true 6 maxReplicas: 10 7 minReplicas: 2 8 metrics: 9 - type: Resource 10 resource: 11 name: cpu 12 target: 13 type: Utilization 14 averageUtilization: 70 15 16 podDisruptionBudget: 17 enabled: true 18 minAvailable: 1 19 20 affinity: 21 podAntiAffinity: 22 requiredDuringSchedulingIgnoredDuringExecution: 23 - labelSelector: 24 matchExpressions: 25 - key: \u0026#34;app\u0026#34; 26 operator: In 27 values: 28 - \u0026#34;vlselect\u0026#34; 29 topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 30 topologySpreadConstraints: 31 - labelSelector: 32 matchLabels: 33 app: vlselect 34 maxSkew: 1 35 topologyKey: topology.kubernetes.io/zone 36 whenUnsatisfiable: ScheduleAnyway 37 38 resources: 39 limits: 40 cpu: 100m 41 memory: 200Mi 42 requests: 43 cpu: 100m 44 memory: 200Mi 45 46 vmServiceScrape: 47 enabled: true 48 49 vlinsert: 50 horizontalPodAutoscaler: 51 enabled: true 52 maxReplicas: 10 53 minReplicas: 2 54 metrics: 55 - type: Resource 56 resource: 57 name: cpu 58 target: 59 type: Utilization 60 averageUtilization: 70 61 62 podDisruptionBudget: 63 enabled: true 64 minAvailable: 1 65 66 affinity: 67 podAntiAffinity: 68 requiredDuringSchedulingIgnoredDuringExecution: 69 - labelSelector: 70 matchExpressions: 71 - key: \u0026#34;app\u0026#34; 72 operator: In 73 values: 74 - \u0026#34;vlinsert\u0026#34; 75 topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 76 topologySpreadConstraints: 77 - labelSelector: 78 matchLabels: 79 app: vlinsert 80 maxSkew: 1 81 topologyKey: topology.kubernetes.io/zone 82 whenUnsatisfiable: ScheduleAnyway 83 84 resources: 85 limits: 86 cpu: 100m 87 memory: 200Mi 88 requests: 89 cpu: 100m 90 memory: 200Mi 91 92 vmServiceScrape: 93 enabled: true 94 95 vlstorage: 96 # -- Enable deployment of vlstorage component. StatefulSet is used 97 enabled: true 98 retentionPeriod: 7d 99 retentionDiskSpaceUsage: \u0026#34;9GiB\u0026#34; 100 replicaCount: 3 101 102 podDisruptionBudget: 103 enabled: true 104 minAvailable: 1 105 106 affinity: 107 podAntiAffinity: 108 requiredDuringSchedulingIgnoredDuringExecution: 109 - labelSelector: 110 matchExpressions: 111 - key: \u0026#34;app\u0026#34; 112 operator: In 113 values: 114 - \u0026#34;vlstorage\u0026#34; 115 topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 116 topologySpreadConstraints: 117 - labelSelector: 118 matchLabels: 119 app: vlstorage 120 maxSkew: 1 121 topologyKey: topology.kubernetes.io/zone 122 whenUnsatisfiable: ScheduleAnyway 123 124 persistentVolume: 125 enabled: true 126 size: 10Gi 127 128 resources: 129 limits: 130 cpu: 500m 131 memory: 512Mi 132 requests: 133 cpu: 500m 134 memory: 512Mi 135 136 vmServiceScrape: 137 enabled: true 138 139 vector: 140 enabled: true Autoscaling: Les composants stateless (VLSelect et VLInsert) sont configur√©s pour scaler automatiquement au-del√† de 70% d'utilisation CPU.\nPersistence des logs: Pour cet environnement de d√©mo, chaque instance VLStorage dispose d'un volume EBS de 10Gi avec une r√©tention de 7 jours afin d'√©viter la saturation des disques.\nHaute disponibilit√©: La configuration garantit une disponibilit√© maximale gr√¢ce √† la r√©partition sur diff√©rentes zones (topologySpreadConstraints) et √† l'anti-affinit√© des pods pour chaque composant.\nSupervision: Les vmServiceScrape exposent automatiquement les m√©triques de chaque composant pour le monitoring via l'op√©rateur VictoriaMetrics.\nLorsque le chart Helm est install√©, nous pouvons v√©rifier que tous les pods sont bien d√©marr√©s\n1kubectl get po -n observability -l app.kubernetes.io/instance=victoria-logs 2NAME READY STATUS RESTARTS AGE 3victoria-logs-vector-9gww4 1/1 Running 0 11m 4victoria-logs-vector-frj8l 1/1 Running 0 10m 5victoria-logs-vector-jxm95 1/1 Running 0 10m 6victoria-logs-vector-kr6q6 1/1 Running 0 12m 7victoria-logs-vector-pg2fc 1/1 Running 0 12m 8victoria-logs-victoria-logs-cluster-vlinsert-dbd47c5fd-cmqj9 1/1 Running 0 11m 9victoria-logs-victoria-logs-cluster-vlinsert-dbd47c5fd-mbkwx 1/1 Running 0 12m 10victoria-logs-victoria-logs-cluster-vlselect-7fbfbd9f8f-nmv8t 1/1 Running 0 11m 11victoria-logs-victoria-logs-cluster-vlselect-7fbfbd9f8f-nrhs4 1/1 Running 0 12m 12victoria-logs-victoria-logs-cluster-vlstorage-0 1/1 Running 0 12m 13victoria-logs-victoria-logs-cluster-vlstorage-1 1/1 Running 0 11m 14victoria-logs-victoria-logs-cluster-vlstorage-2 1/1 Running 0 9m39s Et commencer √† utiliser l'interface Web qui est expos√©e en utilisant Cilium et des ressources Gateway API üéâ\nYour browser does not support the video tag. ‚öôÔ∏è D√©ploiement: o√π trouver toute la configuration Toute la configuration utilis√©e pour l'√©criture de cet article se trouve dans le repository Cloud Native Ref.\nL'ambition de ce projet est de pouvoir d√©marrer rapidement une plateforme compl√®te qui applique les bonnes pratiques en terme d'automatisation, de supervision, de s√©curit√© etc. Les commentaires et contributions sont les bienvenues üôè\nüë©‚Äçüíª LogsQL : Un language puissant et facile √† apprendre LogsQL se distingue par sa capacit√© √† effectuer des recherches full-text rapides et l'utilisation des champs expos√©s par les logs.\nPar exemple, nous pouvons rechercher les logs g√©n√©r√©s par les pods dont le nom commence par loggen, puis filtrer ces r√©sultats en incluant ou excluant (en pr√©fixant par -) certaines cha√Ænes de caract√®res.\n1kubernetes.pod_name: \u0026#34;loggen\u0026#34;* \u0026#34;GET /homepage\u0026#34; -\u0026#34;example.com\u0026#34; Cette requ√™te retournera donc tous les appels √† la homepage avec la m√©thode GET en excluant les logs contenant le domaine \u0026quot;example.com\u0026quot;.\nüí° √Ä retenir : La recherche full-text se fait dans le contenu du champ _msg.\nNous allons voir ici quelques exemples de requ√™tes simples que nous pourrions utiliser dans un environnement Kubernetes.\n‚ò∏ √âv√©nements Kubernetes Les √©v√©nements Kubernetes constituent une source d'information pr√©cieuse car ils r√©v√®lent souvent des probl√®mes li√©s aux changements d'√©tat des ressources ou des erreurs qui ne sont pas visibles ailleurs. Il est donc conseill√© de les analyser r√©guli√®rement.\n‚ö†Ô∏è Limitation : ces √©v√©nements sont √©ph√©m√®res et si l'on veut explorer l'historique, il faut une solution pour persister ces donn√©es. En attendant que Vector prenne en charge cette fonctionnalit√©, j'utilise Kubernetes Event Exporter bien que le projet ne semble pas tr√®s actif.\nLorsque la solution est d√©ploy√©e nous pouvons rechercher les √©v√©nements en utilisant le champs source\nUtiliser le caract√®re ~ pour rechercher une cha√Æne de caract√®res dans un champ. Ici nous pouvons visualiser les notifications d'erreur de validation d'une politique d√©finie par Kyverno. 1source:\u0026#34;kubernetes-event-exporter\u0026#34; AND type: \u0026#34;Warning\u0026#34; AND message:~\u0026#34;validation error: Privileged mode is disallowed\u0026#34; La requ√™te suivante fait usage des op√©rateurs logiques AND et NOT pour visualiser les √©v√©nements de type \u0026quot;Warning\u0026quot; tout en filtrant les erreurs Kyverno. 1source:\u0026#34;kubernetes-event-exporter\u0026#34; AND type: \u0026#34;Warning\u0026#34; AND NOT reason: \u0026#34;PolicyViolation\u0026#34; üåê Logs d'un serveur Web Pour le besoin de cet article, j'ai cr√©√© un petit g√©n√©rateur de logs tout simple. Il permet de simuler des logs de type serveur web afin de pouvoir ex√©cuter quelques requ√™tes.\n1loggen --sleep 1 --error-rate 0.2 --format json 2 3{ 4 \u0026#34;remote_addr\u0026#34;: \u0026#34;208.175.166.30\u0026#34;, 5 \u0026#34;remote_user\u0026#34;: \u0026#34;-\u0026#34;, 6 \u0026#34;time_local\u0026#34;: \u0026#34;19/Apr/2025:02:11:56 \u0026#34;, 7 \u0026#34;request\u0026#34;: \u0026#34;PUT /contact HTTP/1.1\u0026#34;, 8 \u0026#34;status\u0026#34;: 202, 9 \u0026#34;body_bytes_sent\u0026#34;: 3368, 10 \u0026#34;http_referer\u0026#34;: \u0026#34;https://github.com/\u0026#34;, 11 \u0026#34;http_user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15\u0026#34;, 12 \u0026#34;country\u0026#34;: \u0026#34;AU\u0026#34;, 13 \u0026#34;request_time\u0026#34;: 0.532 14} üí° √Ä retenir : Les logs √©mis par les applications au format JSON permettent d'indexer tous les champs de l'objet JSON. Cela permet de simplifier les recherches et calculs. Cependant, il faut rester attentif √† la cardinalit√© qui peut impacter les performances.\nVector est configur√© pour parser les logs JSON et extraire leurs champs. S'il ne s'agit pas d'un log JSON, il conserve le message d'origine sans le modifier.\n1 transforms: 2 parser: 3 inputs: 4 - k8s 5 source: | 6 .log = parse_json(.message) ?? .message 7 del(.message) 8 type: remap Nous obtenons donc de nouveaux champs pr√©fix√©s par log dans les logs stock√©s par VictoriaLogs.\n1{ 2 \u0026#34;log.body_bytes_sent\u0026#34;: \u0026#34;4832\u0026#34;, 3 \u0026#34;log.country\u0026#34;: \u0026#34;AU\u0026#34;, 4 \u0026#34;log.http_referer\u0026#34;: \u0026#34;-\u0026#34;, 5 \u0026#34;log.http_user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15\u0026#34;, 6 \u0026#34;log.remote_addr\u0026#34;: \u0026#34;84.74.62.151\u0026#34;, 7 \u0026#34;log.remote_user\u0026#34;: \u0026#34;-\u0026#34;, 8 \u0026#34;log.request\u0026#34;: \u0026#34;PUT /products HTTP/1.1\u0026#34;, 9 \u0026#34;log.request_time\u0026#34;: \u0026#34;1.191\u0026#34;, 10 \u0026#34;log.status\u0026#34;: \u0026#34;204\u0026#34;, 11 \u0026#34;log.time_local\u0026#34;: \u0026#34;27/Jul/2025:10:57:48 \u0026#34;, 12 \u0026lt;REDACTED\u0026gt; 13} Gr√¢ce √† cela, nous pouvons maintenant ecrire des requ√™tes directement sur la valeur des champs. Voici quelques exemples concrets :\nCompter les codes HTTP et les trier par ordre d√©croissant : 1kubernetes.pod_name:\u0026#34;loggen\u0026#34;* | stats by (log.status) count() as count | sort by (count desc) üí° En utilisant stats, il est possible d'effectuer des calculs avanc√©s, de nombreuses fonctions sont disponibles.\nNous avons vu pr√©c√©demment que le caract√®re ~ permet de rechercher une cha√Æne de caract√®res dans un champ. Ce caract√®re indique que nous utilisons des expressions r√©guli√®res (regexp), comme le montre cet exemple simple pour rechercher uniquement les requ√™tes provenant du Japon ou de l'Italie. 1_time:5m kubernetes.pod_name: \u0026#34;loggen\u0026#34;* AND log.country:~\u0026#34;JP|IT\u0026#34; D'autres op√©rateurs de comparaison peuvent √™tre utilis√©s. Ici \u0026gt; pour filtrer uniquement les logs dont la dur√©e d'ex√©cution d√©passe 1,5 seconde. 1kubernetes.pod_labels.app.kubernetes.io/instance:\u0026#34;loggen\u0026#34; AND log.request_time:\u0026gt;1.5 üíª vlogcli pour lancer des requ√™tes Il existe aussi un outil en ligne de commande qui permet d'ex√©cuter des requ√™tes depuis un terminal: vlogcli.\n1vlogscli -datasource.url=\u0026#39;https://vl.priv.cloud.ogenki.io/select/logsql/query\u0026#39; 2sending queries to -datasource.url=https://vl.priv.cloud.ogenki.io/select/logsql/query 3type ? and press enter to see available commands 4;\u0026gt; kubernetes.pod_labels.app.kubernetes.io/instance:\u0026#34;loggen\u0026#34; | stats quantile(0.5, log.request_time) p50, quantile(0.9, log.request_time) p90, quantile(0.99, log.request_time) p99 5executing [\u0026#34;kubernetes.pod_labels.app.kubernetes.io/instance\u0026#34;:loggen | stats quantile(0.5, log.request_time) as p50, quantile(0.9, log.request_time) as p90, quantile(0.99, log.request_time) as p99]...; duration: 2.500s 6{ 7 \u0026#34;p50\u0026#34;: \u0026#34;1.022\u0026#34;, 8 \u0026#34;p90\u0026#34;: \u0026#34;1.565\u0026#34;, 9 \u0026#34;p99\u0026#34;: \u0026#34;1.686\u0026#34; 10} 11;\u0026gt; üìä Int√©gration √† Grafana L'int√©gration avec Grafana se fait avec la Datasource pr√©vue √† cet effet. Celle-ci permet de construire des graphes √† partir des donn√©es pr√©sentes dans VictoriaLogs.\nNous utilisons ici l'op√©rateur Kubernetes pour Grafana qui permet de d√©clarer de la configuration par le biais de ressources personnalis√©es (Custom Resources).\nIl y a donc une ressource GrafanaDatasource pour ajouter la connexion √† VictoriaLogs en indiquant l'adresse du service VLSelect.\n1apiVersion: grafana.integreatly.org/v1beta1 2kind: GrafanaDatasource 3metadata: 4 name: vl-datasource 5 namespace: observability 6spec: 7 allowCrossNamespaceImport: true 8 datasource: 9 access: proxy 10 type: victoriametrics-logs-datasource 11 name: VictoriaLogs 12 url: http://victoria-logs-victoria-logs-cluster-vlselect.observability:9471 13 instanceSelector: 14 matchLabels: 15 dashboards: grafana Nous pouvons ensuite utiliser cette Datasource pour ex√©cuter des requ√™tes et construire des graphes.\nIl existe √©galement des dashboards pr√™ts √† l'usage.\nLa configuration d'un nouveau dashboard est tout aussi simple gr√¢ce √† l'operateur Grafana. Nous indiquons l'adresse du dashboards accessible depuis l'API Grafana.com.\n1apiVersion: grafana.integreatly.org/v1beta1 2kind: GrafanaDashboard 3metadata: 4 name: observability-victoria-logs-cluster 5 namespace: observability 6spec: 7 allowCrossNamespaceImport: true 8 datasources: 9 - inputName: \u0026#34;DS_VICTORIALOGS\u0026#34; 10 datasourceName: \u0026#34;VictoriaLogs\u0026#34; 11 instanceSelector: 12 matchLabels: 13 dashboards: \u0026#34;grafana\u0026#34; 14 url: \u0026#34;https://grafana.com/api/dashboards/23274/revisions/2/download\u0026#34; Celui-ci nous perment d'analyser les performances des composants du cluster VictoriaLogs.\nUn autre dashboard peut s'av√©rer utile pour visualiser les logs et donc d'unifier sur une seule adresse m√©triques et logs.\nüö® Envoyer des alertes Il est possible de d√©clencher des alertes bas√©es sur l'analyse des logs.\nL'alerting utilise VMAlert, le composant d'alerting de l'√©cosyst√®me VictoriaMetrics.\nUne instance suppl√©mentaire d√©di√©e √† l'analyse des logs a √©t√© ajout√©e (une autre instance √©tant d√©j√† d√©ploy√©e pour les m√©triques) :\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMAlert 3metadata: 4 labels: 5 app.kubernetes.io/component: victoria-logs-vmalert 6 app.kubernetes.io/instance: victoria-logs 7 name: victoria-logs 8 namespace: observability 9spec: 10 ruleSelector: 11 matchLabels: 12 vmlog: \u0026#34;true\u0026#34; 13 datasource: 14 url: http://victoria-logs-victoria-logs-cluster-vlselect.observability.svc.cluster.local.:9471 15 evaluationInterval: 20s 16 image: 17 tag: v1.122.0 18 notifiers: 19 - url: http://vmalertmanager-victoria-metrics-k8s-stack-0.vmalertmanager-victoria-metrics-k8s-stack.observability.svc.cluster.local.:9093 20 port: \u0026#34;8080\u0026#34; 21 remoteRead: 22 url: http://vmselect-victoria-metrics-k8s-stack.observability.svc.cluster.local.:8481 23 remoteWrite: 24 url: http://vminsert-victoria-metrics-k8s-stack.observability.svc.cluster.local.:8480/api/v1/write 25 resources: 26 limits: 27 cpu: 100m 28 memory: 256Mi 29 requests: 30 cpu: 100m 31 memory: 128Mi Integration AlertManager : Utilise l'instance d'AlertManager d√©ploy√©e avec VictoriaMetrics pour la gestion des notifications S√©lecteur de r√®gles : N'√©value que les VMRules avec le label vmlog: \u0026quot;true\u0026quot;, permettant de s√©parer les alertes logs des alertes m√©triques Stockage des alertes : Les alertes sont stock√©es comme m√©triques dans VictoriaMetrics pour l'historique et l'analyse Voici un exemple concret d'alerte qui d√©tecte un taux d'erreurs HTTP trop √©lev√© :\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMRule 3metadata: 4 name: loggen 5 namespace: observability 6 labels: 7 vmlog: \u0026#34;true\u0026#34; 8spec: 9 groups: 10 - name: loggen 11 type: vlogs 12 interval: 10m 13 rules: 14 - alert: LoggenHTTPError500 15 annotations: 16 message: \u0026#34;The application Loggen is throwing too many errors in the last 10 minutes\u0026#34; 17 description: \u0026#39;The pod `{{ index $labels \u0026#34;kubernetes.pod_name\u0026#34; }}` has `{{ $value }}` server errors in the last 10 minutes\u0026#39; 18 expr: \u0026#39;kubernetes.pod_labels.app.kubernetes.io/instance:\u0026#34;loggen\u0026#34; AND log.status:\u0026#34;5\u0026#34;* | stats by (kubernetes.pod_name) count() as server_errors | filter server_errors:\u0026gt;100\u0026#39; 19 labels: 20 severity: warning Si AlertManager est configur√© pour envoyer sur slack comme expliqu√© dans cet article, nous obtenons le r√©sultat suivant:\nüí≠ Derni√®res remarques Cette exploration de VictoriaLogs me permet d'affirmer que la solution est simple √† installer et configurer. Les concepts sont plut√¥t simples √† appr√©hender, que ce soit l'architecture modulaire du mode cluster ou le langage logsQL. En effet, ce langage est tr√®s intuitif et on s'habitue vite √† la syntaxe.\nDe plus, si l'on se r√©f√®re aux tests de performances publi√©s, les temps d'ex√©cution des requ√™tes ainsi que la compression efficace des donn√©es permettent de se projeter sur des plateformes √† large scale.\nVous l'aurez compris, malgr√© le fait que la solution soit relativement jeune, je recommanderais vivement de l'√©tudier et d'avoir une approche permettant de comparer avec vos solutions existantes avant d'envisager une bascule üòâ\nüîñ R√©f√©rences üìö Documentation et ressources officielles Documentation officielle VictoriaLogs Blog VictoriaMetrics Roadmap VictoriaLogs - Features √† venir Playground LogsQL - S'exercer au langage LogsQL üîç Comparaisons et analyses de performance VictoriaLogs vs Loki - Analyse comparative d√©taill√©e VictoriaLogs: The Space-Efficient Alternative to Elasticsearch ClickBench - Benchmarks de performance üõ†Ô∏è Outils et int√©grations Support des √©v√©nements Kubernetes dans Vector - Issue GitHub en cours Kubernetes Event Exporter - Persistence des √©v√©nements K8s üí¨ Communaut√© et support Slack VictoriaMetrics - Canal #victorialogs Issues GitHub VictoriaLogs - Signaler des bugs ou demander des features ","link":"https://blog.ogenki.io/fr/post/series/observability/logs/","section":"post","tags":["observability"],"title":"`VictoriaLogs` : Et si la gestion des logs devenait simple et performante?"},{"body":"","link":"https://blog.ogenki.io/fr/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"https://blog.ogenki.io/fr/tags/observability/","section":"tags","tags":null,"title":"Observability"},{"body":"","link":"https://blog.ogenki.io/fr/series/observability/","section":"series","tags":null,"title":"Observability"},{"body":"","link":"https://blog.ogenki.io/fr/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://blog.ogenki.io/fr/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://blog.ogenki.io/fr/tags/","section":"tags","tags":null,"title":"Tags"},{"body":" Mieux inspecter nos applications üëÅÔ∏è Une fois que notre application est d√©ploy√©e, il est primordial de disposer d'indicateurs permettant d'identifier d'√©ventuels probl√®mes ainsi que suivre les √©volutions de performance. Parmi ces √©l√©ments, les m√©triques et les logs jouent un r√¥le essentiel en fournissant des informations pr√©cieuses sur le fonctionnement de l'application. En compl√©ment, il est souvent utile de mettre en place un tracing d√©taill√© pour suivre pr√©cis√©ment toutes les actions r√©alis√©es par l'application.\nDans cette s√©rie d'articles, nous allons explorer les diff√©rents aspects li√©s √† la supervision applicative. L'objectif √©tant d'analyser en d√©tail l'√©tat de nos applications, afin d'am√©liorer leur disponibilit√© et leurs performances, tout en garantissant une exp√©rience utilisateur optimale.\nLors d'un pr√©c√©dent article, nous avons vu comment collecter et visualiser des m√©triques. Celles-ci permettent d'analyser le comportement et les performances de nos applications. Il est tout aussi primordial de configurer des alertes afin d'√™tre notifi√© en cas d'anomalies sur notre plateforme.\nüéØ Objectifs üìä Comprendre les approches standards pour d√©finir des alertes efficaces : Les \u0026quot;Core Web Vitals\u0026quot; et les \u0026quot;Golden Signals\u0026quot; üîç D√©couvrir les langages PromQL et MetricsQL pour l'√©criture de r√®gles d'alerte ‚öôÔ∏è Configurer des alertes de fa√ßon d√©clarative avec VictoriaMetrics Operator üì± Router ces alertes vers diff√©rents canaux Slack üìã Pr√©requis La suite de cet article suppose que vous avez d√©j√† :\nUne instance VictoriaMetrics fonctionnelle d√©ploy√©e sur Kubernetes Un acc√®s √† un workspace Slack pour les notifications La mise en place d'alertes pertinentes est un √©l√©ment crucial de toute strat√©gie d'observabilit√©. Cependant, d√©finir des seuils appropri√©s et √©viter la fatigue li√©e aux alertes n√©cessite une approche r√©fl√©chie et m√©thodique.\nNous allons voir dans cet article qu'il est tr√®s simple de positionner des seuils au del√† desquels nous serions notifi√©s. Cependant faire en sorte que ces alertes soient pertinentes n'est pas toujours √©vident.\nüîç Qu'est ce qu'une bonne alerte? Une alerte correctement configur√©e permet d'identifier et de r√©soudre les probl√®mes au sein de notre syst√®me de mani√®re proactive, avant qu'ils ne s'aggravent. Des alertes efficaces doivent:\nSignaler des probl√®mes n√©cessitant une intervention imm√©diate. Etre d√©clench√©es au bon moment: suffisamment t√¥t pour pr√©venir un impact sur les utilisateurs, sans toutefois √™tre trop fr√©quentes au point de provoquer une fatigue li√©e aux alertes. Indiquer la cause racine ou la zone n√©cessitant une investigation. Pour ce faire il est recommand√© d'effectuer une analyse permettant la priorisation d'indicateurs pertinents, avec un r√©el impact sur le m√©tier (SLIs). Il est donc important de se focaliser sur un nombre maitris√© d'indicateurs √† surveiller. Il existe pour cela des approches qui permettent de mettre en oeuvre une supervision efficace de nos syst√®mes. Ici nous allons nous pencher sur 2 mod√®les d'alerte reconnus: Les Core Web Vitals et les Golden Signals.\nüåê Les \u0026quot;Core Web Vitals\u0026quot; Les Core Web Vitals sont des m√©triques d√©velopp√©es par Google pour √©valuer l'exp√©rience utilisateur sur les applications web. Ils mettent en √©vidence des indicateurs li√©s √† la satisfaction des utilisateurs finaux et permettent de garantir que notre application offre de bonnes performances pour les utilisateurs r√©els. Ces m√©triques se concentrent sur trois aspects principaux :\nLargest Contentful Paint (LCP), Temps de chargement de la page : Le LCP mesure le temps n√©cessaire pour que le plus grand √©l√©ment de contenu visible sur une page web (par exemple, une image, une vid√©o ou un large bloc de texte) soit enti√®rement rendu dans la fen√™tre d'affichage. Un bon LCP se situe en dessous de 2,5 secondes.\nInteraction to Next Paint (INP), R√©activit√© : L'INP √©value la r√©activit√© d'une page web en mesurant la latence de toutes les interactions utilisateur, telles que les clics, les taps et les entr√©es clavier, etc... Elle refl√®te le temps n√©cessaire pour qu'une page r√©agisse visuellement √† une interaction, c'est-√†-dire le d√©lai avant que le navigateur affiche le prochain rendu apr√®s une action de l'utilisateur. un bon INP doit √™tre inf√©rieur √† 200 millisecondes\nCumulative Layout Shift (CLS), Stabilit√© visuelle : Le CLS √©value la stabilit√© visuelle en quantifiant les d√©calages de mise en page inattendus sur une page, lorsque des √©l√©ments se d√©placent pendant le chargement ou l'interaction. Un bon score CLS est inf√©rieur ou √©gal √† 0,1.\nLa performance d'un site web est consid√©r√©e satisfaisante si elle atteint les seuils d√©crits ci-dessus au 75·µâ percentile, favorisant ainsi une bonne exp√©rience utilisateur et, par cons√©quent, une meilleure r√©tention et un meilleur r√©f√©rencement (SEO).\nAttention aux alertes sur les Core Web Vitals L'ajout d'alertes sp√©cifiques sur ces m√©triques doit √™tre m√ªrement r√©fl√©chi. Contrairement aux indicateurs op√©rationnels classiques, tels que la disponibilit√© ou le taux d'erreurs, qui refl√®tent directement la stabilit√© du syst√®me, les Web Vitals d√©pendent de nombreux facteurs externes, comme les conditions r√©seau des utilisateurs ou leurs appareils, rendant les seuils plus complexes √† surveiller efficacement.\nPour √©viter une surcharge d'alertes inutiles, ces alertes doivent uniquement cibler des d√©gradations significatives. Par exemple, une augmentation soudaine du CLS (stabilit√© visuelle) ou une d√©t√©rioration continue du LCP (temps de chargement) sur plusieurs jours peuvent indiquer des probl√®mes importants n√©cessitant une intervention.\nEnfin, ces alertes n√©cessitent des outils adapt√©s, comme le RUM (Real User Monitoring) pour les donn√©es r√©elles ou le Synthetic Monitoring pour des tests simul√©s, qui requi√®rent une solution sp√©cifique non abord√©e dans cet article.\n‚ú® Les \u0026quot;Golden Signals\u0026quot; Les Golden Signals sont un ensemble de quatre indicateurs cl√©s, largement utilis√©s dans le domaine de la supervision des syst√®mes et des applications, notamment avec des outils comme Prometheus. Ces signaux permettent de surveiller la sant√© et la performance des applications de mani√®re efficace. Ils sont particuli√®rement appropri√©s dans le contexte d'une architecture distribu√©e:\nLa Latence ‚è≥: Elle inclut √† la fois le temps des requ√™tes r√©ussies et le temps des requ√™tes √©chou√©es. La latence est cruciale car une augmentation du temps de r√©ponse peut indiquer des probl√®mes de performance.\nLe Trafic üì∂: Il peut √™tre mesur√©e en termes de nombre de requ√™tes par seconde, de d√©bit de donn√©es, ou d'autres m√©triques qui expriment la charge du syst√®me.\nLes erreurs ‚ùå: Il s'agit du taux d'√©chec des requ√™tes ou des transactions. Cela peut inclure des erreurs d'application, des erreurs d'infrastructure ou toute situation o√π une requ√™te ne s'est pas termin√©e correctement (par exemple, des r√©ponses HTTP 5xx ou des requ√™tes rejet√©es).\nLa saturation üìà: C'est une mesure de l'utilisation des ressources du syst√®me, comme le CPU, la m√©moire ou la bande passante r√©seau. La saturation indique √† quel point le syst√®me est proche de ses limites. Un syst√®me satur√© peut entra√Æner des ralentissements ou des pannes.\nCes Golden Signals sont essentiels car ils permettent de concentrer la surveillance sur les aspects critiques qui peuvent rapidement affecter l'exp√©rience utilisateur ou la performance globale du syst√®me. Avec Prometheus, ces signaux sont souvent surveill√©s via des m√©triques sp√©cifiques pour d√©clencher des alertes lorsque certains seuils sont d√©pass√©s.\nD\u0026#39;autres m√©thodes et indicateurs J'ai √©voqu√© ici 2 m√©thodologies qui, je trouve, sont un bon point de d√©part pour ajuster au mieux notre syst√®me d'alerting. Ceci-dit il en existe d'autres, chacune avec leurs sp√©cificit√©s. On peut ainsi citer USE ou RED.\nDe m√™me, au-del√† des Core Web Vitals pr√©sent√©s plus haut, d'autres m√©triques web comme FCP (First Contentful Paint) ou TTFB (Time To First Byte) peuvent s'av√©rer utiles selon vos besoins sp√©cifiques.\nLe choix des m√©triques √† surveiller d√©pendra de votre contexte et de vos objectifs. L'essentiel est de garder √† l'esprit qu'une bonne strat√©gie d'alerting repose sur un ensemble cibl√© d'indicateurs pertinents üéØ\nVous l'aurez compris: Definir des alertes √ßa se r√©fl√©chit! Maintenant entrons dans le concret et voyons comment d√©finir des seuils √† partir de nos m√©triques.\nüíª Exprimer des requ√™tes avec PromQL/MetricsQL Les m√©triques collect√©es avec Prometheus peuvent √™tre requet√©es avec un langage sp√©cifique appel√© PromQL (Prometheus Query Language). Ce langage permet d'extraire des donn√©es de supervision, d'effectuer des calculs, d'agr√©ger les r√©sultats, d'appliquer des filtres, mais aussi de configurer des alertes.\n(‚ÑπÔ∏è Se r√©f√©rer au pr√©c√©dent article pour comprendre ce que l'on entend par m√©trique.)\nPromQL est un langage puissant dont voici quelques exemples simples appliqu√©s aux m√©triques expos√©es par un serveur web Nginx :\nNombre total de requ√™tes trait√©es (nginx_http_requests_total) :\n1nginx_http_requests_total Nombre moyen de requ√™tes par seconde sur une fen√™tre de 5 minutes.\n1rate(nginx_http_requests_total[5m]) Nombre de requ√™tes HTTP par seconde retournant un code d'erreur 5xx sur les 5 derni√®res minutes.\n1rate(nginx_http_requests_total{status=~\u0026#34;5..\u0026#34;}[5m]) Nombre de requ√™tes par seconde, agr√©g√© par pod et filtr√© sur le namespace \u0026quot;myns\u0026quot; sur les 5 derni√®res minutes.\n1sum(rate(nginx_http_requests_total{namespace=\u0026#34;myns\u0026#34;}[5m])) by (pod) üí° Dans les exemples ci-dessus, nous avons mis en √©vidence deux Golden Signals : le trafic üì∂ et les erreurs ‚ùå.\nMetricsQL est le langage utilis√© avec VictoriaMetrics. Il se veut compatible avec PromQL avec de l√©g√®res diff√©rences qui permettent de faciliter l'√©criture de requ√™tes complexes. Il apporte aussi de nouvelles fonctions dont voici quelques exemples:\nhistogram(q): Cette fonction calcule un histogramme pour chaque groupe de points ayant le m√™me horodatage, ce qui est utile pour visualiser un grand nombre de s√©ries temporelles (timeseries) via un heatmap. Pour cr√©er un histogramme des requ√™tes HTTP\n1histogram(rate(vm_http_requests_total[5m])) quantiles(\u0026quot;phiLabel\u0026quot;, phi1, ..., phiN, q): Utilis√© pour extraire plusieurs quantiles (ou percentiles) d'une m√©trique donn√©e . Pour calculer les 50e, 90e et 99e percentiles du taux de requ√™tes HTTP\n1quantiles(\u0026#34;percentile\u0026#34;, 0.5, 0.9, 0.99, rate(vm_http_requests_total[5m])) Afin de pouvoir tester ses requ√™tes, vous pouvez utiliser la d√©mo fournie par VictoriaMetrics: https://play.victoriametrics.com\nüõ†Ô∏è Configurer des alertes avec VictoriaMetrics Operator VictoriaMetrics propose deux composants essentiels pour la gestion des alertes :\nVMAlert : responsable de l'√©valuation des r√®gles d'alerte AlertManager : g√®re le routage et la distribution des notifications VMAlert : Le moteur d'√©valuation des r√®gles VMAlert est le composant qui √©value en continu les r√®gles d'alerte d√©finies. Il supporte deux types de r√®gles :\nRecording Rules üìä Les recording rules permettent de pr√©-calculer des expressions PromQL complexes et de les stocker comme nouvelles m√©triques pour optimiser les performances.\nAlerting Rules üö® Les alerting rules d√©finissent les conditions qui d√©clenchent des alertes lorsque certains seuils sont d√©pass√©s.\nDans la suite de cet article, nous allons nous concentrer sur les alerting rules qui sont essentielles pour la d√©tection proactive des probl√®mes.\nDes exemples concrets Le reste de cet article est issu d'un ensemble de configurations que vous pouvez retrouver dans le repository Cloud Native Ref. Il y est fait usage de nombreux op√©rateurs et notamment celui pour VictoriaMetrics.\nL'ambition de ce projet est de pouvoir d√©marrer rapidement une plateforme compl√®te qui applique les bonnes pratiques en terme d'automatisation, de supervision, de s√©curit√© etc. Les commentaires et contributions sont les bienvenues üôè D√©clarer une r√®gle d'alerting avec VMRule Nous avons vu pr√©c√©demment que VictoriaMetrics fournit un op√©rateur Kubernetes qui permet de g√©rer les diff√©rents composants de mani√®re d√©clarative. Parmi les ressources personnalis√©es (Custom Resources) disponibles, la VMRule permet de d√©finir des r√®gles d'alertes et d'enregistrement (recording rules).\nSi vous avez d√©j√† utilis√© l'op√©rateur Prometheus, vous retrouverez une syntaxe tr√®s similaire car l'op√©rateur VictoriaMetrics est compatible avec les custom resources de Prometheus. (Ce qui nous permet de faciliter la migration üòâ).\nPrenons un exemple concret avec une VMRule qui surveille l'√©tat de sant√© des ressources Flux :\nflux/observability/vmrule.yaml\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMRule 3metadata: 4 labels: 5 prometheus-instance: main 6 name: flux-system 7 namespace: flux-system 8spec: 9 groups: 10 - name: flux-system 11 rules: 12 - alert: FluxReconciliationFailure 13 annotations: 14 message: Flux resource has been unhealthy for more than 5m 15 description: \u0026#34;{{ $labels.kind }} {{ $labels.exported_namespace }}/{{ $labels.name }} reconciliation has been failing for more than ten minutes.\u0026#34; 16 runbook_url: \u0026#34;https://fluxcd.io/flux/cheatsheets/troubleshooting/\u0026#34; 17 dashboard: \u0026#34;https://grafana.priv.${domain_name}/dashboards\u0026#34; 18 expr: max(gotk_reconcile_condition{status=\u0026#34;False\u0026#34;,type=\u0026#34;Ready\u0026#34;}) by (exported_namespace, name, kind) + on(exported_namespace, name, kind) (max(gotk_reconcile_condition{status=\u0026#34;Deleted\u0026#34;}) by (exported_namespace, name, kind)) * 2 == 1 19 for: 10m 20 labels: 21 severity: warning Il est recommand√© de suivre quelques bonnes pratiques pour donner le maximum de contexte afin d'identifier rapidement la cause racine.\nNommage et Organisation üìù\nUtiliser des noms descriptifs pour les r√®gles, comme FluxReconciliationFailure Grouper les r√®gles par composant Flux (ex: flux-system, flux-controllers) Documenter les conditions de r√©conciliation dans les annotations Seuils et Dur√©es ‚è±Ô∏è\nAjuster la dur√©e d'√©valuation de l'alerte for: 10m pour √©viter les faux positifs Adapter les seuils selon le type de ressource supervis√©es. Consid√©rer des dur√©es diff√©rentes selon l'environnement (prod/staging) Labels et Routing üè∑Ô∏è\nAjouter des labels pour le routage selon le contexte. Mon exemple n'est pas tr√®s pouss√© car il s'agit d'une configuration de d√©mo. Mais nous pourrions tr√®s bien ajouter un label team afin de router vers la bonne √©quipe, ou avoir une politique de routage diff√©rente selon l'environnement. 1labels: 2 severity: [critical|warning|info] 3 team: [sre|dev|ops] 4 environment: [prod|staging|dev] L'importance des annotations üìö\nLes annotations permettent d'ajouter divers informations sur le contexte de l'alerte\nUne description claire du probl√®me de r√©conciliation Le lien vers le runbook de troubleshooting Flux Le lien vers le dashboard Grafana d√©di√© Requ√™te PromQL üîç 1expr: | 2 max(gotk_reconcile_condition{status=\u0026#34;False\u0026#34;,type=\u0026#34;Ready\u0026#34;}) by (exported_namespace, name, kind) 3 + on(exported_namespace, name, kind) 4 (max(gotk_reconcile_condition{status=\u0026#34;Deleted\u0026#34;}) by (exported_namespace, name, kind)) * 2 == 1 Cette alerte se d√©clenchera si Flux n'arrive pas √† r√©concilier une ressource ou si une ressource est supprim√©e alors qu'elle ne devrait pas l'√™tre. Dans le d√©tail: La m√©trique gotk_reconcile_condition expose l'√©tat de sant√© des ressources Flux Le filtre status=\u0026quot;False\u0026quot;,type=\u0026quot;Ready\u0026quot; identifie les ressources qui ne sont pas dans l'√©tat \u0026quot;Ready\u0026quot; La deuxi√®me partie de l'expression (status=\u0026quot;Deleted\u0026quot;) d√©tecte les ressources qui ont √©t√© supprim√©es L'op√©ration + on(...) (...) * 2 == 1 combine ces conditions pour d√©clencher une alerte quand : Une ressource n'est pas \u0026quot;Ready\u0026quot; (premi√®re partie = 1) OU une ressource a √©t√© supprim√©e de fa√ßon inattendue (deuxi√®me partie = 1) Le max et le by permettent de regrouper les alertes par namespace, nom et type de ressource üí¨ Int√©gration avec Slack Nous pouvons envoyer ces alertes au travers de diff√©rents canaux ou outils. Nous pouvons citer Grafana OnCall, Opsg√©nie, Pagerduty ou simplement des emails et j'en passe...\nDans notre exemple nous envoyons des notifications vers un canal Slack. Nous allons donc d'abord cr√©er une application Slack et r√©cup√©rer le token g√©n√©r√© avant de configurer VictoriaMetrics.\nConfiguration de l'Application Slack Cr√©ation de l'Application üîß\nCela se fait sur https://api.slack.com/apps Cliquer sur \u0026quot;Create New App\u0026quot; Choisir \u0026quot;From scratch\u0026quot; Nommer l'application (ex: \u0026quot;AlertManager\u0026quot;) S√©lectionner le workspace cible Configuration des Permissions üîë Dans \u0026quot;OAuth \u0026amp; Permissions\u0026quot;, ajouter les scopes suivants :\nchat:write (Requis) chat:write.public (Pour poster dans les canaux publics) channels:read (Pour lister les canaux) groups:read (Pour les groupes priv√©s) Your browser does not support the video tag. Installation et Token üéüÔ∏è Installer l'application dans le workspace Copier le \u0026quot;Bot User OAuth Token\u0026quot; (commence par xoxb-) Stocker le token de mani√®re s√©curis√©e. Dans notre exemple, le secret est r√©cup√©r√© depuis AWS¬†Secrets Manager en utilisant l'op√©rateur External Secrets. Configuration d'AlertManager pour Slack Le reste de la configuration se fait gr√¢ce √† des values Helm afin de param√®trer AlertManager\nobservability/base/victoria-metrics-k8s-stack/vm-common-helm-values-configmap.yaml\nR√©f√©rencer le point de montage du secret contenant le token 1 alertmanager: 2 enabled: true 3 spec: 4 externalURL: \u0026#34;https://vmalertmanager-${cluster_name}.priv.${domain_name}\u0026#34; 5 secrets: 6 - \u0026#34;victoria-metrics-k8s-stack-alertmanager-slack-app\u0026#34; 7 config: 8 global: 9 slack_api_url: \u0026#34;https://slack.com/api/chat.postMessage\u0026#34; 10 http_config: 11 authorization: 12 credentials_file: /etc/vm/secrets/victoria-metrics-k8s-stack-alertmanager-slack-app/token Le secret victoria-metrics-k8s-stack-alertmanager-slack-app contenant le token est r√©cup√©r√© depuis AWS Secrets Manager. Dans la configuration il faut r√©f√©rencer le point de montage de ce secret (config.globl.http_config.authorization)\nExplication du routage 1 route: 2 group_by: 3 - cluster 4 - alertname 5 - severity 6 - namespace 7 group_interval: 5m 8 group_wait: 30s 9 repeat_interval: 3h 10 receiver: \u0026#34;slack-monitoring\u0026#34; 11 routes: 12 - matchers: 13 - alertname =~ \u0026#34;InfoInhibitor|Watchdog|KubeCPUOvercommit\u0026#34; 14 receiver: \u0026#34;blackhole\u0026#34; 15 receivers: 16 - name: \u0026#34;blackhole\u0026#34; 17 - name: \u0026#34;slack-monitoring\u0026#34; Groupement des alertes : Le groupement des alertes est essentiel pour r√©duire le bruit et am√©liorer la lisibilit√© des notifications. Sans groupement, chaque alerte serait envoy√©e individuellement, ce qui pourrait rapidement devenir ing√©rable. Les crit√®res de groupement choisis permettent une organisation logique:\ngroup_by d√©fini les labels sur lesquels grouper les alertes. group_wait: D√©lai de 30s avant l'envoi initial d'une notification pour permettre le groupement group_interval: Intervalle de 5m entre les notifications pour un m√™me groupe repeat_interval: Les alertes ne sont r√©p√©t√©es que toutes les 3h pour √©viter le spam Receivers: Les receivers sont des composants d'AlertManager qui d√©finissent comment et o√π envoyer les notifications d'alerte. Ils peuvent √™tre configur√©s pour diff√©rents canaux de communication comme Slack, Email, PagerDuty, etc. Dans notre configuration:\nslack-monitoring: Receiver principal qui envoie les alertes vers un canal Slack sp√©cifique avec un formatage personnalis√© blackhole: Receiver sp√©cial qui \u0026quot;absorbe\u0026quot; les alertes sans les transmettre nulle part, utile pour filtrer les alertes non pertinentes ou purement techniques Exemple de routage Selon l'organisation et les proc√©dures en vigueur dans l'entreprise, nous pouvons d√©finir un routage cibl√© des alertes. Supposons, par exemple, que nous souhaitons router les alertes critiques des environnements de production et s√©curit√© vers l'√©quipe d'astreinte :\n1 - matchers: 2 - environment =~ \u0026#34;prod|security\u0026#34; 3 - team = \u0026#34;oncall\u0026#34; 4 receiver: \u0026#34;pagerduty\u0026#34; Templates Personnalis√©s üìù Ce bloc de configuration d√©finit un receiver Slack pour AlertManager qui utilise les templates Monzo. Les templates Monzo sont un ensemble de templates de notification qui permettent de formater les alertes Slack de mani√®re √©l√©gante et informative.\n1 alertmanager: 2 config: 3 receivers: 4 - name: \u0026#34;slack-monitoring\u0026#34; 5 slack_configs: 6 - channel: \u0026#34;#alerts\u0026#34; 7 send_resolved: true 8 title: \u0026#39;{{ template \u0026#34;slack.monzo.title\u0026#34; . }}\u0026#39; 9 icon_emoji: \u0026#39;{{ template \u0026#34;slack.monzo.icon_emoji\u0026#34; . }}\u0026#39; 10 color: \u0026#39;{{ template \u0026#34;slack.monzo.color\u0026#34; . }}\u0026#39; 11 text: \u0026#39;{{ template \u0026#34;slack.monzo.text\u0026#34; . }}\u0026#39; 12 actions: 13 - type: button 14 text: \u0026#34;Runbook :green_book:\u0026#34; 15 url: \u0026#34;{{ (index .Alerts 0).Annotations.runbook_url }}\u0026#34; 16 - type: button 17 text: \u0026#34;Query :mag:\u0026#34; 18 url: \u0026#34;{{ (index .Alerts 0).GeneratorURL }}\u0026#34; 19 - type: button 20 text: \u0026#34;Dashboard :grafana:\u0026#34; 21 url: \u0026#34;{{ (index .Alerts 0).Annotations.dashboard }}\u0026#34; 22 - type: button 23 text: \u0026#34;Silence :no_bell:\u0026#34; 24 url: \u0026#39;{{ template \u0026#34;__alert_silence_link\u0026#34; . }}\u0026#39; 25 - type: button 26 text: \u0026#39;{{ template \u0026#34;slack.monzo.link_button_text\u0026#34; . }}\u0026#39; 27 url: \u0026#34;{{ .CommonAnnotations.link_url }}\u0026#34; Voici un exemple de notification g√©n√©r√©e avec ce format. Il permet notamment d'ajouter des boutons d'action pour visualiser le dashboard Grafana üìä, afficher le runbook üìö ou mettre en silence l'alerte üîï.\nüëÄ Visualiser et interagir avec les alertes La visualisation et la gestion des alertes sont des aspects essentiels d'un syst√®me d'alerting efficace. VictoriaMetrics et son √©cosyst√®me offrent plusieurs options pour interagir avec vos alertes :\nAlertmanager : La solution standard Alertmanager est le composant standard qui permet de :\nVisualiser l'√©tat actuel des alertes Configurer le routage des notifications G√©rer les silences (mise en pause temporaire d'alertes) Consulter l'historique des alertes VMUI : L'interface native de VictoriaMetrics VMUI offre une interface simplifi√©e pour :\nConsulter les alertes actives Visualiser les r√®gles d'alertes Afficher les m√©triques associ√©es Grafana Alerting : Une solution compl√®te Bien que nous utilisions Alertmanager pour la d√©finition et le routage des alertes, Grafana Alerting offre une solution alternative compl√®te qui permet de :\nCentraliser la gestion des alertes Visualiser les alertes dans le contexte des dashboards Configurer des r√®gles d'alertes directement depuis l'interface G√©rer les silences et les notifications Choisir la bonne interface Le choix de l'interface d√©pend de vos besoins sp√©cifiques :\nAlertmanager est id√©al pour la gestion op√©rationnelle des alertes VMUI est parfait pour une vue rapide et simple Grafana Alerting est recommand√© si vous souhaitez une solution int√©gr√©e avec vos dashboards üéØ Conclusion La d√©finition d'alertes pertinentes est un √©l√©ment cl√© de toute strat√©gie d'observabilit√©. L'op√©rateur VictoriaMetrics, avec ses ressources personnalis√©es Kubernetes comme VMRule, simplifie grandement la mise en place d'un syst√®me d'alerting efficace. La configuration d√©clarative permet de d√©finir rapidement des r√®gles d'alerte complexes tout en maintenant une excellente lisibilit√© et maintenabilit√© du code.\nCependant, la configuration technique des alertes, m√™me avec des outils aussi puissants que VictoriaMetrics, ne suffit pas √† elle seule. Une strat√©gie d'alerting efficace doit s'int√©grer dans un cadre organisationnel plus large :\nD√©finition claire des proc√©dures d'astreinte Identification des √©quipes responsables de la surveillance Mise en place de runbooks et proc√©dures de r√©ponse aux incidents Adaptation des canaux de notification selon la criticit√© et le contexte Pour aller plus loin üöÄ D√©couvrez comment int√©grer ces alertes avec d'autres composants de votre stack d'observabilit√© dans les prochains articles de cette s√©rie, notamment la corr√©lation avec les logs et le tracing distribu√©.\nüîñ References https://web.dev/articles/vitals https://medium.com/@romanhavronenko/victoriametrics-promql-compliance-d4318203f51e https://victoriametrics.com/blog/alerting-recording-rules-alertmanager/ https://docs.victoriametrics.com/vmalert/ ","link":"https://blog.ogenki.io/fr/post/series/observability/alerts/","section":"post","tags":["observability"],"title":"`VictoriaMetrics` : Des alertes efficaces, de la th√©orie √† la pratique üõ†Ô∏è"},{"body":" Comment se portent nos applications? üëÅÔ∏è Une fois que notre application est d√©ploy√©e, il est primordial de disposer d'indicateurs permettant d'identifier d'√©ventuels probl√®mes ainsi que suivre les √©volutions de performance. Parmi ces √©l√©ments, les m√©triques et les logs jouent un r√¥le essentiel en fournissant des informations pr√©cieuses sur le fonctionnement de l'application. En compl√©ment, il est souvent utile de mettre en place un tracing d√©taill√© pour suivre pr√©cis√©ment toutes les actions r√©alis√©es par l'application.\nDans cette s√©rie d'articles, nous allons explorer les diff√©rents aspects li√©s √† la supervision applicative. L'objectif √©tant d'analyser en d√©tail l'√©tat de nos applications, afin d'am√©liorer leur disponibilit√© et leurs performances, tout en garantissant une exp√©rience utilisateur optimale.\nCe premier volet est consacr√© √† la collecte et la visualisation des m√©triques. Nous allons d√©ployer une solution performante et √©volutive pour acheminer ces m√©triques vers un syst√®me de stockage fiable et p√©renne. Puis nous allons voir comment les visualiser afin de les analyser.\n‚ùì Qu'est ce qu'une m√©trique Definition Avant de collecter cette dite \u0026quot;m√©trique\u0026quot;, penchons-nous d'abord sur sa d√©finition et ses sp√©cificit√©s: Une m√©trique est une donn√©e mesurable qui permet de suivre l'√©tat et les performances d'une application. Ces donn√©es sont g√©n√©ralement des chiffres collect√©s √† intervals r√©guliers, on peut citer par exemple le nombre de requ√™tes, la quantit√© de m√©moire ou le taux d'erreurs.\nEt quand on s'int√©resse au domaine de la supervision, il est difficile de passer √† cot√© de Prometheus. Ce projet a notamment permis le l'√©mergence d'un standard qui d√©finit la mani√®re dont on expose des m√©triques appel√© OpenMetrics dont voici le format.\nTime Series: Une time series unique est la combinaison du nom de la m√©trique ainsi que ses labels, par cons√©quent request_total{code=\u0026quot;200\u0026quot;} et request_total{code=\u0026quot;500\u0026quot;} sont bien 2 time series distinctes.\nLabels: On peut associer des labels √† une m√©trique afin de la caract√©riser plus pr√©cis√©ment. Ils sont ajout√©es √† la suite du nom de la m√©trique en utilisant des accolades. Bien qu'ils soient optionnels, nous les retrouverons tr√®s souvent, notamment dans sur un cluster Kubernetes (pod, namespace...).\nValue: La value repr√©sente une donn√©e num√©rique recueillie √† un moment donn√© pour une time series sp√©cifique. Selon le type de m√©trique, il s'agit d'une valeur qui peut √™tre mesur√©e ou compt√©e afin de suivre l'√©volution d'un indicateur dans le temps.\nTimestamp: Indique quand la donn√©e a √©t√© collect√©e (format epoch √† la milliseconde). S'il n'est pas pr√©sent, Il est ajout√© au moment o√π la m√©trique est r√©cup√©r√©e.\nCette ligne compl√®te repr√©sente ce que l'on appelle un raw sample.\nAttention √† la cardinalit√©! Plus il y a de labels, plus les combinaisons possibles augmentent, et par cons√©quent, le nombre de timeseries. Le nombre total de combinaisons est appel√© cardinalit√©. Une cardinalit√© √©lev√©e peut avoir un impact significatif sur les performances, notamment en termes de consommation de m√©moire et de ressources de stockage.\nUne cardinalit√© √©lev√©e se produit √©galement lorsque de nouvelles m√©triques sont cr√©√©es fr√©quemment. Ce ph√©nom√®ne, appel√© churn rate, indique le rythme auquel des m√©triques apparaissent puis disparaissent dans un syst√®me. Dans le contexte de Kubernetes, o√π des pods sont r√©guli√®rement cr√©√©s et supprim√©s, ce churn rate peut contribuer √† l'augmentation rapide de la cardinalit√©.\nLa collecte en bref Maintenant que l'on sait ce qu'est une m√©trique, voyons comment elles sont collect√©es. La plupart des solutions modernes exposent un endpoint qui permet de \u0026quot;scraper\u0026quot; les m√©triques, c'est-√†-dire de les interroger √† intervalle r√©gulier. Par exemple, gr√¢ce au SDK Prometheus, disponible dans la plupart des langages de programmation, il est facile d'int√©grer cette collecte dans nos applications.\nIl est d'ailleurs important de souligner que Prometheus utilise, en r√®gle g√©n√©rale, un mod√®le de collecte en mode \u0026quot;Pull\u0026quot;, o√π le serveur interroge p√©riodiquement les services pour r√©cup√©rer les m√©triques via ces endpoints expos√©s. Cette approche permet de mieux contr√¥ler la fr√©quence de collecte des donn√©es et d'√©viter de surcharger les syst√®mes. On distinguera donc le mode \u0026quot;Push\u0026quot; o√π ce sont les applications qui envoient directement les informations.\nIllustrons cela concr√®tement avec un serveur web Nginx. Ce serveur est install√© √† partir du chart Helm en activant le support de Prometheus. Ici le param√®tre metrics.enabled=true permet d'ajouter un chemin qui expose les m√©triques.\n1helm install ogenki-nginx bitnami/nginx --set metrics.enabled=true Ainsi, nous pouvons par exemple r√©cup√©rer via un simple appel http un nombre de m√©triques important\n1kubectl port-forward svc/ogenki-nginx metrics \u0026amp; 2Forwarding from 127.0.0.1:9113 -\u0026gt; 9113 3 4curl -s localhost:9113/metrics 5... 6# TYPE promhttp_metric_handler_requests_total counter 7promhttp_metric_handler_requests_total{code=\u0026#34;200\u0026#34;} 257 8... La commande curl √©tait juste un exemple, La collecte est, en effet r√©alis√©e par un syst√®me dont la responsabilit√© est de stocker ces donn√©es pour pouvoir ensuite les exploiter et les analyser. ‚ÑπÔ∏è Quand on utilise Prometheus, un composant suppl√©mentaire est n√©cessaire pour pouvoir pousser des m√©triques depuis les applications: PushGateway.\nDans cet article, j'ai choisi de vous faire d√©couvrir VictoriaMetrics.\n‚ú® VictoriaMetrics: Un h√©ritier de Prometheus Tout comme Prometheus, VictoriaMetrics est une base de donn√©es Time Series (TSDB). Celles-cis sont con√ßues pour suivre et stocker des √©v√©nements qui √©voluent au fil du temps. M√™me si VictoriaMetrics est apparue quelques ann√©es apr√®s Prometheus, elles partagent pas mal de points communs : ce sont toutes deux des bases de donn√©es open-source sous licence Apache 2.0, d√©di√©es au traitement des time series. VictoriaMetrics reste enti√®rement compatible avec Prometheus, en utilisant le m√™me format de m√©triques, OpenMetrics, et un support total du langage de requ√™tes PromQL.\nCes deux projets sont d‚Äôailleurs tr√®s actifs, avec des communaut√©s dynamiques et des contributions r√©guli√®res venant de nombreuses entreprises comme on peut le voir ici.\nExplorons maintenant les principales diff√©rences et les raisons qui pourraient pousser √† choisir VictoriaMetrics :\nStockage et compression efficace : C'est probablement l'un des arguments majeurs, surtout quand on g√®re un volume important de donn√©es ou qu'on souhaite les conserver √† long terme. Avec Prometheus, il faut ajouter un composant suppl√©mentaire, comme Thanos, pour cela. VictoriaMetrics, en revanche, dispose d'un moteur de stockage optimis√© qui regroupe et optimise les donn√©es avant de les √©crire sur disque. De plus, il utilise des algorithmes de compression tr√®s puissants, offrant une utilisation de l'espace disque bien plus efficace que Prometheus.\nEmpreinte m√©moire : VictoriaMetrics consommerait jusqu'√† 7 fois moins de m√©moire qu'une solution bas√©e sur Prometheus. Cela dit, les benchmarks disponibles en ligne commencent √† dater, et Prometheus a b√©n√©fici√© de nombreuses optimisations de m√©moire.\nMetricsQL : VictoriaMetrics √©tend le langage PromQL avec de nouvelles fonctions. Ce language est aussi con√ßu pour √™tre plus performant, notamment sur un large dataset.\nArchitecture modulaire: VictoriaMetrics peut √™tre d√©ploy√© en 2 modes: \u0026quot;Single\u0026quot; ou \u0026quot;Cluster\u0026quot;. Selon le besoin on pourra aller bien plus loin: On verra cela dans la suite de l'article.\nEt bien d'autres...: Les arguments ci-dessus sont ceux que j'ai retenu mais il y en a d'autres. VictoriaMetrics peut aussi √™tre utilis√© en mode Push, configur√© pour du multitenant et d'autres fonctions que l'on retrouvera dans la version entreprise.\nCase studies: ce qu\u0026#39;ils en disent Sur le site de VictoriaMetrics, on trouve de nombreux t√©moignages et retours d'exp√©rience d'entreprises ayant migr√© depuis d'autres syst√®mes (comme Thanos, InfluxDB, etc.). Certains exemples sont particuli√®rement instructifs, notamment ceux de Roblox, Razorpay ou Criteo, qui g√®rent un volume tr√®s important de m√©triques.\nüîé Une architecture modulaire et scalable D√©ploiement: GitOps et Op√©rateurs Kubernetes Le reste de cet article est issu d'un ensemble de configurations que vous pouvez retrouver dans le repository Cloud Native Ref. Il y est fait usage de nombreux op√©rateurs et notamment ceux pour VictoriaMetrics et pour Grafana.\nL'ambition de ce projet est de pouvoir d√©marrer rapidement une plateforme compl√®te qui applique les bonnes pratiques en terme d'automatisation, de supervision, de s√©curit√© etc. Les commentaires et contributions sont les bienvenues üôè VictoriaMetrics peut √™tre d√©ploy√© de diff√©rentes mani√®res: Le mode par d√©faut est appel√© Single et, comme son nom l'indique, il s'agit de d√©ployer une instance unique qui g√®re la lecture, l'√©criture et le stockage. Il est d'ailleurs recommand√© de commencer par celui-ci car il est optimis√© et r√©pond √† la plupart des cas d'usage comme le pr√©cise ce paragraphe.\nLe mode Single La m√©thode de d√©ploiement choisie dans cet article fait usage du chart Helm victoria-metrics-k8s-stack qui configure de nombreuses ressources (VictoriaMetrics, Grafana, Alertmanager, quelques dashboards...). Voici un extrait de configuration Flux pour un mode Single\nobservability/base/victoria-metrics-k8s-stack/helmrelease-vmsingle.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2 2kind: HelmRelease 3metadata: 4 name: victoria-metrics-k8s-stack 5 namespace: observability 6spec: 7 releaseName: victoria-metrics-k8s-stack 8 chart: 9 spec: 10 chart: victoria-metrics-k8s-stack 11 sourceRef: 12 kind: HelmRepository 13 name: victoria-metrics 14 namespace: observability 15 version: \u0026#34;0.25.15\u0026#34; 16... 17 values: 18 vmsingle: 19 spec: 20 retentionPeriod: \u0026#34;1d\u0026#34; # Minimal retention, for tests only 21 replicaCount: 1 22 storage: 23 accessModes: 24 - ReadWriteOnce 25 resources: 26 requests: 27 storage: 10Gi 28 extraArgs: 29 maxLabelsPerTimeseries: \u0026#34;50\u0026#34; Lorsque l'ensemble des manifests Kubernetes sont appliqu√©s, on obtient l'architecture suivante:\nüîí Acc√®s priv√©: M√™me si cela ne fait pas vraiment partie des composants li√©s √† la collecte des m√©triques, j'ai souhait√© mettre en avant la fa√ßon dont on acc√®de aux diff√©rentes interfaces. J'ai en effet choisi de capitaliser sur Gateway API, que j'utilise depuis quelques temps et qui a fait l'objet de pr√©c√©dents articles. Une alternative serait d'utiliser un composant de VictoriaMetrics, VMAuth, qui peut servir de proxy pour l'autorisation et le routage des acc√®s mais Je n'ai pas retenu cette option pour le moment.\nüë∑ VMAgent: Un agent tr√®s l√©ger, dont la fonction principale est de r√©cup√©rer les m√©triques et de les acheminer vers une base de donn√©es compatible avec Prometheus. Par ailleurs, cet agent peut appliquer des filtres ou des transformations aux m√©triques avant de les transmettre. En cas d'indisponibilit√© de la destination ou en cas de manque de ressources, il peut mettre en cache les donn√©es sur disque. VMAgent dispose aussi d'une interface Web permettant de lister les \u0026quot;Targets\u0026quot; (Services qui sont scrap√©s)\nüî• VMAlert \u0026amp; VMAlertManager: Ce sont les composants charg√©s de notifier en cas de probl√®mes, d'anomalies. Je ne vais volontairement pas approfondir le sujet car cela fera l'objet d'un future acticle.\n‚öôÔ∏è VMsingle: Il s'agit de la base de donn√©es VictoriaMetrics d√©ploy√©e sous forme d'un pod unique qui prend en charge l'ensemble des op√©rations (lecture, √©criture et persistence des donn√©es).\nLorsque tous les pods sont d√©marr√©s, on peut acc√©der √† l'interface principale de VictoriaMetrics: VMUI. Elle permet de visualiser un grand nombre d'informations: √âvidemment nous pourrons parcourir les m√©triques scrap√©es, les requ√™tes les plus utilis√©es, les statistiques relatives √† la cardinalit√© et bien d'autres.\nYour browser does not support the video tag. La Haute disponibilit√© Pour ne jamais perdre de vue ce qui se passe sur nos applications, la solution de supervision doit toujours rester op√©rationnelle. Pour cela, tous les composants de VictoriaMetrics peuvent √™tre configur√©s en haute disponibilit√©. En fonction du niveau de redondance souhait√©, plusieurs options s'offrent √† nous.\nLa plus simple est d'envoyer les donn√©es √† deux instances Single, les donn√©es sont ainsi dupliqu√©es √† 2 endroits. De plus, on peut envisager de d√©ployer ces instances dans deux r√©gions diff√©rentes.\nIl est aussi recommand√© de redonder les agents VMAgent qui vont scraper les m√™mes services, afin de s'assurer qu'aucune donn√©e ne soit perdue.\nBien configurer la De-duplication Dans une telle architecture, √©tant donn√© que plusieurs VMAgents envoient des donn√©es et scrappent les m√™mes services, on se retrouve avec des m√©triques en double. La De-duplication dans VictoriaMetrics permet de ne conserver qu'une seule version lorsque deux raw samples sont identiques. Un param√®tre m√©rite une attention particuli√®re : -dedup.minScrapeInterval: Seule la version la plus r√©cente sera conserv√©e lorsque raw samples identiques sont trouv√©s dans cet intervale de temps.\nIl est aussi recommand√© de :\nConfigurer ce param√®tre avec une valeur √©gale au scrape_interval que l'on d√©finit dans la configuration Prometheus. Garder une valeur de scrape_interval identique pour tous les services scrapp√©s. Le sch√©ma ci-dessous montre l'une des nombreuses combinaisons possibles pour assurer une disponibilit√© optimale. ‚ö†Ô∏è Cependant, il faut tenir compte du surco√ªt, non seulement pour le stockage et le calcul, mais aussi pour les transferts r√©seau entre zones/r√©gions. Il est parfois plus judicieux d'avoir une bonne strat√©gie de sauvegarde et restauration üòÖ.\nLe mode Cluster Comme mentionn√© plus t√¥t, dans la plupart des cas, le mode Single est largement suffisant. Il a l'avantage d'√™tre simple √† maintenir et, avec du scaling vertical, il permet de r√©pondre √† quasiment tous les cas d'usage. Il existe aussi un mode Cluster, qui n'est pertinent que dans deux cas pr√©cis :\nBesoin de multitenant. Par exemple pour isoler plusieurs √©quipes ou clients. Si les limites du scaling vertical sont atteintes. Ma configuration permet de choisir entre l'un ou l'autre des modes:\nobservability/base/victoria-metrics-k8s-stack/kustomization.yaml\n1resources: 2... 3 4 - vm-common-helm-values-configmap.yaml 5 # Choose between single or cluster helm release 6 7 # VM Single 8 - helmrelease-vmsingle.yaml 9 - httproute-vmsingle.yaml 10 11 # VM Cluster 12 # - helmrelease-vmcluster.yaml 13 # - httproute-vmcluster.yaml Dans ce mode, on va s√©parer les fonctions de lecture, √©criture et de stockage en 3 services bien distincts.\n‚úèÔ∏è VMInsert: R√©partit les donn√©es sur les instances de VMStorage en utilisant du consistent hashing bas√© sur la time series (combinaison du nom de la m√©trique et de ses labels).\nüíæ VMStorage: Est charg√© d'√©crire les donn√©es sur disque et de retourner les donn√©es demand√©es par VMSelect.\nüìñ VMSelect: Pour chaque requ√™te va r√©cup√©rer les donn√©es sur les VMStorages.\nL'int√©r√™t principal de ce mode est √©videmment de pouvoir adapter le scaling en fonction du besoin. Par exemple, si on a besoin de plus de capacit√© en √©criture on va ajouter des replicas VMInsert.\nLe param√®tre initial, qui permet d'avoir un niveau de redondance minimum est replicationFactor √† 2. Voici un extrait des values Helm pour le mode cluster.\nobservability/base/victoria-metrics-k8s-stack/helmrelease-vmcluster.yaml\n1 vmcluster: 2 enabled: true 3 spec: 4 retentionPeriod: \u0026#34;10d\u0026#34; 5 replicationFactor: 2 6 vmstorage: 7 storage: 8 volumeClaimTemplate: 9 storageClassName: \u0026#34;gp3\u0026#34; 10 spec: 11 resources: 12 requests: 13 storage: 10Gi 14 resources: 15 limits: 16 cpu: \u0026#34;1\u0026#34; 17 memory: 1500Mi 18 affinity: 19 podAntiAffinity: 20 requiredDuringSchedulingIgnoredDuringExecution: 21 - labelSelector: 22 matchExpressions: 23 - key: \u0026#34;app.kubernetes.io/name\u0026#34; 24 operator: In 25 values: 26 - \u0026#34;vmstorage\u0026#34; 27 topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 28 topologySpreadConstraints: 29 - labelSelector: 30 matchLabels: 31 app.kubernetes.io/name: vmstorage 32 maxSkew: 1 33 topologyKey: topology.kubernetes.io/zone 34 whenUnsatisfiable: ScheduleAnyway 35 vmselect: 36 storage: 37 volumeClaimTemplate: 38 storageClassName: \u0026#34;gp3\u0026#34; \u0026#x2139;\u0026#xfe0f; On notera que certains param√®tres font partie des bonnes pratiques Kubernetes, notamment lorsque l'on utilise Karpenter: topologySpreadConstraints permet de r√©partir sur diff√©rentes zones, podAntiAffinity pour √©viter que 2 pods pour le m√™me service se retrouvent sur le m√™me noeud.\nüõ†Ô∏è La configuration Ok, c'est cool, VictoriaMetrics est maintenant d√©ploy√© üëè. Il est temps de configurer la supervision de nos applications, et pour √ßa, on va s'appuyer sur le pattern op√©rateur de Kubernetes. Concr√®tement, cela signifie que l'on va d√©clarer des ressources personnalis√©es (Custom Resources) qui seront interpr√©t√©es par VictoriaMetrics Operator pour configurer et g√©rer VictoriaMetrics.\nLe Helm chart qu‚Äôon a utilis√© ne d√©ploie pas directement VictoriaMetrics, mais il installe principalement l‚Äôop√©rateur. Cet op√©rateur se charge ensuite de cr√©er et de g√©rer des custom resources comme VMSingle ou VMCluster, qui d√©terminent comment VictoriaMetrics est d√©ploy√© et configur√© en fonction des besoins.\nLe r√¥le de VMServiceScrape est de d√©finir o√π aller chercher les m√©triques pour un service donn√©. On s‚Äôappuie sur les labels Kubernetes pour identifier le bon service et le bon port.\nobservability/base/victoria-metrics-k8s-stack/vmservicecrapes/karpenter.yaml\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMServiceScrape 3metadata: 4 name: karpenter 5 namespace: karpenter 6spec: 7 selector: 8 matchLabels: 9 app.kubernetes.io/name: karpenter 10 endpoints: 11 - port: http-metrics 12 path: /metrics 13 namespaceSelector: 14 matchNames: 15 - karpenter Nous pouvons v√©rifier que les param√®tres sont bien configur√©s gr√¢ce √† kubectl\n1kubectl get services -n karpenter --selector app.kubernetes.io/name=karpenter -o yaml | grep -A 4 ports 2 ports: 3 - name: http-metrics 4 port: 8000 5 protocol: TCP 6 targetPort: http-metrics Parfois il n'y pas de service, nous pouvons alors indiquer comment identifier les pods directement avec VMPodScrape.\nobservability/base/flux-config/observability/vmpodscrape.yaml\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMPodScrape 3metadata: 4 name: flux-system 5 namespace: flux-system 6spec: 7 namespaceSelector: 8 matchNames: 9 - flux-system 10 selector: 11 matchExpressions: 12 - key: app 13 operator: In 14 values: 15 - helm-controller 16 - source-controller 17 - kustomize-controller 18 - notification-controller 19 - image-automation-controller 20 - image-reflector-controller 21 podMetricsEndpoints: 22 - targetPort: http-prom Toutes nos applications ne sont pas forc√©ment d√©ploy√©es sur Kubernetes. La ressource VMScrapeConfig dans VictoriaMetrics permet d'utiliser plusieurs m√©thodes de \u0026quot;Service Discovery\u0026quot;. Cette ressource offre la flexibilit√© de d√©finir comment scrapper les cibles via diff√©rents m√©canismes de d√©couverte, tels que les instances EC2 (AWS), les services Cloud ou d'autres syst√®mes. Dans l'exemple ci-dessous, on utilise le tag personnalis√© observability:node-exporter, et on applique des transformations de labels. Ce qui nous permet de r√©cup√©rer les m√©triques expos√©es par les node-exporters install√©s sur ces instances.\nobservability/base/victoria-metrics-k8s-stack/vmscrapeconfigs/ec2.yaml\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMScrapeConfig 3metadata: 4 name: aws-ec2-node-exporter 5 namespace: observability 6spec: 7 ec2SDConfigs: 8 - region: ${region} 9 port: 9100 10 filters: 11 - name: tag:observability:node-exporter 12 values: [\u0026#34;true\u0026#34;] 13 relabelConfigs: 14 - action: replace 15 source_labels: [__meta_ec2_tag_Name] 16 target_label: ec2_name 17 - action: replace 18 source_labels: [__meta_ec2_tag_app] 19 target_label: ec2_application 20 - action: replace 21 source_labels: [__meta_ec2_availability_zone] 22 target_label: ec2_az 23 - action: replace 24 source_labels: [__meta_ec2_instance_id] 25 target_label: ec2_id 26 - action: replace 27 source_labels: [__meta_ec2_region] 28 target_label: ec2_region ‚ÑπÔ∏è Si on utilisait d√©j√† le Prometheus Operator, la migration vers VictoriaMetrics est tr√®s simple car il est compatible avec les CRDs d√©finies par le Prometheus Operator.\nüìà Visualiser nos m√©triques avec l'op√©rateur Grafana Il est facile de deviner √† quoi sert le Grafana Operator: Utiliser des ressources Kubernetes pour configurer Grafana üòù. Il permet de d√©ployer des instances Grafana, d'ajouter des datasources, d'importer des dashboards de diff√©rentes √† partir de diff√©rentes sources (URL, JSON), de les classer dans des r√©pertoires etc... Il s'agit d'une alternative au fait de tout d√©finir dans le chart Helm ou d'utiliser des configmaps et, selon moi, offre une meilleure lecture. Dans cet exemple, je regroupe l'ensemble des ressources relatives √† la supervision de Cilium\n1tree infrastructure/base/cilium/ 2infrastructure/base/cilium/ 3‚îú‚îÄ‚îÄ grafana-dashboards.yaml 4‚îú‚îÄ‚îÄ grafana-folder.yaml 5‚îú‚îÄ‚îÄ httproute-hubble-ui.yaml 6‚îú‚îÄ‚îÄ kustomization.yaml 7‚îú‚îÄ‚îÄ vmrules.yaml 8‚îî‚îÄ‚îÄ vmservicescrapes.yaml La d√©finition du r√©pertoire est super simple\nobservability/base/infrastructure/cilium/grafana-folder.yaml\n1apiVersion: grafana.integreatly.org/v1beta1 2kind: GrafanaFolder 3metadata: 4 name: cilium 5spec: 6 allowCrossNamespaceImport: true 7 instanceSelector: 8 matchLabels: 9 dashboards: \u0026#34;grafana\u0026#34; Puis voici une ressource Dashboard qui va chercher la configuration √† partir d'un lien HTTP. Nous pouvons aussi utiliser les dashboards disponibles depuis le site de Grafana, en indiquant l'ID appropri√© ou carr√©ment mettre la d√©finition au format JSON.\nobservability/base/infrastructure/cilium/grafana-dashboards.yaml\n1apiVersion: grafana.integreatly.org/v1beta1 2kind: GrafanaDashboard 3metadata: 4 name: cilium-cilium 5spec: 6 folderRef: \u0026#34;cilium\u0026#34; 7 allowCrossNamespaceImport: true 8 datasources: 9 - inputName: \u0026#34;DS_PROMETHEUS\u0026#34; 10 datasourceName: \u0026#34;VictoriaMetrics\u0026#34; 11 instanceSelector: 12 matchLabels: 13 dashboards: \u0026#34;grafana\u0026#34; 14 url: \u0026#34;https://raw.githubusercontent.com/cilium/cilium/main/install/kubernetes/cilium/files/cilium-agent/dashboards/cilium-dashboard.json\u0026#34; Notez que j'ai choisi de ne pas utiliser l'op√©rateur Grafana pour d√©ployer l'instance, mais de garder celle qui a √©t√© install√©e via le Helm chart de VictoriaMetrics. Il faut donc simplement fournir √† l'op√©rateur Grafana les param√®tres d'authentification pour qu'il puisse appliquer les modifications sur cette instance.\nobservability/base/grafana-operator/grafana-victoriametrics.yaml\n1apiVersion: grafana.integreatly.org/v1beta1 2kind: Grafana 3metadata: 4 name: grafana-victoriametrics 5 labels: 6 dashboards: \u0026#34;grafana\u0026#34; 7spec: 8 external: 9 url: http://victoria-metrics-k8s-stack-grafana 10 adminPassword: 11 name: victoria-metrics-k8s-stack-grafana-admin 12 key: admin-password 13 adminUser: 14 name: victoria-metrics-k8s-stack-grafana-admin 15 key: admin-user Enfin nous pouvons utiliser Grafana et explorer nos diff√©rents dashboards üéâ!\nYour browser does not support the video tag. üí≠ Derni√®res remarques Si l'on se r√©f√®re aux diff√©rents articles consult√©s, l'une des principales raisons pour lesquelles migrer ou choisir VictoriaMetrics serait une meilleure performance en r√®gle g√©n√©rale. Cependant il est judicieux de rester prudent car les r√©sultats des benchmarks d√©pendent de plusieurs facteurs, ainsi que de l'objectif recherch√©. C'est pourquoi il est fortement conseill√© de lancer des tests soit m√™me. VictoriaMetrics propose un jeu de test qui peut √™tre r√©alis√© sur les TSDB compatibles avec Prometheus.\nVous l'aurez compris, aujourd'hui mon choix se porte sur VictoriaMetrics pour la collecte des m√©triques, car j'appr√©cie l'architecture modulaire avec une multitude de combinaisons possibles en fonction de l'√©volution du besoin. Cependant, une solution utilisant l'op√©rateur Prometheus fonctionne tr√®s bien dans la plupart des cas et a l'int√©r√™t d'√™tre gouvern√© par une fondation.\nPar aileurs, il est important de noter que certaines fonctionnalit√©s ne sont disponibles qu'en version Entreprise, notamment le downsampling qui est fort utile lorsque l'on veut garder une grosse quantit√© de donn√©es sur du long terme.\nDans cet article nous avons surtout pu mettre en √©vidence la facilit√© de mise en oeuvre pour obtenir une solution qui permette, d'une part de collecter efficacement les m√©triques, et de les visualiser. Ceci, toujours en utilisant le pattern operateur Kubernetes qui permet de faire du GitOps, et de d√©clarer les diff√©rents types de ressources au travers de Custom resources. Ainsi Un d√©veloppeur peut tr√®s bien inclure √† ses manifests, un VMServiceScrape et une VMRule et, ainsi, inclure la culture de l'observabilit√© dans les processes de livraisons applicative.\nDisposer de m√©triques c'est bien bien, mais est-ce suffisant? On va essayer d'y r√©pondre dans les prochains articles ...\nüîñ References Articles sur VictoriaMetrics ","link":"https://blog.ogenki.io/fr/post/series/observability/metrics/","section":"post","tags":["observability"],"title":"Une solution compl√®te et performante pour g√©rer vos m√©triques avec les op√©rateurs `VictoriaMetrics` et `Grafana`!"},{"body":"Dagger est un projet open source qui promet de r√©volutionner la fa√ßon de d√©finir les pipelines d'int√©gration continue (CI). Il a √©t√© cr√©√© par les fondateurs de Docker qui se sont bas√©s sur une √©tude des difficult√©s courantes dans les entreprises. Il ont alors identifi√© un manque d'outillage efficaces le long du cycle de d√©veloppement jusqu'au passage en production.\nIl y a notamment un manque d'homog√©n√©it√© entre les environnements d'ex√©cution, vous avez probablement d√©j√† entendu votre voisin(e) se plaindre avec un truc du genre: \u0026quot;Erf, √ßa marchait bien sur ma machine! C'est quoi cette erreur sur la CI?\u0026quot; üòÜ\nOffrant une m√©thode commune et centralis√©e, Dagger serait LA r√©ponse √† cette probl√©matique et permettrait, par ailleurs, d'am√©liorer l'exp√©rience d√©veloppeur locale, la collaboration et ainsi d'acc√©lerer le cycle de d√©veloppement.\nBeaucoup d'entre nous ont d√©j√† utilis√© des scripts bash, des Makefiles et d'autres m√©thodes traditionnelles pour automatiser certaines actions. Cependant, ces solutions peuvent vite devenir complexes et difficiles √† maintenir. Dagger propose une alternative moderne et simplifi√©e, permettant de standardiser et d'uniformiser nos pipelines, peu importe l'environnement.\nMais alors, quelles sont les principales fonctionnalit√©s de Dagger, et comment l'utiliser efficacement?\nüéØ Notre objectif Voici les points que nous allons aborder dans cet article :\nTout d'abord, nous allons comprendre le fonctionnement de Dagger et faire nos premiers pas.\nEnsuite, nous prendrons des cas concrets pour sa mise en ≈ìuvre. Nous verrons comment transformer un projet existant, et je vous pr√©senterai √©galement un module que j'utilise d√©sormais quotidiennement.\nEnfin, nous d√©crirons une solution de mise en cache efficace, qui nous permettra de nous projeter dans la mise √† l'√©chelle avec Dagger.\nüîé La d√©couverte En gros, Dagger est un outil qui nous permet de d√©finir des t√¢ches dans notre langage pr√©f√©r√© et de rendre ce code portable. Autrement dit, ce que j'ex√©cute sur ma machine sera ex√©cut√© de la m√™me mani√®re sur la CI ou sur l'ordinateur de mon/ma coll√®gue.\nIl y a 2 composants principaux qui entrent en jeu\nLa CLI Dagger: Notre point d'acc√®s principal pour interagir avec les diff√©rentes fonctions et modules, les t√©l√©charger et afficher le r√©sultat de leur ex√©cution. Le moteur Dagger: Toutes les op√©rations effectu√©es avec la CLI passent par une API GraphQL expos√©e par un moteur Dagger. Chaque client initie sa propre session avec l'API Core qui dispose des fonctionnalit√©s de base. Elles peuvent ensuite √™tre √©tendues gr√¢ce √† des modules. Commen√ßons par installer la CLI. Si vous avez parcouru mes pr√©c√©dents articles, vous savez que j'affectionne particuli√®rement asdf\n1asdf plugin-add dagger 2 3asdf install dagger 0.12.1 4Downloading dagger from https://github.com/dagger/dagger/releases/download/v0.12.1/dagger_v0.12.1_linux_amd64.tar.gz 5 6asdf global dagger 0.12.1 7dagger version 8dagger v0.12.1 (registry.dagger.io/engine) linux/amd64 Entrons dans le vif du sujet, nous pouvons tout de suite ex√©cuter un module fournie par la communaut√©. Supposons que l'on veuille scanner un repo git et une image Docker avec trivy.\nLe Daggerverse Le Daggerverse est une plateforme permettant a quiconque de partager des modules. Lorsque vous avez un besoin, il est conseill√© de regarder ce qui est d√©j√† propos√© par d'autres. Faites le test en recherchant par example golangci, black, gptscript, wolfi...\nNous pouvons consulter les fonctions disponibles dans le module en utilisant l'argument functions\n1TRIVY_MODULE=\u0026#34;github.com/purpleclay/daggerverse/trivy@c3f44e0c8a396b2adf024bb862714037ae4cc8e7\u0026#34; 2 3dagger functions -m ${TRIVY_MODULE} 4Name Description 5filesystem Scan a filesystem for any vulnerabilities 6image Scan a published (or remote) image for any vulnerabilities 7image-local Scan a locally exported image for any vulnerabilities Les functions peuvent aussi prendre divers param√®tres\n1dagger call -m ${TRIVY_MODULE} filesystem --help 2... 3ARGUMENTS 4 --dir Directory the path to directory to scan [required] 5 --exit-code int the returned exit code when vulnerabilities are detected (0) 6 --format string the type of format to use when generating the compliance report (table) 7 --ignore-unfixed filter out any vulnerabilities without a known fix 8 --scanners string the types of scanner to execute (vuln,secret) 9 --severity string the severity of security issues to detect (UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL) 10 --template string a custom go template to use when generating the compliance report 11 --vuln-type string the types of vulnerabilities to scan for (os,library) Analysons donc le niveau de s√©curit√© de mon repository local üïµÔ∏è\n1dagger call -m ${TRIVY_MODULE} filesystem --dir \u0026#34;.\u0026#34; 2 3scan/go.mod (gomod) 4=================== 5Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) 6 7‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê 8‚îÇ Library ‚îÇ Vulnerability ‚îÇ Severity ‚îÇ Status ‚îÇ Installed Version ‚îÇ Fixed Version ‚îÇ Title ‚îÇ 9‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ 10‚îÇ github.com/vektah/gqlparser/v2 ‚îÇ CVE-2023-49559 ‚îÇ MEDIUM ‚îÇ fixed ‚îÇ 2.5.11 ‚îÇ 2.5.14 ‚îÇ gqlparser denial of service vulnerability via the ‚îÇ 11‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ parserDirectives function ‚îÇ 12‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ https://avd.aquasec.com/nvd/cve-2023-49559 ‚îÇ 13‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Oups! il semble qu'il y ait une vuln√©rabilit√© critique dans mon image üò®.\n1dagger call -m ${TRIVY_MODULE} image --ref smana/dagger-cli:v0.12.1 --severity CRITICAL 2 3smana/dagger-cli:v0.12.1 (ubuntu 23.04) 4======================================= 5Total: 0 (CRITICAL: 0) 6 7 8usr/local/bin/dagger (gobinary) 9=============================== 10Total: 1 (CRITICAL: 1) 11 12‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê 13‚îÇ Library ‚îÇ Vulnerability ‚îÇ Severity ‚îÇ Status ‚îÇ Installed Version ‚îÇ Fixed Version ‚îÇ Title ‚îÇ 14‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ 15‚îÇ stdlib ‚îÇ CVE-2024-24790 ‚îÇ CRITICAL ‚îÇ fixed ‚îÇ 1.22.3 ‚îÇ 1.21.11, 1.22.4 ‚îÇ golang: net/netip: Unexpected behavior from Is methods for ‚îÇ 16‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ IPv4-mapped IPv6 addresses ‚îÇ 17‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ https://avd.aquasec.com/nvd/cve-2024-24790 ‚îÇ 18‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò C'est d√©j√† super cool de pouvoir b√©n√©ficier de nombreuses sources ü§© ! Ces modules peuvent donc √™tre utilis√©s directement ou devenir une source d'inspiration pr√©cieuse pour nos futurs pipelines.\nUn module est une collection de fonctions qui prend des param√®tres en entr√©e et nous renvoie une r√©ponse sous diff√©rentes formes : du texte en sortie, l'ex√©cution d'un terminal, le lancement d'un service, etc. Notons aussi que toutes les fonctions sont ex√©cut√©es dans des conteneurs.\nApr√®s cette courte intro, passons √† des cas d'usage r√©els en commen√ßant par ajouter des t√¢ches/fonctions √† un projet existant.\nü¶ã Daggeriser une application existante Prenons un projet de d√©monstration existant, un simple serveur web avec une fonction de stockage de mots dans une base de donn√©es. Nous allons progressivement transformer ce projet en y injectant du Dagger üíâ. Cette approche it√©rative, √©tape par √©tape, peut √©galement √™tre appliqu√©e √† des projets plus importants pour les Daggeriser progressivement. Premi√®re fonction üë∂ Notre priorit√© va √™tre de tester le code en utilisant la commande go test.\nCommen√ßons donc par initialiser le repo git afin de g√©n√©rer l'arborescence requise pour l'ex√©cution des fonctions Dagger.\n1git clone https://github.com/Smana/golang-helloworld.git 2cd golang-helloworld 3dagger init --sdk=go 1ls -l dagger* 2.rw-r--r-- 101 smana 28 Jun 21:54 dagger.json 3 4dagger: 5.rw------- 25k smana 28 Jun 21:54 dagger.gen.go 6drwxr-xr-x - smana 28 Jun 21:54 internal 7.rw------- 1.4k smana 28 Jun 21:54 main.go La commande d'initialisation g√©n√®re donc un fichier main.go qui contient des fonctions d'exemple que nous allons totalement remplacer par le code suivant:\n1package main 2 3import ( 4\t\u0026#34;context\u0026#34; 5) 6 7type GolangHelloworld struct{} 8 9// Test runs the tests for the GolangHelloworld project 10func (m *GolangHelloworld) Test(ctx context.Context, source *Directory) (string, error) { 11\tctr := dag.Container().From(\u0026#34;golang:1.22\u0026#34;) 12\treturn ctr. 13\tWithWorkdir(\u0026#34;/src\u0026#34;). 14\tWithMountedDirectory(\u0026#34;/src\u0026#34;, source). 15\tWithExec([]string{\u0026#34;go\u0026#34;, \u0026#34;test\u0026#34;, \u0026#34;./...\u0026#34;}). 16\tStdout(ctx) 17} Il s'agit l√† d'une fonction tr√®s simple:\nCette fonction, appel√©e Test, prend en param√®tre un r√©pertoire source. Nous utilisons une image golang:1.22. Le code du r√©pertoire donn√© en param√®tre est mont√© dans le dossier /src du conteneur. Ensuite, nous ex√©cutons la commande go test ./... sur le r√©pertoire source. Enfin, nous r√©cup√©rons le r√©sultat des tests (stdout). Mise √† jour de l\u0026#39;environnement de dev Il est r√©guli√®rement n√©cessaire de lancer la commande suivante afin de mettre √† jour les fichiers Dagger (d√©pendances etc...)\n1dagger develop C'est parti, testons notre code!\n1dagger call test --source \u0026#34;.\u0026#34; 2? helloworld/cmd/helloworld [no test files] 3? helloworld/dagger [no test files] 4? helloworld/dagger/internal/dagger [no test files] 5? helloworld/dagger/internal/querybuilder [no test files] 6? helloworld/dagger/internal/telemetry [no test files] 7ok helloworld/internal/server 0.004s \u0026#x2139;\u0026#xfe0f; La premi√®re ex√©cution prend du temps car elle construit t√©l√©charge l'image et installe les d√©pendances Go, mais les ex√©cutions suivantes sont beaucoup plus rapides. Nous aborderons le sujet de la mise en cache plus tard dans cet article.\nEt mon docker-compose alors? üê≥ Le projet initial permet de lancer un environnement de test local en utilisant Docker Compose\nLa commande docker-compose up --build effectue plusieurs actions : elle construit l'image Docker de l'application en se basant sur le Dockerfile local, puis lance deux conteneurs : un pour l'application et un pour la base de donn√©es. Elle permet √©galement la communication entre ces deux conteneurs.\n1docker ps 2CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3a1673d56f9c8 golang-helloworld-app \u0026#34;/app/main\u0026#34; 3 seconds ago Up 3 seconds 0.0.0.0:8080-\u0026gt;8080/tcp, :::8080-\u0026gt;8080/tcp golang-helloworld-app-1 4bb3dee1305dc postgres:16 \u0026#34;docker-entrypoint.s‚Ä¶\u0026#34; 3 seconds ago Up 3 seconds 0.0.0.0:5432-\u0026gt;5432/tcp, :::5432-\u0026gt;5432/tcp golang-helloworld-database-1 Il est ensuite possible d'acc√©der √† l'application et de stocker des mots dans la base de donn√©es.\n1curl -X POST -d \u0026#39;{\u0026#34;word\u0026#34;:\u0026#34;foobar\u0026#34;}\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; http://localhost:8080/store 2 3curl http://localhost:8080/list 4[\u0026#34;foobar\u0026#34;] Comment r√©aliser la m√™me chose avec Dagger?\nTout d'abord nous allons construire l'image:\n1// Build the Docker container 2func (m *GolangHelloworld) Build(ctx context.Context, source *Directory) *Container { 3\t// build the binary 4\tbuilder := dag.Container(). 5\tFrom(golangImage). 6\tWithDirectory(\u0026#34;/src\u0026#34;, source). 7\tWithWorkdir(\u0026#34;/src\u0026#34;). 8\tWithEnvVariable(\u0026#34;CGO_ENABLED\u0026#34;, \u0026#34;0\u0026#34;). 9\tWithExec([]string{\u0026#34;go\u0026#34;, \u0026#34;build\u0026#34;, \u0026#34;-o\u0026#34;, \u0026#34;helloworld\u0026#34;, \u0026#34;cmd/helloworld/main.go\u0026#34;}) 10 11\t// Create the target image with the binary 12\ttargetImage := dag.Container(). 13\tFrom(alpineImage). 14\tWithFile(\u0026#34;/bin/helloworld\u0026#34;, builder.File(\u0026#34;/src/helloworld\u0026#34;), ContainerWithFileOpts{Permissions: 0700, Owner: \u0026#34;nobody\u0026#34;}). 15\tWithUser(\u0026#34;nobody:nobody\u0026#34;). 16\tWithEntrypoint([]string{\u0026#34;/bin/helloworld\u0026#34;}) 17 18\treturn targetImage 19} Ce code d√©montre l'utilisation du \u0026quot;multi-stage build\u0026quot; pour optimiser la s√©curit√© et la taille de l'image. Cette m√©thode permet de n'inclure que ce qui est n√©cessaire dans l'image finale, r√©duisant ainsi la surface d'attaque et la taille de l'image.\nEnsuite nous avons besoin d'une instance PostgreSQL. √áa tombe bien il y a un module pour √ßa ¬Æ!\nNous allons donc installer cette d√©pendance pour pouvoir utiliser ses fonctions directement dans notre code.\n1dagger install github.com/quartz-technology/daggerverse/postgres@v0.0.3 La fonction Database() permet de lancer un conteneur Postgres.\n1... 2\topts := PostgresOpts{ 3\tDbName: dbName, 4\tCache: cache, 5\tVersion: \u0026#34;13\u0026#34;, 6\tConfigFile: nil, 7\tInitScript: initScriptDir, 8\t} 9 10... 11\tpgCtr := dag.Postgres(pgUser, pgPass, pgPortInt, opts).Database() Ensuite, nous devons cr√©er un lien entre les deux conteneurs. Ci-dessous, nous r√©cup√©rons les informations du service expos√© par le conteneur Postgres pour les utiliser dans notre application.\n1... 2\tpgSvc := pgCtr.AsService() 3 4\tpgHostname, err := pgSvc.Hostname(ctx) 5\tif err != nil { 6\treturn nil, fmt.Errorf(\u0026#34;could not get postgres hostname: %w\u0026#34;, err) 7\t} 8 9\treturn ctr. 10\tWithSecretVariable(\u0026#34;PGPASSWORD\u0026#34;, pgPass). 11\tWithSecretVariable(\u0026#34;PGUSER\u0026#34;, pgUser). 12\tWithEnvVariable(\u0026#34;PGHOST\u0026#34;, pgHostname). 13\tWithEnvVariable(\u0026#34;PGDATABASE\u0026#34;, opts.DbName). 14\tWithEnvVariable(\u0026#34;PGPORT\u0026#34;, pgPort). 15\tWithServiceBinding(\u0026#34;database\u0026#34;, pgSvc). 16\tWithExposedPort(8080), nil 17... Les secrets üîí Les informations sensibles peuvent √™tre pass√©es lors de l'appel aux fonctions Dagger de plusieurs fa√ßon: Des variables d'environnement, lecture du contenu de fichiers ou la sortie d'une ligne de commande. Dans cet article nous avons privil√©gi√© les variables d'environnement mais nous aurions tr√®s bien pu utiliser une commande vault. (Article pr√©c√©dent sur Vault)\nup permet de transf√©rer les appels locaux aux services expos√©s par le conteneur.\n1export PGUSER=\u0026#34;user\u0026#34; 2export PGPASS=\u0026#34;password\u0026#34; 3dagger call serve --pg-user=env:PGUSER --pg-pass=env:PGPASS --source \u0026#34;.\u0026#34; as-service up 4 5... 6 ‚óè start /bin/helloworld 30.7s 7 ‚îÉ 2024/06/30 08:27:50 Starting server on :8080 8 ‚îÉ 2024/06/30 08:27:50 Starting server on :8080 Et voil√†! nous pouvons d√©sormais tester notre application en local.\nD\u0026#39;autres fonctions J'ai volontairement tronqu√© ces derniers extraits, mais je vous invite √† consulter la configuration compl√®te ici. Vous y trouverez notamment la possibilit√© de publier l'image dans un registry.\nDe plus, je vous conseille de parcourir le Cookbook dans la documentation Dagger, o√π vous trouverez de nombreux exemples.\nüß© Le module Kubeconform Je suis parti d'un r√©el cas d'usage: J'utilise depuis quelques ann√©es un script bash pour valider les manifests Kubernetes/Kustomize ainsi que la configuration Flux. L'id√©e est donc de r√©pondre √† ce m√™me besoin mais aussi d'aller un peu plus loin...\nL'initialisation d'un module se fait de la fa√ßon suivante:\n1dagger init --name=kubeconform --sdk=go kubeconform Il faut ensuite d√©cider des param√®tres d'entr√©e. Par exemple je souhaite pouvoir choisir la version du binaire Kubeconform.\n1... 2\t// Kubeconform version to use for validation. 3\t// +optional 4\t// +default=\u0026#34;v0.6.6\u0026#34; 5\tversion string, 6... Les commentaires ci-dessus sont importants: La description sera affich√©e √† l'utilisateur et nous pouvons faire en sorte que ce param√®tre ne soit pas requis avec une version par d√©fault.\n1dagger call -m github.com/Smana/daggerverse/kubeconform@v0.1.0 validate --help 2Validate the Kubernetes manifests in the provided directory and optional source CRDs directories 3... 4 --version string Kubeconform version to use for validation. (default \u0026#34;v0.6.6\u0026#34;) L'objectif est de pouvoir partager ce module, donc tous les √©l√©ments de contexte doivent √™tre clairs et compr√©hensibles.\nEn d√©veloppant ce module, j'ai suis pass√© par plusieurs it√©rations et j'ai obtenu des infos tr√®s utiles sur le Discord de Dagger. C'est un super moyen d'√©changer avec la communaut√©.\nAnalysons par exemple ceci:\n1kubeconformBin := dag.Arc(). 2 Unarchive(dag.HTTP(fmt.Sprintf(\u0026#34;https://github.com/yannh/kubeconform/releases/download/%s/kubeconform-linux-amd64.tar.gz\u0026#34;, kubeconform_version)). 3 WithName(\u0026#34;kubeconform-linux-amd64.tar.gz\u0026#34;)).File(\u0026#34;kubeconform-linux-amd64/kubeconform\u0026#34;) J'utilise le module Arc pour d√©compresser un fichier r√©cup√©r√© avec la fonction HTTP et je ne prends que le binaire inclus dans cette archive. Plut√¥t efficace !\nDans cet autre exemple j'utilise le module Apko pour construire l'image initial, y installer des packages...\n1ctr := dag.Apko().Wolfi([]string{\u0026#34;bash\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;kustomize\u0026#34;, \u0026#34;git\u0026#34;, \u0026#34;python3\u0026#34;, \u0026#34;py3-pip\u0026#34;, \u0026#34;yq\u0026#34;}). 2 WithExec([]string{\u0026#34;pip\u0026#34;, \u0026#34;install\u0026#34;, \u0026#34;pyyaml\u0026#34;}) Au moment o√π j'√©cris cet article, le module Kubeconform inclut aussi un bout de script bash, essentiellement pour parcourir l'arborescence efficacement et ex√©cuter kubeconform\n1\tscriptContent := `#!/bin/bash 2... 3` 4 5\t// Add the manifests and the script to the container 6\tctr = ctr. 7\tWithMountedDirectory(\u0026#34;/work\u0026#34;, manifests). 8\tWithNewFile(\u0026#34;/work/run_kubeconform.sh\u0026#34;, ContainerWithNewFileOpts{ 9\tPermissions: 0750, 10\tContents: scriptContent, 11\t}) 12 13\t// Execute the script 14\tkubeconform_command := []string{\u0026#34;bash\u0026#34;, \u0026#34;/work/run_kubeconform.sh\u0026#34;} 15... Pour tester et corriger le module nous pouvons l'ex√©cuter localement sur un repo qui contient des manifests Kubernetes.\n1dagger call validate --manifests ~/Sources/demo-cloud-native-ref/clusters --catalog 2... 3Summary: 1 resource found in 1 file - Valid: 1, Invalid: 0, Errors: 0, Skipped: 0 4Validation successful for ./mycluster-0/crds.yaml 5Processing file: ./mycluster-0/flux-config.yaml 6Summary: 1 resource found in 1 file - Valid: 1, Invalid: 0, Errors: 0, Skipped: 0 7Validation successful for ./mycluster-0/flux-config.yaml Nous pouvons aussi augmenter le niveau de verbosit√©. Le niveau le plus √©lev√© √©tant -vvv --debug\n1dagger call validate --manifests ~/Sources/demo-cloud-native-ref/clusters --catalog -vvv --debug 2... 309:32:07 DBG new end old=\u0026#34;2024-07-06 09:32:07.436103097 +0200 CEST\u0026#34; new=\u0026#34;2024-07-06 09:32:07.436103273 +0200 CEST\u0026#34; 409:32:07 DBG recording span span=telemetry.LogsSource/Subscribe id=b3fc48ec7900f581 509:32:07 DBG recording span child span=telemetry.LogsSource/Subscribe parent=ae535768bb2be9d7 child=b3fc48ec7900f581 609:32:07 DBG new end old=\u0026#34;2024-07-06 09:32:07.436103273 +0200 CEST\u0026#34; new=\u0026#34;2024-07-06 09:32:07.438699251 +0200 CEST\u0026#34; 709:32:07 DBG recording span span=\u0026#34;/home/smana/.asdf/installs/dagger/0.12.1/bin/dagger call -m github.com/Smana/daggerverse/kubeconform@v0.1.0 validate --manifests /home/smana/Sources/demo-cloud-native-ref/clusters --catalog -vvv --debug\u0026#34; id=ae535768bb2be9d7 809:32:07 DBG frontend exporting logs logs=4 909:32:07 DBG exporting log span=0xf62760 body=\u0026#34;\u0026#34; 1009:32:07 DBG got EOF 1109:32:07 DBG run finished err=\u0026lt;nil\u0026gt; 12 13‚úî 609fcdee60c94c07 connect 0.6s 14 ‚úî c873c2d69d2b7ce7 starting engine 0.5s 15 ‚úî 5f48c41bd0a948ca create 0.5s 16 ‚úî dbd62c92c3db105f exec docker start dagger-engine-ceb38152f96f1298 0.0s 17 ‚îÉ dagger-engine-ceb38152f96f1298 18 ‚úî 4db8303f1d7ec940 connecting to engine 0.1s 19 ‚îÉ 09:32:03 DBG connecting runner=docker-image://registry.dagger.io/engine:v0.12.1 client=5fa0kn1nc4qlku1erer3868nj 20 ‚îÉ 09:32:03 DBG subscribing to telemetry remote=docker-image://registry.dagger.io/engine:v0.12.1 21 ‚îÉ 09:32:03 DBG subscribed to telemetry elapsed=19.095¬µs √Ä partir de la version v0.12.x, Dagger propose un mode interactif. En utilisant le param√®tre -i ou --interactive, il est possible de lancer automatiquement un terminal lorsque le code rencontre une erreur. Cela permet de r√©aliser des v√©rifications et des op√©rations directement dans le conteneur.\nDe plus, il est possible d'ajouter l'ex√©cution de Terminal() √† n'importe quel endroit dans la d√©finition du conteneur pour entrer en mode interactif √† ce moment pr√©cis.\n1... 2\tstdout, err := ctr.WithExec(kubeconform_command). 3\tTerminal(). 4\tStdout(ctx) 5... Avec ce module j'ai pu ajouter aussi des fonctionnalit√©s manquantes qui sont fort utiles:\nConvertir toutes les CRDs en JSONSchemas afin de valider 100% des manifests Kubernetes Le rendre compatible avec les substitutions de variables de Flux. Enfin, j'ai pu le partager dans le Daggerverse et mettre √† jour mes workflows de CI sur Github Actions.\nüöÄ It√©ration rapide et collaboration gr√¢ce √† un cache partag√© Utiliser un cache permet de ne pas r√©ex√©cuter les √©tapes dont le code n'a pas chang√©. Lors de la premi√®re ex√©cution, toutes les √©tapes seront ex√©cut√©es, mais les suivantes ne reprendront que les √©tapes modifi√©es, ce qui permet de gagner un temps consid√©rable.\nDagger permet de mettre en cache, √† chaque ex√©cution, les op√©rations de manipulation des fichiers, la construction des conteneurs, l'ex√©cution des tests, la compilation du code, ainsi que les volumes qui doivent √™tre explicitement d√©finis dans le code.\nPar d√©faut, le moteur Dagger est disponible en local, et utilise du cache local.\n1docker ps 2CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 33cec5bf51843 registry.dagger.io/engine:v0.12.1 \u0026#34;dagger-entrypoint.s‚Ä¶\u0026#34; 8 days ago Up 2 hours dagger-engine-ceb38152f96f1298 La proposition suivante vise √† d√©finir un cache partag√© et distant, accessible √† tous les collaborateurs ainsi que depuis la CI. L'objectif est d'acc√©l√©rer les ex√©cutions ult√©rieures, peu importe o√π Dagger est ex√©cut√©.\nNous allons voir comment mettre cela en pratique avec:\nDes Github Runners ex√©cut√©s en priv√©, sur notre plateforme (Self-Hosted) Un moteur Dagger centralis√© Cloud Native Reference Cette solution de CI sur EKS est d√©ploy√©e en utilisant le repository Cloud Native Ref. Je vous encourage vivement √† le consulter, car j'y aborde de nombreux sujets relatifs aux technlogies Cloud Native. L'id√©e initial de ce projet est de pouvoir d√©marrer rapidement une plateforme qui applique les bonnes pratiques en terme d'automatisation, de supervision, de s√©curit√© etc. Les commentaires et contributions sont les bienvenues üôè Voici comment les composants de CI interagissent, avec Dagger jouant un r√¥le central gr√¢ce au cache partag√©.\nMaintenant que nous avons une vue d'ensemble de Dagger et de son utilisation, nous allons explorer comment optimiser son utilisation en entreprise en utilisant un cache partag√©.\nü§ñ Github Self Hosted Runners: Acc√®s au cache Dagger s'int√®gre bien avec la plupart des solutions de CI. Il suffit en effet de lancer une commande dagger. Dans cet article nous faisons usage de l'Action pour Github Actions.\n1 kubernetes-validation: 2 name: Kubernetes validation ‚ò∏ 3 runs-on: ubuntu-latest 4 steps: 5 - name: Checkout 6 uses: actions/checkout@v4 7 8 - name: Validate Flux clusters manifests 9 uses: dagger/dagger-for-github@v6 10 with: 11 version: \u0026#34;latest\u0026#34; 12 verb: call 13 module: github.com/Smana/daggerverse/kubeconform@kubeconform/v0.1.0 14 args: validate --manifests \u0026#34;./clusters\u0026#34; --catalog Ce job t√©l√©charge le code source du repo git et ex√©cute le module kubeconform. Bien que cela fonctionne tr√®s bien, il faut noter que ce job est ex√©cut√© sur les runners fournis par Github sur leur infrastructure.\nLes GitHub self-hosted runners sont des machines que vous configurez pour ex√©cuter des workflows GitHub Actions sur votre propre infrastructure, plut√¥t que d'utiliser les runners h√©berg√©s par GitHub. Ils offrent plus de contr√¥le et de flexibilit√©, permettant de personnaliser l'environnement d'ex√©cution selon vos besoins sp√©cifiques. Cela peut conduire √† des performances am√©lior√©es et permet un acc√®s s√©curis√© √† des ressources priv√©es.\nUn Scale set est un group de runners Github qui partage une configuration commune: .github/workflows/ci.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2 2kind: HelmRelease 3metadata: 4 name: dagger-gha-runner-scale-set 5spec: 6 releaseName: dagger-gha-runner-scale-set 7... 8 values: 9 runnerGroup: \u0026#34;default\u0026#34; 10 githubConfigUrl: \u0026#34;https://github.com/Smana/demo-cloud-native-ref\u0026#34; 11 githubConfigSecret: gha-runner-scale-set 12 maxRunners: 5 13 14 containerMode: 15 type: \u0026#34;dind\u0026#34; Ce scale set est configur√© pour le repo Cloud Native Ref Il faut lui indiquer un secret dans lequel est configur√© les param√®tres de la Github App dind indique le mode utilis√© pour lancer les conteneurs. ‚ö†Ô∏è Attention cependant en termes de s√©curit√© : Dagger doit s'ex√©cuter en tant qu'utilisateur root et avoir des permissions √©lev√©es pour contr√¥ler les conteneurs, volumes, r√©seaux, etc. (Plus d'informations ici). ‚ò∏Ô∏è Consid√©ration sp√©cifiques √† EKS Il existe plusieurs approches lorsqu'il s'agit de l'optimisation du cache, chacune pr√©sentant des avantages et inconv√©nients. Cela fait d'ailleurs l'objet de discussions tr√®s int√©ressantes ici. J'ai fait quelques choix qui, selon moi, sont un bon compromis entre disponibilit√© et performances dont voici les principales lignes:\nLe moteur Dagger: Un unique pod expose un service HTTP.\nNodePool sp√©cifique : Un node pool avec des contraintes permettant d'obtenir des disques NVME locaux.\n1 - key: karpenter.k8s.aws/instance-local-nvme 2 operator: Gt 3 values: [\u0026#34;100\u0026#34;] 4 - key: karpenter.k8s.aws/instance-category 5 operator: In 6 values: [\u0026#34;c\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;r\u0026#34;] 7 taints: 8 - key: ogenki/io 9 value: \u0026#34;true\u0026#34; 10 effect: NoSchedule Les points de montages des conteneurs: Lorsqu'un n≈ìud du nodepool io d√©marre, il ex√©cute la commande /usr/bin/setup-local-disks raid0. Cette commande pr√©pare les disques en cr√©ant un array en raid0 et monte les syst√®mes de fichiers des conteneurs dessus. Ainsi, tout cet espace est directement accessible depuis le pod !\n‚ö†Ô∏è Notez que c'est un volume √©ph√©m√®re : les donn√©es sont perdues lorsque le pod est arr√™t√©. C'est cet espace que nous utilisons pour le cache Dagger.\n1... 2 - name: varlibdagger 3 ephemeral: 4 volumeClaimTemplate: 5 spec: 6 accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] 7 resources: 8 requests: 9 storage: 10Gi 10 - name: varrundagger 11 ephemeral: 12 volumeClaimTemplate: 13 spec: 14 accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] 15 resources: 16 requests: 17 storage: 90Gi 18... 1kubectl exec -ti -n tooling dagger-engine-c746bd8b8-b2x6z -- /bin/sh 2/ # df -h | grep nvme 3/dev/nvme3n1 9.7G 128.0K 9.7G 0% /var/lib/dagger 4/dev/nvme2n1 88.0G 24.0K 88.0G 0% /run/buildkit Bonnes pratiques avec Karpenter: Afin d'optimiser la disponibilit√© du moteur Dagger, nous l'avons configur√© avec un Pod Disruption Budget ainsi que l'annotation karpenter.sh/do-not-disrupt: \u0026quot;true\u0026quot;. Par ailleurs il est pr√©f√©rable d'utiliser des instances On-demand, que nous pourrions envisager de r√©server aupr√®s de AWS afin d'obtenir un discount.\nNetwork policies: √âtant donn√© que les runners peuvent ex√©cuter n'importe quel code, il est fortement recommand√© de limiter les flux r√©seaux au strict n√©cessaire, que ce soit pour les self-hosted runners ou le moteur dagger.\nAfin de tester cela, nous allons lancer un job qui cr√©e un conteneur en installer de nombreux paquets relativement lourds. L'id√©e etant que cela prenne en peu de temps.\n.github/workflows/ci.yaml\n1 test-cache: 2 name: Testing in-cluster cache 3 runs-on: dagger-gha-runner-scale-set 4 container: 5 image: smana/dagger-cli:v0.12.1 6 env: 7 _EXPERIMENTAL_DAGGER_RUNNER_HOST: \u0026#34;tcp://dagger-engine:8080\u0026#34; 8 cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }} 9 10 steps: 11 - name: Simulate a build with heavy packages 12 uses: dagger/dagger-for-github@v6 13 with: 14 version: \u0026#34;latest\u0026#34; 15 verb: call 16 module: github.com/shykes/daggerverse.git/wolfi@dfb1f91fa463b779021d65011f0060f7decda0ba 17 args: container --packages \u0026#34;python3,py3-pip,go,rust,clang\u0026#34; ‚ÑπÔ∏è Acc√©der au moteur Dagger distant se fait en utilisant la variable d'environnement _EXPERIMENTAL_DAGGER_RUNNER_HOST\nLors de la premi√®re ex√©cution, le job met 3min et 37secs\nEn revanche tout autre ex√©cution ult√©rieure sera beaucoup plus rapide (10secs)! üéâ üöÄ ü•≥\nEn local üíª , je peux aussi b√©n√©ficier de ce cache en configurant mon environnement comme cela:\n1kubectl port-forward -n tooling svc/dagger-engine 8080 2_EXPERIMENTAL_DAGGER_RUNNER_HOST=\u0026#34;tcp://127.0.0.1:8080\u0026#34; Mes tests locaux seront aussi accessible par la CI et un autre d√©veloppeur reprenant mon travail n'aura pas √† tout reconstruire de z√©ro.\nAttention ‚ûï Cette solution a l'avantage non n√©gigeable de disposer d'un stockage ulta rapide! De plus l'architecture est on ne peut plus simple: un seul moteur Dagger avec un stockage local qui expose un service.\n‚ûñ ‚ö†Ô∏è C'est pourtant loin d'√™tre parfait: il faut, en effet acc√©pter que ce cache soit √©ph√©m√®re malgr√© les pr√©cautions prises pour garantir un niveau de disponibilit√© √©lev√©. Par ailleurs il faut aussi prendre en compte le co√ªt d'une instance qui tourne tout le temps, le scaling ne peut se faire qu'en prenant une machine plus grosse.\nDagger Cloud Dagger Cloud est une solution pour les entreprises permettant une visualisation tr√®s claire de l'ex√©cution des pipelines, avec la possibilit√© de parcourir toutes les √©tapes et d'identifier rapidement les √©ventuelles probl√®mes. C'est gratuit pour un usage individuel et je vous encourage √† tester. Cette offre fourni √©galement une alternative √† la solution propos√©e ci-dessus: un cache distribu√©, g√©r√© par Dagger. (Plus d'informations ici) Your browser does not support the video tag. üí≠ Derni√®res remarques Dagger est un projet assez r√©cent qui √©volue vite, soutenu par une communaut√© toujours plus grande et active. Les questions de scaling abord√©es dans cet article seront probablement am√©lior√©es dans le futur.\nPour les modules disponibles dans le Daggerverse, il est parfois difficile de juger leur qualit√©. Il n'y a pas de modules \u0026quot;valid√©s\u0026quot; ou \u0026quot;officiels\u0026quot;, il faut donc souvent en tester plusieurs, analyser le code et parfois en cr√©er un soi-m√™me.\nCet article vous a permis de d√©couvrir Dagger et ses principales fonctions que j'ai utilis√©es. Mon exp√©rience s'est limit√©e au SDK Golang, mais l'exp√©rience devrait √™tre similaire avec d'autres langages. Je d√©couvre des choses nouvelles chaque jour. La prise en main initiale n'est pas √©vidente pour ceux qui, comme moi, ne d√©veloppent pas quotidiennement, mais plus je l'utilise sur des cas concrets, plus je me sens √† l'aise. J'ai m√™me migr√© les quelques jobs de mon repo √† 100% sur Dagger.\nJe suis pass√© de Makefile √† Task et j'esp√®re maintenant aller plus loin avec Dagger. J'ai l'ambition de construire des pipelines plus complexes, comme la restauration et la v√©rification d'un backup Vault ou encore la cr√©ation et les tests d'un cluster EKS avant de le d√©truire. Quoi qu'il en soit, Dagger fait maintenant partie de ma bo√Æte √† outils ! \u0026#x2705;\nüîñ References Doc Discord Youtube ","link":"https://blog.ogenki.io/fr/post/dagger-intro/","section":"post","tags":["devxp"],"title":"`Dagger`: la pi√®ce manquante de l'exp√©rience d√©veloppeur?"},{"body":"","link":"https://blog.ogenki.io/fr/tags/devxp/","section":"tags","tags":null,"title":"Devxp"},{"body":"Le chiffrement TLS est un standard incontournable dans la s√©curisation des services et applications, que ce soit sur Internet ou au sein m√™me de l'entreprise. Sur Internet, le recours √† un certificat TLS, valid√© par une autorit√© de certification reconnue, est essentiel pour garantir la confidentialit√© des √©changes de donn√©es.\nEn ce qui concerne les communications internes, la PKI priv√©e (Private Public Key Infrastructure) joue un r√¥le crucial dans la distribution et la validation des certificats n√©cessaires au chiffrement des communications au sein de l'entreprise, assurant ainsi une s√©curit√© renforc√©e.\nDans cet article, nous allons plonger dans le vif du sujet : la mise en place d'une gestion efficace et robuste des certificats TLS au sein d'une entreprise. Nous explorerons les meilleures pratiques, les outils et les strat√©gies pour une infrastructure de certificats fiable.\nüéØ Notre objectif Afin que les utilisateurs puissent acc√©der aux applications, nous utiliserons le standard Gateway API. (Je vous invite √† lire mon pr√©c√©dent article sur le sujet.) Dans l'impl√©mentation pr√©sent√©e ci-dessus, un composant joue un r√¥le majeur: Cert-manager. Il s'agit en effet du moteur central qui se chargera de g√©n√©rer et de renouveler les certificats. Pour les applications destin√©es √† rester internes et non expos√©es sur Internet, nous opterons pour la g√©n√©ration de certificats via une PKI priv√©e avec Vault d'Hashicorp. Quant aux applications publiques, elles utiliseront des certificats d√©livr√©s par Let's Encrypt. üõÇ A propos de Let's Encrypt Bas√© sur le protocole ACME (Automatic Certificate Management Environment), cette solution permet une installation et un renouvellement automatiques des certificats.\nLet's Encrypt est simple √† mettre en oeuvre, gratuit et am√©liore la s√©curit√©. Cependant, il est important de noter que la dur√©e des certificats est courte, et n√©cessite donc des renouvellements fr√©quents.\nPour en savoir plus sur son fonctionnement, vous pouvez vous r√©f√©rer √† cette documentation. üîê Une PKI priv√©e avec Vault Une PKI priv√©e, ou Infrastructure √† Cl√© Publique priv√©e, est un syst√®me cryptographique utilis√© au sein d'une organisation pour s√©curiser les donn√©es et les communications. Elle repose sur une Autorit√© de Certification (CA) interne qui √©met des certificats TLS sp√©cifiques √† l'organisation.\nCe syst√®me permet √† une organisation de :\nContr√¥ler enti√®rement les proc√©dures de v√©rification de l'identit√© et de l'authentification, et d'√©mettre des certificats pour des domaines internes, ce qui n'est pas possible avec Let's Encrypt. S√©curiser les communications et les donn√©es internes avec une authentification et un chiffrement forts au sein de l'organisation. Cependant, la mise en ≈ìuvre de ce type d'infrastructure requiert une attention particuli√®re et une gestion de plusieurs composants. Ici, nous allons explorer une des fonctionnalit√©s principales de Vault, qui est initialement un outil de gestion de secrets mais qui peut aussi faire office de PKI interne.\nUne plateforme Cloud Native de r√©f√©rence Toutes les actions r√©alis√©es dans cet article proviennent de ce d√©p√¥t git\nOn y trouve le code Opentofu permettant de d√©ployer et configurer Vault mais aussi de nombreuses sources qui me permettent de construire mes articles de blog. N'h√©sitez pas √† me faire des retours, ouvrir des issues si n√©cessaire ... üôè\n‚úÖ Pr√©requis Three-tier PKI Une infrastructure √† trois niveaux de PKI (Three-tier PKI) comprend une Autorit√© de Certification Racine (CA) au sommet, des Autorit√©s de Certification Interm√©diaires au milieu, et des Entit√©s Finales √† la base. L'Autorit√© de Certification Racine d√©livre des certificats aux Autorit√©s de Certification Interm√©diaires, qui √† leur tour √©mettent des certificats aux utilisateurs finaux ou aux dispositifs. Cette structure renforce la s√©curit√© en r√©duisant l'exposition de l'Autorit√© de Certification Racine et simplifie la gestion et la r√©vocation des certificats, offrant une solution √©volutive et flexible pour la s√©curit√© num√©rique. Pour renforcer la s√©curit√© le syst√®me de gestion de certificats, il est recommand√© de cr√©er une Autorit√© de Certification Racine (AC Racine) hors ligne. Nous devons donc r√©aliser, au pr√©alable, les √©tapes suivantes :\nG√©n√©rer l'Autorit√© de Certification Racine hors ligne : Cette approche minimise les risques de s√©curit√© en isolant l'AC Racine du r√©seau.\nCr√©er une Autorit√© de Certification Interm√©diaire : Elle agit sous l'autorit√© de l'AC Racine et est utilis√©e pour √©mettre des certificats, permettant une gestion plus flexible et s√©curis√©e.\nG√©n√©rer le certificat pour le serveur Vault depuis l'AC Interm√©diaire : Cela assure une cha√Æne de confiance depuis l'AC Racine jusqu'aux certificats utilisateurs finaux, en passant par l'AC Interm√©diaire.\nEn suivant la proc√©dure d√©crite ici vous devriez obtenir les fichiers suivants qui seront utilis√©s dans le reste de cet article. Il s'agit l√† d'une proposition bas√© sur openssl, et vous pouvez utiliser la m√©thode qui vous convient pour parvenir au m√™me r√©sultat\n1cd opentofu/vault/cluster 2 3ls .tls/*.pem 4.tls/bundle.pem .tls/ca-chain.pem .tls/intermediate-ca-key.pem .tls/intermediate-ca.pem .tls/root-ca-key.pem .tls/root-ca.pem .tls/vault-key.pem .tls/vault.pem üèóÔ∏è Construire le cluster Il existe plusieurs m√©thodes pour d√©ployer un cluster Vault mais je n'ai pas trouv√© celle qui me convenait, je l'ai donc construite en prenant les d√©cisions suivantes:\nStockage int√©gr√© bas√© sur le protocole Raft, qui est particuli√®rement adapt√© aux syst√®mes distribu√©s et garantit une r√©silience √©lev√©e. Voici un tableau illustrant la tol√©rance aux pannes en fonction de la taille du cluster :\nCluster size Failure tolerance 1 0 3 1 5 2 7 3 Par cons√©quent notre cluster Vault sera compos√© de 5 membres, ce qui nous permettra de tol√©rer la d√©faillance de 2 n≈ìuds.\nStrat√©gie de n≈ìuds √©ph√©m√®res avec instances SPOT : L'architecture est constitu√©e exclusivement d'instances SPOT pour une efficacit√© optimale en termes de co√ªt. Ce groupe est configur√© avec trois pools d'instances Spot distincts, chacun exploitant un type d'instance diff√©rent. Cette diversification strat√©gique vise √† pallier toute d√©faillance potentielle li√©e √† une p√©nurie sp√©cifique de type d'instance SPOT, assurant ainsi une haute disponibilit√© et une continuit√© de service ininterrompue, tout en maximisant l'efficience des co√ªts.\nFonctionnalit√© de d√©verrouillage automatique de Vault (Unseal) : Cette fonction est essentielle compte tenu de la nature √©ph√©m√®re de nos n≈ìuds. Elle permet de minimiser les temps d'arr√™t et d'√©liminer le besoin d'interventions manuelles pour le d√©verrouillage de Vault.\nCet article n'a pas pour but de d√©crire toutes les √©tapes qui sont disponibles dans la documentation du repo Github. Le fichier de variables Opentofu contient la configuration souhait√©e.\n1name = \u0026#34;ogenki-vault\u0026#34; 2leader_tls_servername = \u0026#34;vault.priv.cloud.ogenki.io\u0026#34; 3domain_name = \u0026#34;priv.cloud.ogenki.io\u0026#34; 4env = \u0026#34;dev\u0026#34; 5mode = \u0026#34;ha\u0026#34; 6region = \u0026#34;eu-west-3\u0026#34; 7enable_ssm = true 8 9# Use hardened AMI 10ami_owner = \u0026#34;xxx\u0026#34; # Compte AWS o√π se trouve l\u0026#39;AMI 11ami_filter = { 12 \u0026#34;name\u0026#34; = [\u0026#34;*hardened-ubuntu-*\u0026#34;] 13} Apr√®s avoir ex√©cut√© l'ensemble des √©tapes, Vault peut √™tre utilis√© et nous obtenons un cluster constitu√© de 5 noeuds.\nüõ†Ô∏è Configuration Le d√©ploiement d'une plateforme compl√®te se fait par √©tapes distinctes car certaines op√©rations doivent √™tre faites manuellement afin de garantir une s√©curit√© optimale: La g√©n√©ration du certificat racine qui doit √™tre conserv√© hors ligne et l'initialisation de Vault avec le token root initial.\nIl faut bien entendu tous les composants r√©seaux afin d'y d√©ployer des machines, puis le cluster Vault peut √™tre install√© et configur√© avant de consid√©rer l'ajout d'autres √©l√©ments d'infrastructure, qui d√©pendront probablement des informations sensibles stock√©es dans Vault.\nLa configuration de Vault est appliqu√©e gr√¢ce au provider Terraform dont l'authentification se fait via un token g√©n√©r√© depuis l'instance Vault. La proposition ici d√©montre comment configurer la PKI et autoriser les applications internes √† interagir avec l'API de Vault et, en particulier, comment configurer Cert-Manager.\nIl suffit donc de d√©clarer les variables propre √† votre organisation\n1domain_name = \u0026#34;priv.cloud.ogenki.io\u0026#34; 2pki_common_name = \u0026#34;Ogenki Vault Issuer\u0026#34; 3pki_country = \u0026#34;France\u0026#34; 4pki_organization = \u0026#34;Ogenki\u0026#34; 5pki_domains = [ 6 \u0026#34;cluster.local\u0026#34;, 7 \u0026#34;priv.cloud.ogenki.io\u0026#34; 8] Apr√®s avoir suivi la proc√©dure, la PKI est configur√©e et il est alors possible de g√©n√©rer des certificats.\nInstaller la CA priv√©e sur les machines Contrairement aux PKI publiques, o√π les certificats sont automatiquement approuv√©s par les logiciels clients, dans une PKI priv√©e, les certificats doivent √™tre approuv√©s manuellement par les utilisateurs ou d√©ploy√©s sur tous les appareils par l'administrateur de domaine‚Äã\nUbuntu Archlinux macOS Windows Server üíæ Sauvegardes planifi√©es Comme toute solution contenant des donn√©es, il est indispensable de les sauvegarder. Notons aussi la sensibilit√© de celles stock√©es dans Vault. Il nous faut donc une sauvegarde r√©guli√®re dans lieu s√©curis√©. La solution propos√©e ici est tout simplement une Cronjob. Elle utilise Crossplane pour construire les ressources AWS et se d√©compose comme suit:\nUn bucket S3 o√π seront stock√©s les snapshots Une polique de r√©tention pour ne conserver que les 30 derni√®res sauvegardes. Le bucket est chiffr√© avec une cl√© KMS sp√©cifique. Un external-secret pour pouvoir r√©cup√©rer les param√®tres d'authentificatinon de l'Approle sp√©cifique √† la Cronjob. Une Cronjob qui ex√©cute le script disponible dans le repo et qui √©ffectue un snapshot tel que d√©crit dans la doc d'Hashicorp. Un r√¥le IRSA qui donne les permissions au pod d'√©crire les snapshots sur S3. üöÄ En pratique avec Gateway API! L'objectif de cet article est de d√©montrer une utilisation concr√®te avec Gateway-API et, en fonction du protocole utilis√©, plusieurs options sont possibles pour s√©curiser les connexions avec du TLS. Nous pouvons notamment faire du Passthrough et faire en sorte que la terminaison TLS se fasse sur l'upstream (expos√© directement par le pod). En revanche pour notre cas d'usage, nous allons utiliser le cas le plus commun: HTTPS au niveau de la Gateway. Voici un exemple simple car il suffit uniquement d'indiquer le secret Kubernetes contenant le certificat\n1listeners: 2- protocol: HTTPS 3 port: 443 4 tls: 5 mode: Terminate 6 certificateRefs: 7 - name: foobar-tls Voyons cela en d√©tail, car il y a certains √©l√©ments √† pr√©parer afin de pouvoir obtenir ces secrets üîç.\n‚òÅÔ∏è Un certificat publique Info Cert-Manager est un outil open source permettant de g√©rer les certificats TLS dans Kubernetes. Il s'agit, en fait, d'un op√©rateur Kubernetes qui est contr√¥l√© par l'usage de CRDs (Custom Resources Definitions): il est en effet possible de g√©n√©rer des certificats en cr√©ant des resources de type certificate. Cert-manager se charge ensuite de v√©rifier qu'ils sont toujours valides et d√©clenche un renouvellement lorsque c'est n√©cessaire. Il peut √™tre int√©gr√© avec un nombre grandissant d'autorit√© de certifications comme Let's Encrypt, Venafi, Google, Vault ... Dans la mise en place de cert-manager avec Let's Encrypt, on utilise un Issuer pour configurer la g√©n√©ration de certificats dans un namespace sp√©cifique. Par contre, un ClusterIssuer √©tend cette capacit√© √† tous les namespaces du cluster, offrant ainsi une solution plus globale et flexible pour la gestion des certificats.\nsecurity/base/cert-manager/le-clusterissuer-prod.yaml\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: letsencrypt-prod 5spec: 6 acme: 7 email: mymail@domain.tld 8 server: https://acme-v02.api.letsencrypt.org/directory 9 privateKeySecretRef: 10 name: ogenki-issuer-account-key 11 solvers: 12 - selector: 13 dnsZones: 14 - \u0026#34;cloud.ogenki.io\u0026#34; 15 dns01: 16 route53: 17 region: eu-west-3 Nous utilisons ici l'instance de prod de Let's Encrypt qui est soumise √† certaines r√®gles et il est recommand√© de commencer vos tests sur l'instance de staging. L'adresse email est utilis√©e pour recevoir des notifications, comme la n√©cessit√© de renouveler Une cl√© ogenki-issuer-account-key est g√©n√©r√©e et est utilis√©e pour s'authentifier aupr√®s du serveur ACME. Le m√©canisme qui permet de prouver la l√©gitimit√© d'une demande de certificat est faite gr√¢ce √† une r√©solution DNS. A pr√©sent, comment pouvons-nous faire appel √† ce ClusterIssuer depuis une resource Gateway-API? Figurez-vous qu'il y a une int√©gration tr√®s simple par l'usage d'une annotation au niveau de la Gateway. Cette solution est exp√©rimentale et requiert un param√®tre sp√©cifique lors du d√©ploiment de cert-manager.\nsecurity/base/cert-manager/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta2 2kind: HelmRelease 3metadata: 4 name: cert-manager 5 namespace: security 6spec: 7 values: 8... 9 featureGates: ExperimentalGatewayAPISupport=true Il est aussi n√©cessaire de donner les permissions au contr√¥leur Cert-manager d'interagir avec Route53 pour pouvoir compl√©ter le challenge DNS. Ici j'utilise une Composition Crossplane. (‚ÑπÔ∏è Si vous souhaitez creuser le sujet c'est par ici.)\nPuis il faut ajouter l'annotation dans la Gateway et indiquer le secret cible.\ninfrastructure/base/gapi/platform-public-gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: platform-public 5 annotations: 6 cert-manager.io/cluster-issuer: letsencrypt-prod 7spec: 8 gatewayClassName: cilium 9 listeners: 10 - name: http 11 hostname: \u0026#34;*.${domain_name}\u0026#34; 12... 13 tls: 14 mode: Terminate 15 certificateRefs: 16 - name: platform-public-tls Lorsque la Gateway est cr√©√©, un certificat est g√©n√©r√©. Ce certificat utilise le ClusterIssuer letsencrypt-prod indiqu√© ci-dessus.\n1kubectl describe certificate -n infrastructure platform-public-tls 2Name: platform-public-tls 3Namespace: infrastructure 4API Version: cert-manager.io/v1 5Kind: Certificate 6... 7Spec: 8 Dns Names: 9 *.cloud.ogenki.io 10 Issuer Ref: 11 Group: cert-manager.io 12 Kind: ClusterIssuer 13 Name: letsencrypt-prod 14 Secret Name: platform-public-tls 15 Usages: 16 digital signature 17 key encipherment 18Status: 19 Conditions: 20 Last Transition Time: 2024-01-24T20:43:26Z 21 Message: Certificate is up to date and has not expired 22 Observed Generation: 1 23 Reason: Ready 24 Status: True 25 Type: Ready 26 Not After: 2024-04-23T19:43:24Z 27 Not Before: 2024-01-24T19:43:25Z 28 Renewal Time: 2024-03-24T19:43:24Z 29 Revision: 1 Enfin, au bout de quelques secondes, un secret Kubernetes est cr√©√© et contient le certificat. Il s'agit d'un secret de type TLS contenant les fichiers tls.crt tls.key et ca.crt\nLe plugin view-cert Les certificats g√©n√©r√©s par cert-manager sont stock√©s dans des secrets Kubernetes. Bien qu'il soit possible de les extraire √† coup de commandes base64 et openssl. Pourquoi ne pas se simplifier la vie? Je suis un adepte de la ligne de commande et j'utilise pour ma part r√©guli√®rement le plugin view-cert qui permet d'afficher une synth√®se des secrets de type tls.\n1kubectl view-cert -n infrastructure platform-public-tls 2[ 3 { 4 \u0026#34;SecretName\u0026#34;: \u0026#34;platform-public-tls\u0026#34;, 5 \u0026#34;Namespace\u0026#34;: \u0026#34;infrastructure\u0026#34;, 6 \u0026#34;Version\u0026#34;: 3, 7 \u0026#34;SerialNumber\u0026#34;: \u0026#34;35f659ad03e437805fbf48111b74738efe3\u0026#34;, 8 \u0026#34;Issuer\u0026#34;: \u0026#34;CN=R3,O=Let\u0026#39;s Encrypt,C=US\u0026#34;, 9 \u0026#34;Validity\u0026#34;: { 10 \u0026#34;NotBefore\u0026#34;: \u0026#34;2024-01-28T09:41:35Z\u0026#34;, 11 \u0026#34;NotAfter\u0026#34;: \u0026#34;2024-04-27T09:41:34Z\u0026#34; 12 }, 13 \u0026#34;Subject\u0026#34;: \u0026#34;CN=*.cloud.ogenki.io\u0026#34;, 14 \u0026#34;IsCA\u0026#34;: false 15 } 16] Il peut √™tre install√© en utilisant krew\n1kubectl krew install view-cert üè† Un certificat priv√©e Pour un certificat priv√© avec Vault, nous devons aussi d√©clarer un ClusterIssuer mais sa d√©finition diff√©re l√©gerement:\nsecurity/base/cert-manager/vault-clusterissuer.yaml\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: vault 5 namespace: security 6spec: 7 vault: 8 server: https://vault.priv.cloud.ogenki.io:8200 9 path: pki_private_issuer/sign/ogenki 10 caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0... 11 auth: 12 appRole: 13 path: approle 14 roleId: f8363d0f-b7db-9b08-67ab-8425ab527587 15 secretRef: 16 name: cert-manager-vault-approle 17 key: secretId L'URL indiqu√©e est celle du serveur Vault. Elle doit √™tre accessible depuis les pods dans Kubernetes Le path dans Vault fait partie de la phase de configuration de Vault. Il s'agit du r√¥le autoris√© √† g√©n√©r√© des certificats. Nous utilisons ici une authentification via un Approle. Pour plus de d√©tails sur l'ensemble des actions n√©cessaires √† la configuration de Cert-Manager avec Vault, vous r√©f√©rer √† cette proc√©dure.\nLa principale diff√©rence avec la m√©thode utilis√©e pour Let's Encrypt r√©side dans le faut que le certificat doit √™tre cr√©√© explicitement. En effet, la m√©thode pr√©c√©dente permettait de le faire automatiquement avec une annotation.\ninfrastructure/base/gapi/platform-private-gateway-certificate.yaml\n1apiVersion: cert-manager.io/v1 2kind: Certificate 3metadata: 4 name: private-gateway-certificate 5spec: 6 secretName: private-gateway-tls 7 duration: 2160h # 90d 8 renewBefore: 360h # 15d 9 commonName: private-gateway.priv.cloud.ogenki.io 10 dnsNames: 11 - gitops-${cluster_name}.priv.${domain_name} 12 - grafana-${cluster_name}.priv.${domain_name} 13 - harbor.priv.${domain_name} 14 issuerRef: 15 name: vault 16 kind: ClusterIssuer 17 group: cert-manager.io Comme on peut le voir, ce certificat pourra √™tre utilis√© pour acc√©der aux applications weave-gitops, grafana et harbor. Il a une dur√©e de validit√© de 90 jours et sera renouvel√© automatiquement 15 jours avant son expiration.\nQuelques secondes apr√®s la cr√©ation de la resource certificate, un secret Kubernetes est g√©n√©r√©.\n1kubectl describe certificates -n infrastructure private-gateway-certificate 2Name: private-gateway-certificate 3Namespace: infrastructure 4API Version: cert-manager.io/v1 5Kind: Certificate 6... 7Spec: 8 Common Name: private-gateway.priv.cloud.ogenki.io 9 Dns Names: 10 gitops-mycluster-0.priv.cloud.ogenki.io 11 grafana-mycluster-0.priv.cloud.ogenki.io 12 harbor.priv.cloud.ogenki.io 13 Duration: 2160h0m0s 14 Issuer Ref: 15 Group: cert-manager.io 16 Kind: ClusterIssuer 17 Name: vault 18 Renew Before: 360h0m0s 19 Secret Name: private-gateway-tls 20Status: 21 Conditions: 22 Last Transition Time: 2024-01-27T19:54:57Z 23 Message: Certificate is up to date and has not expired 24 Observed Generation: 1 25 Reason: Ready 26 Status: True 27 Type: Ready 28 Not After: 2024-04-26T19:54:57Z 29 Not Before: 2024-01-27T19:54:27Z 30 Renewal Time: 2024-04-11T19:54:57Z 31 Revision: 1 32Events: 33 Type Reason Age From Message 34 ---- ------ ---- ---- ------- 35 Normal Issuing 41m cert-manager-certificates-trigger Issuing certificate as Secret does not exist 36 Normal Generated 41m cert-manager-certificates-key-manager Stored new private key in temporary Secret resource \u0026#34;private-gateway-certificate-jggkv\u0026#34; 37 Normal Requested 41m cert-manager-certificates-request-manager Created new CertificateRequest resource \u0026#34;private-gateway-certificate-1\u0026#34; 38 Normal Issuing 38m cert-manager-certificates-issuing The certificate has been successfully issued Enfin, Il suffit d'utiliser ce secret dans la d√©claration de la Gateway priv√©e.\ninfrastructure/base/gapi/platform-private-gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: platform-private 5spec: 6 gatewayClassName: cilium 7 listeners: 8 - name: http 9 hostname: \u0026#34;*.priv.${domain_name}\u0026#34; 10... 11 tls: 12 mode: Terminate 13 certificateRefs: 14 - name: private-gateway-tls Nous pouvons v√©rifier l'autorit√© de certification en utilisant la commande curl:\n1curl --verbose -k https://gitops-mycluster-0.priv.cloud.ogenki.io 2\u0026gt;\u0026amp;1 | grep \u0026#39;issuer:\u0026#39; 2* issuer: O=Ogenki; CN=Ogenki Vault Issuer üí≠ Derni√®res remarques ‚ùì Qui n'a pas subit un incident li√© au renouvellement d'un certificat? ‚ùì Comment obtenir un niveau de s√©curit√© correspondant aux √©xigences de l'entreprise? ‚ùì Comment peut-on se simplifier les t√¢ches op√©rationnelles li√©es √† la maintenance des certificats TLS?\nNous avons pu explorer une r√©ponse concr√®te √† ces questions dans cet article. L'automatisation mise en oeuvre gr√¢ce √† Cert-manager permet de minimiser les t√¢ches op√©rationnelles tout en am√©liorant le niveau de s√©curit√©.\nLa mise en oeuvre avec Let's Encrypt et Gateway API est vraiment super simple! D'autre part le niveau de s√©curit√© apport√© par Vault pour les commmunications internes est vraiment √† consid√©rer. Cependant, il est vrai que cela n√©cessite la mise en oeuvre de plusieurs composants et une rigueur sans faille pour conserver un niveau de s√©curit√© optimal.\nPoints d\u0026#39;attention pour la prod Il est important de rappeler quelques recommandations et bonnes pratiques avant de consid√©rer une mise en production. Afin que cet article reste lisible, certains points ne sont m√™me pas √©t√© adress√©s mais il est primordial de les inclure dans votre strat√©gie:\nConservez le certificat racine hors ligne. En d'autres termes, il est imp√©ratif de le stocker sur un support non connect√© pour le prot√©ger de toute menace potentielle. La r√©vocation de la CA root ou interm√©diaire n'a pas √©t√© √©voqu√©. Ainsi que la mise √† disposition d'une liste de r√©vocation (Certificate Revocation List). L'acc√®s √† l'API Vault doit √™tre rigoureusement restreint √† un r√©seau priv√©. Vous devriez jeter un coup d'oeil √† mon article sur Tailscale. Vous noterez aussi que je ne parle pas du tout d'authentification mais Il est essentiel de configurer un fournisseur d'identit√© d√®s le d√©but et d'activer l'authentification multi-facteurs (MFA) pour renforcer la s√©curit√©. Par ailleurs, il est conseill√© de r√©voquer le token racine de Vault une fois l'authentification et les autorisations ad√©quates mises en place. Si n√©cessaire, le token peut √™tre r√©g√©n√©r√© suivant la proc√©dure disponible ici. Par d√©faut le code propos√© d√©ploie des AMI (Images d'instances AWS) Ubuntu de Canonical. Il est conseill√© d'en utiliser une dont la s√©curit√© a √©t√© renforc√©e (Hardened AMI). J'ai construit la mienne en utilisant ce projet. Afin de pouvoir initialiser Vault une commande doit √™tre lanc√©e sur l'instance ce qui justifie l'utilisation de SSM. Cependant il est conseill√© de le d√©sactiver lorsque la phase d'initialisation est termin√©e (enable_ssm: false dans les variables Opentofu) Envoyez les logs d'audit vers un SIEM afin de pouvoir d√©tecter des comportements suspects. Alerter avant que les certificats n'arrivent √† expiration. Vous pouvez, par exemple, utiliser cet exporter Prometheus opensourc√© par les potes d'Enix üòâ. Il s'agit l√† d'une s√©curit√© suppl√©mentaire sachant que l'architecture propos√©e rend le tout automatis√©. Accordez une attention particuli√®re aux cl√©s KMS: celle utilis√© pour d√©v√©rouiller Vault, mais aussi celle qui permet de cr√©er des snapshots. Elles sont vitalespour la restauration de vos sauvegardes. \u0026quot;Une sauvegarde qui n'est pas v√©rifi√©e ne sert √† rien\u0026quot;: Il faut donc construire un workflow qui permettra de v√©rifier la consistence des donn√©es dans Vault. C'est peut √™tre le sujet d'un autre article, stay tuned! Organisez p√©riodiquement des exercices de reprise apr√®s sinistre (PRA) pour garantir votre capacit√© √† reconstruire l'ensemble du syst√®me √† partir de z√©ro, en vous assurant de disposer de toute la documentation et des outils n√©cessaires. üîñ References Github Issues\nCert-manager et Vault: chaine compl√®te de l'autorit√© de certification Blog posts\nPriv√© vs Public PKI: Construire un plan efficace (Author: Nick Naziridis) PKI Meilleures pratiques pour 2023 Build an Internal PKI with Vault (Author: St√©phane Este-Gracias) Documentation Hashicorp\nA propos du stockage Raft: Reference Architecture Deployment Guide AWS Production hardening PKI ","link":"https://blog.ogenki.io/fr/post/pki-gapi/","section":"post","tags":["security"],"title":"`TLS` avec Gateway API: Une gestion efficace et s√©curis√©e des certificats publiques et priv√©s"},{"body":"","link":"https://blog.ogenki.io/fr/tags/security/","section":"tags","tags":null,"title":"Security"},{"body":" Update 2024-11-23 J'utilise d√©sormais KCL (Kusion Configuration Language) pour les compositions Crossplane.\nAvec l'√©mergence du Platform engineering, on assiste √† une √©volution vers la cr√©ation de solutions dites \u0026quot;self-service\u0026quot; √† destination des d√©veloppeurs. Cette approche permet une standardisation des pratiques DevOps, une meilleure exp√©rience pour les d√©veloppeurs, et une r√©duction de la charge cognitive li√©e √† la gestion des outils.\nCrossplane, un projet sous l'√©gide de la Cloud Native Computing Foundation (CNCF) vise √† devenir le framework incontournable pour cr√©er des plateformes Cloud Natives. Dans mon premier article sur Crossplane, j'ai pr√©sent√© cet outil et expliqu√© comment il utilise les principes GitOPs pour l'infrastructure, permettant ainsi de cr√©er un cluster GKE.\nLe projet, qui f√™te maintenant ses 5 ans üéÇüéâ, a gagn√© en maturit√© et s'est enrichi de nouvelles fonctionnalit√©s au fil du temps.\nDans cet article, nous explorerons certaines fonctionnalit√©s cl√©s de Crossplane, avec un int√©r√™t particulier pour les compositions functions qui g√©n√®rent un vif int√©r√™t au sein de la communaut√©. Allons-nous assister √† un tournant d√©cisif pour le projet ?\nüéØ Notre objectif La documentation de Crossplane est tr√®s bien fournie, nous allons donc passer rapidement sur les concepts de base pour se concentrer sur un cas d'usage concret: D√©ployer Harbor sur un cluster EKS en suivant les recommandations en terme de haute disponibilit√©.\nHarbor Harbor, issue aussi de la CNCF, est une solution de gestion d'artefacts de conteneurs centr√©e sur la s√©curit√©. Son r√¥le principal est de stocker, signer et analyser les vuln√©rabilit√©s des images de conteneurs. Harbor dispose d'un contr√¥le d'acc√®s fin, d'une API ainsi que d'une interface web afin de permettre aux √©quipes de dev d'y acc√©der et g√©rer leurs images simplement.\nLa disponibilit√© d'Harbor d√©pend principalement de ses composants avec des donn√©es persistantes (stateful). L'utilisateur est responsable de leur mise en ≈ìuvre, qui doit √™tre adapt√©e √† l'infrastructure cible. L'article pr√©sente les options choisies pour un niveau de disponibilit√© optimal.\nRedis d√©ploy√© avec le chart Helm de Bitnami en mode \u0026quot;master/slave\u0026quot; Les artefacts sont stock√©s dans un bucket AWS S3 Une instance RDS pour la base de donn√©es PostgreSQL Nous allons maintenant explorer comment Crossplane facilite le provisionnement d'une base de donn√©es (RDS), en offrant un niveau d'abstraction simple, exposant uniquement les options n√©cessaires. üöÄ\nüèóÔ∏è Pr√©requis Avant de pouvoir construire nos compositions, nous devons pr√©parer le terrain car certaines op√©rations pr√©alables sont n√©cessaires. Ces √©tapes sont r√©alis√©es dans un ordre bien pr√©cis:\nD√©ploiement du contr√¥lleur Crossplane en utilisant le chart Helm. Installation des providers et de leur configuration. D√©ploiement de diverses configurations faisant usage des providers install√©s pr√©alablement. Notamment les Compositions et les Composition Functions. D√©clarations de Claims pour consommer les Compositions. Ces √©tapes sont traduites en d√©pendances Flux, et peuvent √™tre consult√©es ici.\nLes sources Toutes les actions r√©alis√©es dans cet article proviennent de ce d√©p√¥t git\nOn peut y trouver de nombreuses sources qui me permettent de construire mes articles de blog. N'h√©sitez pas √† me faire des retours, ouvrir des issues si n√©cessaire ... üôè\nüì¶ Les compositions Pour le dire simplement, une Composition dans Crossplane est un moyen d'agr√®ger et de g√©rer automatiquement plusieurs ressources dont la configuration peut s'av√©rer parfois complexe.\nElle utilise l'API de Kubernetes pour d√©finir et orchestrer non seulement des √©l√©ments d'infrastructure tels que le stockage et le r√©seau, mais aussi de nombreux autres composants (se r√©f√©rer √† la liste des providers). Cette m√©thode offre aux d√©veloppeurs une interface simplifi√©e, repr√©sentant une couche d'abstraction qui masque les d√©tails techniques plus complexes de l'infrastructure sous-jacente.\nPour atteindre mon objectif, qui est de cr√©er une base de donn√©es RDS lors du d√©ploiement de l'application Harbor, j'ai d'abord recherch√© s'il existait un exemple pertinent. Pour ce faire, j'ai utilis√© le marketplace d'Upbound, o√π l'on peut trouver de nombreuses Compositions pouvant servir de point de d√©part.\nEn me basant sur la composition configuration-rds, j'ai souhait√© y ajouter les √©l√©ments suivants:\nüîë Permettre aux pods d'acc√©der √† l'instance. ‚ñ∂Ô∏è Cr√©ation d'un service de type ExternalName avec un nom pr√©dictible qui peut √™tre utilis√© dans la configuration de Harbor üíæ Cr√©ation de bases de donn√©es et des r√¥les qui en seront propri√©taires. ‚ùì Comment cette Composition serait-elle alors utilis√©e si, par exemple, un d√©veloppeur souhaite disposer d'une base de donn√©es? Il suffit de d√©clarer une Claim qui repr√©sente le niveau d'abstraction expos√© aux utilisateurs.\ntooling/base/harbor/sqlinstance.yaml\n1apiVersion: cloud.ogenki.io/v1alpha1 2kind: SQLInstance 3metadata: 4 name: xplane-harbor 5 namespace: tooling 6spec: 7 parameters: 8 engine: postgres 9 engineVersion: \u0026#34;15\u0026#34; 10 size: small 11 storageGB: 20 12 databases: 13 - owner: harbor 14 name: registry 15 passwordSecretRef: 16 namespace: tooling 17 name: harbor-pg-masterpassword 18 key: password 19 compositionRef: 20 name: xsqlinstances.cloud.ogenki.io 21 writeConnectionSecretToRef: 22 name: xplane-harbor-rds ici nous constatons que cela se limite √† une simple ressource avec peu de param√®tres pour exprimer nos souhaits:\nUne instance PostgreSQL en version 15 sera cr√©√©e La type d'instance de celle-ci est laiss√© √† l'appr√©ciation de l'√©quipe plateforme (les mainteneurs de la composition). Dans la Claim ci-dessus nous souhaitons une \u0026quot;petite\u0026quot; instance, qui est traduit par la composition en db.t3.small. infrastructure/base/crossplane/configuration/sql-instance-composition.yaml\n1transforms: 2 - type: map 3 map: 4 large: db.t3.large 5 medium: db.t3.medium 6 small: db.t3.small Le mot de passe de l'utilisateur master est extrait d'un secret harbor-pg-masterpassword, g√©n√©r√© via un External Secret. Une fois l'instance cr√©√©e, les d√©tails pour la connexion sont stock√©s dans un secret xplane-harbor-rds C'est l√† que nous pouvons pleinement appr√©cier la puissance des Compositions Crossplane! En effet, de nombreuses ressources sont g√©n√©r√©es de mani√®re transparente, comme illustr√© par le sch√©ma suivant :\nAu bout de quelques minutes, toutes les ressources sont cr√©√©es. (‚ÑπÔ∏è La CLI Crossplane permet d√©sormais de nombreuses op√©rations, notamment visualiser les ressources d'une Composition. Elle est d√©nomm√©e crank pour la diff√©rencier du binaire crossplane qui est le binaire qui tourne sur Kubernetes. )\n1kubectl get xsqlinstances 2NAME SYNCED READY COMPOSITION AGE 3xplane-harbor-jmdhp True True xsqlinstances.cloud.ogenki.io 8m32s 4 5crank beta trace xsqlinstances.cloud.ogenki.io xplane-harbor-jmdhp 6NAME SYNCED READY STATUS 7XSQLInstance/xplane-harbor-jmdhp True True Available 8‚îú‚îÄ SecurityGroupIngressRule/xplane-harbor-jmdhp-n785k True True Available 9‚îú‚îÄ SecurityGroup/xplane-harbor-jmdhp-8jnhc True True Available 10‚îú‚îÄ Object/external-service-xplane-harbor True True Available 11‚îú‚îÄ Object/providersql-xplane-harbor True True Available 12‚îú‚îÄ Database/registry True True Available 13‚îú‚îÄ Role/harbor True True Available 14‚îú‚îÄ Instance/xplane-harbor-jmdhp-whv4g True True Available 15‚îî‚îÄ SubnetGroup/xplane-harbor-jmdhp-fjfth True True Available Et Harbor devient accessible gr√¢ce √† Cilium et Gateway API (Vous pouvez jeter un oeil √† un pr√©c√©dent post sur le sujet üòâ)\nEnvironmentConfig Les EnvironmentConfigs permettent d'utiliser des variables sp√©cifiques au cluster local. Ces √©l√©ments de configuration sont charg√©s en m√©moire et peuvent ensuite √™tre utilis√©s dans la composition.\n√âtant donn√© que le cluster EKS est cr√©√© avec Opentofu, nous stockons ses propri√©t√©s par le biais de variables Flux. (plus d'infos sur les variables de substitution ici)\ninfrastructure/base/crossplane/configuration/environmentconfig.yaml\n1apiVersion: apiextensions.crossplane.io/v1alpha1 2kind: EnvironmentConfig 3metadata: 4 name: eks-environment 5data: 6 clusterName: ${cluster_name} 7 oidcUrl: ${oidc_issuer_url} 8 oidcHost: ${oidc_issuer_host} 9 oidcArn: ${oidc_provider_arn} 10 accountId: ${aws_account_id} 11 region: ${region} 12 vpcId: ${vpc_id} 13 CIDRBlock: ${vpc_cidr_block} 14 privateSubnetIds: ${private_subnet_ids} Ces variables peuvent ensuite √™tre utilis√©es dans les Compositions via la directive FromEnvironmentFieldPath. Par exemple pour permettre aux pods d'acc√©der √† notre instance RDS, nous autorisons le CIDR du VPC:\ninfrastructure/base/crossplane/configuration/irsa-composition.yaml\n1- name: SecurityGroupIngressRule 2 base: 3 apiVersion: ec2.aws.upbound.io/v1beta1 4 kind: SecurityGroupIngressRule 5 spec: 6 forProvider: 7 cidrIpv4: \u0026#34;\u0026#34; 8 patches: 9... 10 - fromFieldPath: CIDRBlock 11 toFieldPath: spec.forProvider.cidrIpv4 12 type: FromEnvironmentFieldPath ‚ö†Ô∏è A l'heure o√π j'√©cris cet article, la fonctionnalit√© est toujours en alpha.\nüõ†Ô∏è Les fonctions de compositions Les fonctions de compositions (Composition Functions) repr√©sentent une √©volution significative pour le d√©veloppement de Compositions. En effet, la m√©thode traditionnelle de patching dans une composition pr√©sentait certaines limitations, telles que l'incapacit√© √† utiliser des conditions, des boucles dans le code, ou √† ex√©cuter des fonctions avanc√©es (ex: calcul de sous-r√©seaux, v√©rification de l'√©tat des ressources externes...).\nLes Composition Functions permettent de lever ces limites et sont en r√©alit√© des programmes qui √©tendent les capacit√©s de templating des ressources au sein de Crossplane. Elles sont r√©dig√©es dans n'importe quel langage de programmation, offrant ainsi une souplesse et une puissance presque infinies lors de la d√©finition des compositions. Cela permet d√©sormais des t√¢ches complexes telles que les transformations conditionnelles, les it√©rations, et les op√©rations dynamiques.\nCes fonctions sont ex√©cut√©es de mani√®re s√©quentielle (mode Pipeline), chaque fonction manipulant et transformant les ressources, puis transmettant le r√©sultat √† la fonction suivante, ouvrant ainsi la porte √† des combinaisons puissantes.\nMais revenons √† notre composition RDS üîç ! Celle-ci utilise, en effet, cette nouvelle fa√ßon de d√©finir des Compositions et est compos√©e de 3 √©tapes:\ninfrastructure/base/crossplane/configuration/sql-instance-composition.yaml\n1apiVersion: apiextensions.crossplane.io/v1 2kind: Composition 3metadata: 4 name: xsqlinstances.cloud.ogenki.io 5... 6spec: 7 mode: Pipeline 8... 9 pipeline: 10 - step: patch-and-transform 11 functionRef: 12 name: function-patch-and-transform 13... 14 - step: sql-go-templating 15 functionRef: 16 name: function-go-templating 17... 18 - step: ready 19 functionRef: 20 name: function-auto-ready La syntaxe de la premi√®re √©tape patch-and-transform vous parait probablement famili√®re üòâ. Il s'agit en effet de la m√©thode de patching traditionnelle de Crossplane mais cette fois ci, elle est ex√©cut√©e en tant que fonction dans le Pipeline. La seconde √©tape consiste √† faire appel √† la fonction function-go-templating dont nous allons parler un peu plus loin. Enfin la derni√®re √©tape utilise la fonction function-auto-ready qui permet de v√©rifier que la ressource composite (XR) est pr√™te. C'est √† dire que l'ensemble des ressources qui la compose ont atteint l'√©tat Ready Migration Si vous avez d√©j√† des Compositions dans le format pr√©c√©dent (Patch \u0026amp; Transforms), il existe un super outil qui permet de migrer vers le mode Pipeline: crossplane-migrator\nInstaller crossplane-migrator 1go install github.com/crossplane-contrib/crossplane-migrator@latest Puis lancer la commande suivante qui permettra d'obtenir le bon format dans compostion-pipeline.yaml 1crossplane-migrator new-pipeline-composition --function-name crossplane-contrib-function-patch-and-transform -i composition.yaml -o composition-pipeline.yaml ‚ÑπÔ∏è Cet outil devrait √™tre ajout√© √† la CLI Crossplane en version 1.15\nüêπ Du Go Template dans Crossplane Comme √©voqu√© pr√©c√©demment, la puissance des Composition functions r√©side principalement dans le fait que n'importe quel langage peut √™tre utilis√©. Il est notamment possible de g√©n√©rer des ressources √† partir de templates Go. Cela n'est pas si diff√©rent de l'√©criture de Charts Helm.\nIl suffit de faire appel √† la fonction et lui fournir en entr√©e le template permettant de g√©n√©rer des ressources Kubernetes. Dans la composition SQLInstance, les YAMLs sont g√©n√©r√©s directement en ligne mais il est aussi possible de charger des fichiers locaux (source: Filesystem)\n1 - step: sql-go-templating 2 functionRef: 3 name: function-go-templating 4 input: 5 apiVersion: gotemplating.fn.crossplane.io/v1beta1 6 kind: GoTemplate 7 source: Inline 8 inline: 9 template: | 10 ... Ensuite c'est √† vous de jouer! Par exemple il y a peu de diff√©rence pour g√©n√©rer une base de donn√©es MariaDB ou PostgreSQL et nous pouvons donc formuler des conditions comme la suivante:\n1{{- $apiVersion := \u0026#34;\u0026#34; }} 2{{- if eq $parameters.engine \u0026#34;postgres\u0026#34; }} 3 {{- $apiVersion = \u0026#34;postgresql.sql.crossplane.io/v1alpha1\u0026#34; }} 4{{- else }} 5 {{- $apiVersion = \u0026#34;mysql.sql.crossplane.io/v1alpha1\u0026#34; }} 6{{- end -}} Cela m'a aussi permit de d√©finir une liste de bases de donn√©es ainsi que le propri√©taire associ√©\n1apiVersion: cloud.ogenki.io/v1alpha1 2kind: SQLInstance 3metadata: 4... 5spec: 6 parameters: 7... 8 databases: 9 - owner: owner1 10 name: db1 11 - owner: owner2 12 name: db2 Puis d'utiliser des boucles Golang pour les cr√©er en utilisant le provider SQL.\n1{{- range $parameters.databases }} 2--- 3apiVersion: {{ $apiVersion }} 4kind: Database 5metadata: 6 name: {{ .name | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} 7 annotations: 8 {{ setResourceNameAnnotation (print \u0026#34;db-\u0026#34; (replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; .name)) }} 9spec: 10... 11{{- end }} Il est m√™me possible de d√©velopper une logique plus complexe dans des fonctions go template avec les directives habituelles define et include. Voici un extrait des exemples disponibles dans le repo de la fonction\n1{{- define \u0026#34;labels\u0026#34; -}} 2some-text: {{.val1}} 3other-text: {{.val2}} 4{{- end }} 5... 6labels: 7 {{- include \u0026#34;labels\u0026#34; $vals | nindent 4}} 8... Enfin nous pouvons tester la Composition et afficher le rendu du template avec la commande suivante:\n1crank beta render tooling/base/harbor/sqlinstance.yaml infrastructure/base/crossplane/configuration/sql-instance-composition.yaml infrastructure/base/crossplane/configuration/function-go-templating.yaml Comme on peut le voir, les possibilit√©s sont d√©cupl√©es gr√¢ce √† la capacit√© de construire des ressources en utilisant un langage de programmation. Cependant, il est √©galement n√©cessaire de veiller √† ce que la composition reste lisible et maintenable sur le long terme. Nous assisterons probablement √† l'√©mergence de bonnes pratiques √† mesure que nous gagnons en exp√©rience sur l'utilisation de ces fonctions.\nüí≠ Derni√®res remarques Lorsqu'on parle d'Infrastructure As Code, Terraform est souvent le premier nom qui nous vient √† l'esprit. Cet outil, soutenu par une vaste communaut√© et qui dispose d'un √©cosyst√®me bien √©tabli, reste un choix de premier ordre. Mais il est int√©ressant de se demander comment Terraform a √©volu√© face aux nouveaux paradigmes apport√©s par Kubernetes. Nous avons abord√© cette question dans notre article sur terraform controller. Depuis, vous avez s√ªrement remarqu√© le petit s√©isme caus√© par la d√©cision de Hashicorp de passer sous licence BSL. Cette √©volution a suscit√© de nombreuses r√©actions et a peut-√™tre influenc√© la strat√©gie et la roadmap d'autres solutions...\nIl est difficile de dire si c'est une r√©action directe, mais r√©cemment, Crossplane a mis √† jour sa charte pour √©largir son champ d'action √† l'ensemble de l'√©cosyst√®me (providers, functions), notamment en int√©grant le projet Upjet sous l'√©gide de la CNCF. L'objectif de cette d√©marche est de renforcer la gouvernance des projets associ√©s et d'am√©liorer, en fin de compte, l'exp√©rience des d√©veloppeurs.\nPersonnellement, j'utilise Crossplane depuis un moment pour des cas d'usage sp√©cifiques. Je l'ai m√™me mis en production dans une entreprise, utilisant une composition pour d√©finir des permissions sp√©cifiques pour les pods sur EKS (IRSA). Nous avions √©galement restreint les types de ressources qu'un d√©veloppeur pouvait d√©clarer.\n‚ùì Alors, que penser de cette nouvelle exp√©rience avec Crossplane?\nIl faut le dire, les Composition Functions offrent de larges perspectives et on peut s'attendre √† voir de nombreuses fonctions appara√Ætre en 2024 üöÄ\nToutefois, √† mon avis, il est crucial que les outils de d√©veloppement et d'exploitation se perfectionnent pour favoriser l'adoption du projet. Par exemple, une interface web ou un plugin k9s seraient utiles.\nPour un d√©butant souhaitant d√©velopper une composition ou une fonction, le premier pas peut sembler difficile. La validation d'une composition n'est pas simple, et les exemples √† suivre ne sont pas tr√®s nombreux. On esp√®re que le marketplace s'enrichira avec le temps.\nCela dit, ces pr√©occupations ont √©t√© prises en compte par la communaut√© de Crossplane, notamment par le SIG Dev XP dont il faut f√©liciter l'effort et qui m√®ne actuellement un travail important. üëè\nJe vous encourage √† suivre de pr√®s l'√©volution du projet dans les prochains mois üëÄ, et √† tenter l'exp√©rience Crossplane pour vous faire votre propre id√©e. Pour ma part, je suis particuli√®rement int√©ress√© par la fonction CUElang, un langage sur lequel je pr√©vois de me pencher prochainement.\nüîñ References Crossplane blog: Improve Crossplane Compositions Authoring with go-templating-function Dev XP Roadmap Vid√©o (Kubecon NA 2023): Crossplane Intro and Deep Dive - the Cloud Native Control Plane Framework Vid√©o (DevOps Toolkit): Crossplane Composition Functions: Unleashing the Full Potential ","link":"https://blog.ogenki.io/fr/post/crossplane_composition_functions/","section":"post","tags":["infrastructure","devxp","gitops"],"title":"Aller plus loin avec `Crossplane`: Compositions et fonctions"},{"body":"","link":"https://blog.ogenki.io/fr/tags/gitops/","section":"tags","tags":null,"title":"Gitops"},{"body":"","link":"https://blog.ogenki.io/fr/tags/infrastructure/","section":"tags","tags":null,"title":"Infrastructure"},{"body":"","link":"https://blog.ogenki.io/fr/tags/network/","section":"tags","tags":null,"title":"Network"},{"body":"Lorsqu'on parle de s√©curisation de l'acc√®s aux ressources Cloud, l'une des r√®gles d'or est d'√©viter les expositions directes √† Internet. La question qui se pose alors pour les Devs/Ops est : comment, par exemple, acc√©der √† une base de donn√©es, un cluster Kubernetes ou un serveur via SSH sans compromettre la s√©curit√©? Les r√©seaux priv√©s virtuels (VPN) offrent une r√©ponse en √©tablissant un lien s√©curis√© entre diff√©rents √©l√©ments d'un r√©seau, ind√©pendamment de leur localisation g√©ographique. De nombreuses solutions existent, allant de mod√®les en SaaS aux solutions que l'on peut h√©berger soi-m√™me, utilisant divers protocoles et √©tant soit open source, soit propri√©taires.\nParmi ces options, je souhaitais vous parler de Tailscale. Cette solution utilise le protocole WireGuard, r√©put√© pour sa simplicit√© et sa performance. Avec Tailscale, il est possible de connecter des appareils ou serveurs de mani√®re s√©curis√©e, comme s'ils √©taient sur un m√™me r√©seau local, bien qu'ils soient r√©partis √† travers le monde.\nüéØ Nos objectifs Comprendre comment fonctionne Tailscale Mise en oeuvre d'une connexion s√©curis√©e avec AWS en quelques minutes Interragir avec l'API d'un cluster EKS via un r√©seau priv√© Acc√©der √† des services h√©berg√©s sur Kubernetes en utilisant le r√©seau priv√© Pour le reste de cet article il faudra √©videmment cr√©er un compte Tailscale. A noter que l'authentification est d√©l√©gu√©e √† des fournisseurs d'identit√© tiers (ex: Okta, Onelogin, Google ...).\nLorsque le compte est cr√©e, on a directement acc√®s √† la console de gestion ci-dessus. Elle permet notamment de lister les appareils connect√©s, de consulter les logs, de modifier la plupart des param√®tres...\nüí° Sous le capot Terminologie Mesh VPN: Un mesh VPN est un type de r√©seau VPN o√π chaque n≈ìud (c'est-√†-dire chaque appareil ou machine) est connect√© √† tous les autres n≈ìuds du r√©seau, formant ainsi un maillage. √Ä distinguer des configurations VPN traditionnelles qui sont con√ßues g√©n√©ralement \u0026quot;en √©toile\u0026quot;, o√π plusieurs clients se connectent √† un serveur central.\nZero trust: Signifie que chaque demande d'acc√®s √† un r√©seau est trait√©e comme si elle venait d'une source non fiable. Une application ou utilisateur doit prouver son identit√© et √™tre autoris√©e avant d'acc√©der √† une ressource. On ne fait pas confiance simplement parce qu'une machine ou un utilisateur provient d'un r√©seau interne ou d'une certaine zone g√©ographique.\nTailnet: D√®s la premi√®re utilisation de Tailscale, un Tailnet est cr√©e pour vous et correspond √† votre propre r√©seau priv√©. Chaque appareil dans un tailnet re√ßoit une IP Tailscale unique, permettant une communication directe entre eux.\nL'architecture de Tailscale est con√ßue de telle sorte que le Control plane et le Data plane sont clairement s√©par√©s:\nD'une part, il y a le serveur de coordination. Son r√¥le est d'√©changer des m√©tadonn√©es et des cl√©s publiques entre tous les participants d'un Tailnet (La cl√© priv√©e √©tant gard√©e en toute s√©curit√© son n≈ìud d'origine).\nD'autre part, les n≈ìuds du Tailnet s'organisent en un r√©seau maill√© (Mesh). Au lieu de passer par le serveur de coordination pour √©changer des donn√©es, ces n≈ìuds communiquent directement les uns avec les autres en mode point √† point. Chaque n≈ìud dispose d'une identit√© unique pour s'authentifier et rejoindre le Tailnet.\n\u0026#x1f4e5; Installation du client La majorit√© des plateformes sont support√©es et les proc√©dures d'installation sont list√©es ici. En ce qui me concerne je suis sur Archlinux:\n1sudo pacman -S tailscale Il est possible de d√©marrer le service automatiquement au d√©marrage de la machine.\n1sudo systemctl enable --now tailscaled Pour enregistrer son ordinateur perso, lancer la commande suivante:\n1sudo tailscale up --accept-routes 2 3To authenticate, visit: 4 5 https://login.tailscale.com/a/f50... ‚ÑπÔ∏è l'option --accept-routes est n√©cessaire sur Linux et permettra d'accepter les routes annonc√©es par les Subnet routers. On verra cela dans la suite de l'article\nV√©rifier que vous avez bien obtenu une IP du r√©seau Tailscale:\n1tailscale ip -4 2100.118.83.67 3 4tailscale status 5100.118.83.67 ogenki smainklh@ linux - ‚ÑπÔ∏è Pour les utilisateurs de Linux, v√©rifier que Tailscale fonctionne bien avec votre configuration DNS: Suivre cette documentation.\nLes sources Toutes les √©tapes r√©alis√©es dans cet article proviennent de ce d√©p√¥t git\nIl va permettre de cr√©er l'ensemble des composants qui ont pour objectif d'obtenir un cluster EKS de Lab et font suite √† un pr√©c√©dent article sur Cilium et Gateway API.\n‚òÅÔ∏è Acc√©der √† AWS en priv√© Afin de pouvoir acc√©der de mani√®re s√©curis√©e √† l'ensemble des ressources disponibles sur AWS, il est possible de d√©ployer un Subnet router.\nUn Subnet router est une instance Tailscale qui permet d'acc√©der √† des sous-r√©seaux qui ne sont pas directement li√©s √† Tailscale. Il fait office de pont entre le r√©seau priv√© virtuel de Tailscale (Tailnet) et d'autres r√©seaux locaux.\nNous pouvons alors router des sous r√©seaux du Clouder √† travers le VPN de Tailscale.\n‚ö†Ô∏è Pour ce faire, sur AWS, il faudra bien entendu configurer les security groups correctement pour autoriser les Subnet routers.\nüöÄ D√©ployer un Subnet router Entrons dans le vif du sujet et deployons un Subnet router sur un r√©seau AWS! Tout est fait en utilisant le code Terraform pr√©sent dans le r√©pertoire opentofu/network. Nous allons analyser la configuration sp√©cifique √† Tailscale qui est pr√©sente dans le fichier tailscale.tf avant de proc√©der au d√©ploiement.\nLe provider Terraform Il est possible de configurer certains param√®tres au travers de l'API Tailscale gr√¢ce au provider Terraform. Pour cela il faut au pr√©alable g√©nerer une cl√© d'API üîë sur la console d'admin:\nIl faudra conserver cette cl√© dans un endroit s√©curis√© car elle est utilis√©e pour d√©ployer le Subnet router\n1provider \u0026#34;tailscale\u0026#34; { 2 api_key = var.tailscale.api_key 3 tailnet = var.tailscale.tailnet 4} les ACL's\nLes ACL's permettent de d√©finir qui est autoris√© √† communiquer avec qui (utilisateur ou appareil). √Ä la cr√©ation d'un compte, celle-cis sont tr√®s permissives et il n'y a aucune restriction (tout le monde peut parler avec tout le monde).\n1resource \u0026#34;tailscale_acl\u0026#34; \u0026#34;this\u0026#34; { 2 acl = jsonencode({ 3 acls = [ 4 { 5 action = \u0026#34;accept\u0026#34; 6 src = [\u0026#34;*\u0026#34;] 7 dst = [\u0026#34;*:*\u0026#34;] 8 } 9 ] 10... 11} Note Pour mon environnement de Lab, j'ai conserv√© cette configuration par d√©fault car je suis la seule personne √† y acc√©der. De plus les seuls appareils connect√©s √† mon Tailnet sont mon laptop et le Subnet router. En revanche dans un cadre d'entreprise, il faudra bien y r√©fl√©chir. Il est alors possible de d√©finir une politique bas√©e sur des groupes d'utilisitateurs ou sur les tags des noeuds.\nConsulter cette doc pour plus d'info.\nLes noms de domaines (DNS)\nIl y a diff√©rentes fa√ßons possibles de g√©rer les noms de domaines avec Tailscale:\nMagic DNS: Lorsqu'un appareil rejoint le Tailnet, il s'enregistre avec un nom et celui-ci peut-√™tre utilis√© directement pour communiquer avec l'appareil.\n1tailscale status 2100.118.83.67 ogenki smainklh@ linux - 3100.115.31.152 ip-10-0-43-98 smainklh@ linux active; relay \u0026#34;par\u0026#34;, tx 3044 rx 2588 4 5ping ip-10-0-43-98 6PING ip-10-0-43-98.tail9c382.ts.net (100.115.31.152) 56(84) bytes of data. 764 bytes from ip-10-0-43-98.tail9c382.ts.net (100.115.31.152): icmp_seq=1 ttl=64 time=11.4 ms AWS: Pour utiliser les noms de domaines internes √† AWS il est possible d'utiliser la deuxi√®me IP du VPC qui correspond toujours au serveur DNS. Cela permet d'utiliser les √©ventuelles zones priv√©es sur route53 ou de se connecter aux ressources en utilisant les noms de domaines.\nLa configuration la plus simple est donc de d√©clarer la liste des serveurs DNS √† utiliser et d'y ajouter celui de AWS. Ici un exemple avec le DNS publique de Cloudflare.\n1resource \u0026#34;tailscale_dns_nameservers\u0026#34; \u0026#34;this\u0026#34; { 2 nameservers = [ 3 \u0026#34;1.1.1.1\u0026#34;, 4 cidrhost(module.vpc.vpc_cidr_block, 2) 5 ] 6} La cl√© d'authentification (\u0026quot;auth key\u0026quot;)\nPour qu'un appareil puisse rejoindre le Tailnet au d√©marrage il faut que Tailscale soit d√©marr√© en utilisant une cl√© d'authentification. Celle-ci est g√©n√©r√©e comme suit\n1resource \u0026#34;tailscale_tailnet_key\u0026#34; \u0026#34;this\u0026#34; { 2 reusable = true 3 ephemeral = false 4 preauthorized = true 5} reusable: S'agissant d'un autoscaling group, il faut que cette m√™me cl√© puisse √™tre utilis√©e plusieurs fois. ephemeral: Pour cette d√©mo nous cr√©ons une cl√© qui n'expire pas. En production il serait pr√©f√©rable d'activer l'expiration. preauthorized: Il faut que cette cl√© soit d√©j√† valide et autoris√©e pour que l'instance rejoigne automatiquement le Tailscale. La cl√© ainsi g√©n√©r√©e est utilis√©e pour lancer tailscale avec le param√®tre --auth-key\n1sudo tailscale up --authkey=\u0026lt;REDACTED\u0026gt; Annoncer les routes pour les r√©seaux AWS\nEnfin il faut annoncer le r√©seau que l'on souhaite faire passer par le Subnet router. Dans notre exemple, nous d√©cidons de router tout le r√©seau du VPC qui a pour CIDR 10.0.0.0/16.\nAfin que cela soit possible de fa√ßon automatique, il y a une r√®gle autoApprovers √† ajouter. Cela permet d'indiquer que les routes annonc√©es par l'utilisateur smainklh@gmail.com sont autoris√©es sans que cela requiert une √©tape d'approbation.\n1 autoApprovers = { 2 routes = { 3 \u0026#34;10.0.0.0/16\u0026#34; = [\u0026#34;smainklh@gmail.com\u0026#34;] 4 } 5 } La commande lanc√©e au d√©marrage de l'instance Subnet router est la suivante:\n1sudo tailscale up --authkey=\u0026lt;REDACTED\u0026gt; --advertise-routes=\u0026#34;10.0.0.0/16\u0026#34; Le module Terraform J'ai cr√©√© un module tr√®s simple qui permet de d√©ployer un autoscaling group sur AWS et de configurer Tailscale. Au d√©marrage de l'instance, elle s'authentifiera en utilisant une auth_key et annoncera les r√©seaux indiqu√©s. Dans l'exemple ci-dessous l'instance annonce le CIDR du VPC sur AWS.\n1module \u0026#34;tailscale_subnet_router\u0026#34; { 2 source = \u0026#34;Smana/tailscale-subnet-router/aws\u0026#34; 3 version = \u0026#34;1.0.4\u0026#34; 4 5 region = var.region 6 env = var.env 7 8 name = var.tailscale.subnet_router_name 9 auth_key = tailscale_tailnet_key.this.key 10 11 vpc_id = module.vpc.vpc_id 12 subnet_ids = module.vpc.private_subnets 13 advertise_routes = [module.vpc.vpc_cidr_block] 14... 15} Maintenant que nous avons analys√© les diff√©rents param√®tres, il est temps de d√©marrer notre Subnet router üöÄ !! Il faut au pr√©alable cr√©er un fichier variable.tfvars dans le r√©pertoire opentofu/network.\n1env = \u0026#34;dev\u0026#34; 2region = \u0026#34;eu-west-3\u0026#34; 3private_domain_name = \u0026#34;priv.cloud.ogenki.io\u0026#34; 4 5tailscale = { 6 subnet_router_name = \u0026#34;ogenki\u0026#34; 7 tailnet = \u0026#34;smainklh@gmail.com\u0026#34; 8 api_key = \u0026#34;tskey-api-...\u0026#34; 9} 10 11tags = { 12 project = \u0026#34;demo-cloud-native-ref\u0026#34; 13 owner = \u0026#34;Smana\u0026#34; 14} Puis lancer la commande suivante:\n1tofu plan --var-file variables.tfvars Apr√®s v√©rification du plan, appliquer les changements\n1tofu apply --var-file variables.tfvars Quand l'instance est d√©marr√©e, elle apparaitra dans la liste des appareils du Tailnet.\n1tailscale status 2100.118.83.67 ogenki smainklh@ linux - 3100.68.109.138 ip-10-0-26-99 smainklh@ linux active; relay \u0026#34;par\u0026#34;, tx 33868 rx 32292 Nous pouvons aussi v√©rifier que la route est bien annonc√©e comme suit:\n1tailscale status --json|jq \u0026#39;.Peer[] | select(.HostName == \u0026#34;ip-10-0-26-99\u0026#34;) .PrimaryRoutes\u0026#39; 2[ 3 \u0026#34;10.0.0.0/16\u0026#34; 4] ‚ö†Ô∏è Pour des raisons de s√©curit√©, pensez √† supprimer le fichier variables.tfvars car il contient la cl√© d'API.\nüëè Et voil√† ! Nous sommes maintenant en mesure d'acc√©der au r√©seau sur AWS, √† condition d'avoir √©galement configur√© les r√®gles de filtrage, comme les ACL et les security groups. Nous pouvons par exemple acc√©der √† une base de donn√©es depuis le poste de travail\n1psql -h demo-tailscale.cymnaynfchjt.eu-west-3.rds.amazonaws.com -U postgres 2Password for user postgres: 3psql (15.4, server 15.3) 4SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, compression: off) 5Type \u0026#34;help\u0026#34; for help. 6 7postgres=\u0026gt; üíª Une autre fa√ßon de faire du SSH Traditionnellement, nous devons parfois nous connecter √† des serveurs en utilisant le protocole SSH. Pour ce faire, il faut g√©n√©rer une cl√© priv√©e et distribuer la cl√© publique correspondante sur les serveurs distants.\nContrairement √† l'utilisation des cl√©s SSH classiques, √©tant donn√© que Tailscale utilise Wireguard pour l'authentification et le chiffrement des connexions il n'est pas n√©cessaire de r√©-authentifier le client. De plus, Tailscale g√®re √©galement la distribution des cl√©s SSH d'h√¥tes. Les r√®gles ACL permettent de r√©voquer l'acc√®s des utilisateurs sans avoir √† supprimer les cl√©s SSH. De plus, il est possible d'activer un mode de v√©rification qui renforce la s√©curit√© en exigeant une r√©-authentification p√©riodique. On peut donc affirmer que l'utilisation de Tailscale SSH simplifie l'authentification, la gestion des connexions SSH et am√©liore le niveau de s√©curit√©.\nLes autorisations pour utiliser SSH sont aussi g√©r√©es au niveau des ACL's\n1... 2 ssh = [ 3 { 4 action = \u0026#34;check\u0026#34; 5 src = [\u0026#34;autogroup:member\u0026#34;] 6 dst = [\u0026#34;autogroup:self\u0026#34;] 7 users = [\u0026#34;autogroup:nonroot\u0026#34;] 8 } 9 ] 10... La r√®gle ci-dessus autorise tous les utilisateurs √† acc√©der √† leurs propres appareils en utilisant SSH. Lorsqu'ils essaient de se connecter, ils doivent utiliser un compte utilisateur autre que root. Pour chaque tentative de connexion, une authentification suppl√©mentaire est n√©cessaire (action=check). Cette authentification se fait en visitant un lien web sp√©cifique\n1ssh ubuntu@ip-10-0-26-99 2... 3# Tailscale SSH requires an additional check. 4# To authenticate, visit: https://login.tailscale.com/a/f1f09a548cc6 5... 6ubuntu@ip-10-0-26-99:~$ Pour que cela soit possible il faut aussi d√©marrer Tailscale avec l'option --ssh\nLes logs d'acc√®s √† la machine peuvent √™tre consult√©s en utilisant journalctl\n1ubuntu@ip-10-0-26-99:~$ journalctl -aeu tailscaled|grep ssh 2Oct 15 15:51:34 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155130-00ede660b8: handling conn: 100.118.83.67:55098-\u0026gt;ubuntu@100.68.109.138:22 3Oct 15 15:51:56 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155156-b6d1dc28c0: handling conn: 100.118.83.67:44560-\u0026gt;ubuntu@100.68.109.138:22 4Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155156-b6d1dc28c0: starting session: sess-20231015T155252-5b2acc170e 5Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): handling new SSH connection from smainklh@gmail.com (100.118.83.67) to ssh-user \u0026#34;ubuntu\u0026#34; 6Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): access granted to smainklh@gmail.com as ssh-user \u0026#34;ubuntu\u0026#34; 7Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): starting pty command: [/usr/sbin/tailscaled be-child ssh --uid=1000 --gid=1000 --groups=1000,4,20,24,25,27,29,30,44,46,115,116 --local-user=ubuntu --remote-user=smainklh@gmail.com --remote-ip=100.118.83.67 --has-tty=true --tty-name=pts/0 --shell --login-cmd=/usr/bin/login --cmd=/bin/bash -- -l] ‚ÑπÔ∏è Avec Tailscale SSH il est possible de se connecter en SSH peu importe o√π est situ√© l'appareil. En revanche dans un contexte 100% AWS, on pr√©ferera probablement utiliser AWS SSM.\nLogs üíæ En s√©curit√© il est primordial de pouvoir conserver les logs pour un usage ult√©rieur. Il existe diff√©rents types de logs:\nLogs d'audit: Ils sont essentiels pour savoir qui a fait quoi. Ils sont accessibles sur la console d'admin et peuvent aussi √™tre envoy√©s vers un SIEM.\nLogs sur les appareils: Ceux-cis peuvent √™tre consult√©s en utilisant les commandes appropri√©es √† l'appareil. (journalctl -u tailscaled sur Linux)\nLogs r√©seau: Utiles pour visualiser quels appareils sont connect√©s les uns aux autres.\n‚ò∏ Qu'en est-il de Kubernetes? Sur Kubernetes il existe plusieurs options pour acc√©der √† un Service:\nProxy: Il s'agit d'un pod suppl√©mentaire qui transfert les appels √† un Service existant. Sidecar: Permet de connecter le pod au Tailnet. Donc la connectivit√© se fait de bout en bout et il est m√™me possible de communiquer dans les 2 sens. (du pod vers les noeuds du Tailnet). Operator: Permet d'exposer les services et l'API Kubernetes (ingress) ainsi que de permettre aux pods d'acc√©der aux noeuds du Tailnet (egress). La configuration se fait en configurant les ressources existantes: Services et Ingresses Dans notre cas, nous disposons d√©j√† d'un Subnet router qui route tout le r√©seau du VPC. Il suffit donc que notre service soit expos√© sur une IP priv√©e.\nL'API Kubernetes Pour acc√©der √† l'API Kubernetes il est n√©cessaire d'autoriser le Subnet router. Cela se fait en d√©finissant la r√®gle suivante pour le security group source.\n1module \u0026#34;eks\u0026#34; { 2... 3 cluster_security_group_additional_rules = { 4 ingress_source_security_group_id = { 5 description = \u0026#34;Ingress from the Tailscale security group to the API server\u0026#34; 6 protocol = \u0026#34;tcp\u0026#34; 7 from_port = 443 8 to_port = 443 9 type = \u0026#34;ingress\u0026#34; 10 source_security_group_id = data.aws_security_group.tailscale.id 11 } 12 } 13... 14} Nous allons v√©rifier que l'API est bien accessible sur une IP priv√©e.\n1CLUSTER_URL=$(TERM=dumb kubectl cluster-info | grep \u0026#34;Kubernetes control plane\u0026#34; | awk \u0026#39;{print $NF}\u0026#39;) 2 3curl -s -o /dev/null -w \u0026#39;%{remote_ip}\\n\u0026#39; ${CLUSTER_URL} 410.228.244.167 5 6kubectl get ns 7NAME STATUS AGE 8cilium-secrets Active 5m46s 9crossplane-system Active 4m1s 10default Active 23m 11flux-system Active 5m29s 12infrastructure Active 4m1s 13... Acc√©der aux services en priv√© Un Service Kubernetes expos√© est une resource AWS comme une autre üòâ. Il faut juste s'assurer que ce service utilise bien une IP priv√©e. Dans mon exemple j'utilise Gateway API pour configurer la r√©partition de charge du Clouder et je vous invite √† lire mon pr√©c√©dent article sur le sujet.\nIl suffirait donc de cr√©er un NLB interne en s'assurant que le Service ait bien l'annotation service.beta.kubernetes.io/aws-load-balancer-scheme ayant pour valeur internal. Dans le cas de Gateway API, cela se fait via la clusterPolicy Kyverno.\n1 metadata: 2 annotations: 3 external-dns.alpha.kubernetes.io/hostname: gitops-${cluster_name}.priv.${domain_name},grafana-${cluster_name}.priv.${domain_name} 4 service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internal\u0026#34; 5 service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp 6 spec: 7 loadBalancerClass: service.k8s.aws/nlb Il y a cependant un pr√©requis suppl√©mentaire car nous ne pouvons pas utiliser Let's Encrypt pour les certificats internes. J'ai donc g√©n√©r√© une PKI interne qui g√©n√®re des certificates auto-sign√©s avec Cert-manager.\nIci je ne d√©taillerai pas le d√©ploiement du cluster EKS, ni la configuration de Flux. Lorsque le cluster est cr√©√© et que toutes les ressources Kubernetes ont √©t√© r√©concili√©, nous avons un service qui est expos√© via un LoadBalancer interne AWS.\n1NLB_DOMAIN=$(kubectl get svc -n infrastructure cilium-gateway-platform -o jsonpath={.status.loadBalancer.ingress[0].hostname}) 2dig +short ${NLB_DOMAIN} 310.0.33.5 410.0.26.228 510.0.9.183 Une entr√©e DNS est √©galement cr√©√©e automatiquement pour les services expos√©s et nous pouvons donc acc√©der en priv√© gr√¢ce √† Tailscale.\n1dig +short gitops-mycluster-0.priv.cloud.ogenki.io 210.0.9.183 310.0.26.228 410.0.33.5 üí≠ Derni√®res remarques Il y a quelques temps, dans le cadre professionnel, j'ai mis en place Cloudflare Zero Trust. Je d√©couvre ici que Tailscale pr√©sente de nombreuses similitudes avec cette solution. La d√©cision entre les deux est loin d'√™tre triviale et d√©pend grandement du contexte. Pour ma part, j'ai √©t√© particuli√®rement convaincu par la simplicit√© de mise en ≈ìuvre de Tailscale, r√©pondant parfaitement √† mon besoin d'acc√©der au r√©seau du Clouder. Bien entendu il existe d'autres solutions comme Teleport, qui offre une approche diff√©rente pour acc√©der √† des ressources internes.\nCela dit, focalisons-nous sur Tailscale.\nUne partie du code de Tailscale est open source, notamment le client qui est sous license BSD 3-Clause. La partie propri√©taire concerne √©ssentiellement la plateforme de coordination. √Ä noter qu'il existe une alternative open source nomm√©e Headscale. Celle-ci est une initiative distincte qui n'a aucun lien avec la soci√©t√© Tailscale.\nPour un usage personnel, Tailscale est vraiment g√©n√©reux, offrant un acc√®s gratuit pour jusqu'√† 100 appareils et 3 utilisateurs. Ceci-dit Tailscale est une option s√©rieuse √† consid√©rer en entreprise et il est important, selon moi, d'encourager ce type d'entreprises qui ont une politique open source claire et un produit de qualit√©.\n","link":"https://blog.ogenki.io/fr/post/tailscale/","section":"post","tags":["security","network"],"title":"S√©curiser le Cloud avec `Tailscale` : Mise en ≈ìuvre d'un VPN simplifi√©e"},{"body":"Lorsque l'on d√©ploie une application sur Kubernetes, l'√©tape suivante consiste g√©n√©ralement √† l'exposer aux utilisateurs. On utilise habituellement des \u0026quot;Ingress controllers\u0026quot;, comme Nginx, Haproxy, Traefik ou encore ceux des diff√©rents Clouders afin de diriger le trafic entrant vers l'application, g√©rer l'√©quilibrage de charge, la terminaison TLS et j'en passe.\nIl faut alors choisir parmi la pl√©thore d'options disponibles ü§Ø la solution qui sera en charge de tous ces aspects et Cilium est, depuis relativement r√©cemment, l'une d'entre elles.\nCilium est une solution Open-Source de connectivit√© r√©seau et de s√©curit√© bas√©e sur eBPF dont l'adoption est grandissante. Il s'agit probablement du plugin r√©seau qui fournit le plus de fonctionnalit√©s. Nous n'allons pas toutes les parcourir mais l'une d'entre elles consiste √† g√©rer le trafic entrant en utilisant le standard Gateway API (GAPI).\nüéØ Notre objectif Comprendre ce qu'est exactement Gateway API et en quoi il s'agit d'une √©volution par rapport √† l'API Ingress. D√©monstrations de cas concrets √† la sauce GitOps. Les limitations actuelles et les √©volutions √† venir. Tip Toutes les √©tapes r√©alis√©es dans cet article proviennent de ce d√©p√¥t git\nJe vous invite √† le parcourir car, il va bien au del√† du contexte de cet article:\nInstallation d'un cluster EKS avec Cilium configur√© en mode sans kube-proxy et un Daemonset d√©di√© √† Envoy Proposition de structure de configuration Flux avec une gestion des d√©pendances et une factorisation que je trouve efficaces. Crossplane et composition IRSA qui simplifie la gestion des permissions IAM pour les composants plateforme Gestion des noms de domaine ainsi que des certificats automatis√©s avec External-DNS et Let's Encrypt L'id√©e √©tant d'avoir l'ensemble configur√© au bout de quelques minutes, en une seule ligne de commande ü§©.\n‚ò∏ Introduction √† Gateway API Comme √©voqu√© pr√©c√©demment, il y a de nombreuses options qui font office d' Ingress controller et chacune a ses propres sp√©cificit√©s et des fonctionnalit√©s particuli√®res, rendant leur utilisation parfois complexe. Par ailleurs, l'API Ingress, utilis√©e historiquement dans Kubernetes poss√®de tr√®s peu d'options. Certaines solutions ont d'ailleurs cr√©√© des CRDs (Ressources personalis√©es) quand d'autres font usage des annotations pour lever ces limites.\nC'est dans ce contexte que Gateway API fait son apparition. Il s'agit d'un standard qui permet de d√©finir des fonctionnalit√©s r√©seau avanc√©es sans n√©cessiter d'extensions sp√©cifiques au contr√¥leur sous-jacent. De plus √©tant donn√© que tous les contr√¥leurs utilisent la m√™me API , il est possible de passer d'une solution √† une autre sans changer de configuration (les ressources qui g√®rent le trafic entrant restent les m√™mes).\nParmi les concepts que nous allons explorer la GAPI introduit un sch√©ma de r√©partition des responsabilit√©s. Elle d√©finit des roles explicites avec des permissions bien distinctes. (Plus d'informations sur le mod√®le de s√©curit√© GAPI ici).\nEnfin il est important de noter que ce projet est dirig√© par le groupe de travail sig-network-kubernetes et un canal slack vous permettra de les solliciter si n√©cessaire.\nVoyons comment cela s'utilise concr√®tement üöÄ!\n\u0026#x2611;\u0026#xfe0f; Pr√©requis Pour le reste de cet article nous consid√©rons qu'un cluster EKS a √©t√© d√©ploy√©. Si vous n'utilisez pas la m√©thode propos√©e dans le repo de d√©mo servant de socle √† cet article, il y a certains points √† valider pour que GAPI puisse √™tre utilis√©.\n‚ÑπÔ∏è La m√©thode d'installation decrite ici se base sur Helm, l'ensemble des values peuvent √™tre consult√©es ici.\nInstaller les CRDs (resources personnalis√©s) disponibles dans le repository Gateway API Note Si Cilium est configur√© avec le support GAPI (voir ci-dessous) et que les CRDs sont absentes, il ne d√©marrera pas.\nDans le repo de demo les CRDs GAPI sont install√©es une premi√®re fois lors de la cr√©ation du cluster afin que Cilium puisse d√©marrer puis elles sont ensuite g√©r√©es par Flux.\nRemplacer kube-proxy par les fonctionnalit√©s de transfert r√©seau apport√©es par Cilium et eBPF.\n1kubeProxyReplacement: true Activer le support de Gateway API 1gatewayAPI: 2 enabled: true V√©rifier L'installation Il faut pour cela installer le client en ligne de commande cilium. J'utilise personnellement asdf pour cela:\n1asdf plugin-add cilium-cli 2asdf install cilium-cli 0.15.7 3asdf global cilium 0.15.7 La commande suivante permet de s'assurer que tous les composants sont d√©marr√©s et op√©rationnels\n1cilium status --wait 2 /¬Ø¬Ø\\ 3/¬Ø¬Ø\\__/¬Ø¬Ø\\ Cilium: OK 4\\__/¬Ø¬Ø\\__/ Operator: OK 5/¬Ø¬Ø\\__/¬Ø¬Ø\\ Envoy DaemonSet: OK 6\\__/¬Ø¬Ø\\__/ Hubble Relay: disabled 7 \\__/ ClusterMesh: disabled 8 9Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 10DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 11DaemonSet cilium-envoy Desired: 2, Ready: 2/2, Available: 2/2 12Containers: cilium Running: 2 13 cilium-operator Running: 2 14 cilium-envoy Running: 2 15Cluster Pods: 33/33 managed by Cilium 16Helm chart version: 1.14.2 17Image versions cilium quay.io/cilium/cilium:v1.14.2@sha256:6263f3a3d5d63b267b538298dbeb5ae87da3efacf09a2c620446c873ba807d35: 2 18 cilium-operator quay.io/cilium/operator-aws:v1.14.2@sha256:8d514a9eaa06b7a704d1ccead8c7e663334975e6584a815efe2b8c15244493f1: 2 19 cilium-envoy quay.io/cilium/cilium-envoy:v1.25.9-e198a2824d309024cb91fb6a984445e73033291d@sha256:52541e1726041b050c5d475b3c527ca4b8da487a0bbb0309f72247e8127af0ec: 2 Enfin le support de GAPI peut √™tre v√©rifi√© comme suit\n1cilium config view | grep -w \u0026#34;enable-gateway-api\u0026#34; 2enable-gateway-api true 3enable-gateway-api-secrets-sync true Il est aussi possible de lancer des tests de connectivit√© pour s'assurer qu'il n'y a pas de probl√®mes avec la configuration r√©seau du cluster:\n1cilium connectivity test ‚ö†Ô∏è Cette commande (connectivity test) provoque actuellement des erreurs lors de l'activation d'Envoy en tant que DaemonSet. (Issue Github).\nInfo as DaemonSet\nPar d√©faut l'agent cilium int√©gre Envoy et lui d√©legue les op√©rations r√©seau de niveau 7. Depuis la version v1.14, il est possible de d√©ployer Envoy s√©par√©ment ce qui apporte certains avantages:\nSi l'on modifie/red√©marre un composant (que ce soit Cilium ou Envoy), cela n'affecte pas l'autre. Mieux attribuer les ressources √† chacun des composants afin d'optimiser les perfs. Limite la surface d'attaque en cas de compromission d'un des pods. Les logs Envoy et de l'agent Cilium ne sont pas m√©lang√©s Il est possible d'utiliser la commande suivante pour v√©rifier que cette fonctionnalit√© est bien active:\n1cilium status 2 /¬Ø¬Ø\\ 3 /¬Ø¬Ø\\__/¬Ø¬Ø\\ Cilium: OK 4 \\__/¬Ø¬Ø\\__/ Operator: OK 5 /¬Ø¬Ø\\__/¬Ø¬Ø\\ Envoy DaemonSet: OK 6 \\__/¬Ø¬Ø\\__/ Hubble Relay: disabled 7 \\__/ ClusterMesh: disabled Plus d'information.\nüö™ La porte d'entr√©e: GatewayClass et Gateway Une fois les conditions n√©cessaires remplies, nous avons acc√®s √† plusieurs √©l√©ments. Nous pouvons notamment utiliser les ressources de la Gateway API gr√¢ce aux CRDs. D'ailleurs, d√®s l'installation de Cilium, une GatewayClass est directement disponible.\n1kubectl get gatewayclasses.gateway.networking.k8s.io 2NAME CONTROLLER ACCEPTED AGE 3cilium io.cilium/gateway-controller True 7m59s Sur un cluster il est possible de configurer plusieurs GatewayClass et donc d'avoir la possibilit√© de faire usage de diff√©rentes impl√©mentations. Nous pouvons par exemple utiliser Linkerd en r√©f√©rencant la GatewayClass dans la configuration de la Gateway.\nLa Gateway est la ressource qui permet d√©clencher la cr√©ation de composants de r√©partition de charge chez le Clouder.\nVoici un exemple simple: apps/base/echo/gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: echo-gateway 5 namespace: echo 6spec: 7 gatewayClassName: cilium 8 listeners: 9 - protocol: HTTP 10 port: 80 11 name: echo-1-echo-server 12 allowedRoutes: 13 namespaces: 14 from: Same Sur AWS (EKS), quand on configure une Gateway, Cilium cr√©e un Service de type LoadBalancer. Ce service est alors interpr√©t√© par un autre contr√¥leur, l'(AWS Load Balancer Controler), qui produit un NLB.\n1kubectl get svc -n echo cilium-gateway-echo 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3cilium-gateway-echo LoadBalancer 172.20.19.82 k8s-echo-ciliumga-64708ec85c-fcb7661f1ae4e4a4.elb.eu-west-3.amazonaws.com 80:30395/TCP 2m58s Il est int√©ressant de noter que l'adresse en question est aussi associ√©e √† la resource Gateway.\n1kubectl get gateway -n echo echo 2NAME CLASS ADDRESS PROGRAMMED AGE 3echo cilium k8s-echo-ciliumga-64708ec85c-fcb7661f1ae4e4a4.elb.eu-west-3.amazonaws.com True 16m \u0026#x21aa;\u0026#xfe0f; Les r√®gles de routage: HTTPRoute Un routage simple Pour r√©sumer le sch√©ma ci-dessus en quelques mots: Une ressource HTTPRoute permet de configurer le routage vers le service en r√©f√©rencant la gateway et en d√©finissant le les param√®tres de routage souhait√©s.\nNote workaround\n√Ä ce jour, il n'est pas possible de configurer les annotations des services g√©n√©r√©s par les Gateways (Issue Github). Une solution de contournement a √©t√© propos√© afin de modifier le service g√©n√©r√© par la Gateway d√®s lors qu'il est cr√©√©.\nKyverno est un outil qui permet de garantir la conformit√© des configurations par rapport aux bonnes pratiques et aux exigences de s√©curit√©. Nous utilisons ici uniquement sa capacit√© √† d√©crire une r√®gle de mutation facilement.\nsecurity/mycluster-0/echo-gw-clusterpolicy.yaml\n1spec: 2 rules: 3 - name: mutate-svc-annotations 4 match: 5 any: 6 - resources: 7 kinds: 8 - Service 9 namespaces: 10 - echo 11 name: cilium-gateway-echo 12 mutate: 13 patchStrategicMerge: 14 metadata: 15 annotations: 16 external-dns.alpha.kubernetes.io/hostname: echo.${domain_name} 17 service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; 18 service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp 19 spec: 20 loadBalancerClass: service.k8s.aws/nlb Le service cilium-gateway-echo se verra donc ajouter les annotations du contr√¥leur AWS ainsi qu'une annotation permettant de configurer une entr√©e DNS automatiquement.\napps/base/echo/httproute.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7 parentRefs: 8 - name: echo 9 namespace: echo 10 rules: 11 - matches: 12 - path: 13 type: PathPrefix 14 value: / 15 backendRefs: 16 - name: echo-1-echo-server 17 port: 80 L'exemple utilis√© ci-dessus est simpliste: toutes les requ√™tes sont transf√©r√©es au service echo-1-echo-server.\nparentRefs permet d'indiquer la Gateway √† utiliser puis les r√®gles de routage sont d√©finies dans rules.\nLes r√®gles de routages pourraient aussi √™tre bas√©es sur le path.\n1... 2spec: 3 hostnames: 4 - foo.bar.com 5 rules: 6 - matches: 7 - path: 8 type: PathPrefix 9 value: /login Ou une ent√™te HTTP\n1... 2spec: 3 rules: 4 - matches: 5 headers: 6 - name: \u0026#34;version\u0026#34; 7 value: \u0026#34;2\u0026#34; 8... V√©rifions que le service est joignable:\n1curl -s http://echo.cloud.ogenki.io | jq -rc \u0026#39;.environment.HOSTNAME\u0026#39; 2echo-1-echo-server-fd88497d-w6sgn Comme vous pouvez le voir le service est expos√© en HTTP sans certificat. Essayons de corriger cela üòâ\nExposer un service en utilisant un certificat TLS Il existe plusieurs m√©thodes pour configurer du TLS avec GAPI. Ici nous allons utiliser le cas le plus commun: protocole HTTPS et terminaison TLS sur la Gateway.\nSupposons que nous souhaitons configurer le nom de domaine echo.cloud.ogenki.io utilis√© pr√©c√©demment. La configuration se fait principalement au niveau de la Gateway\napps/base/echo/tls-gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: echo 5 namespace: echo 6 annotations: 7 cert-manager.io/cluster-issuer: letsencrypt-prod 8spec: 9 gatewayClassName: cilium 10 listeners: 11 - name: http 12 hostname: \u0026#34;echo.${domain_name}\u0026#34; 13 port: 443 14 protocol: HTTPS 15 allowedRoutes: 16 namespaces: 17 from: Same 18 tls: 19 mode: Terminate 20 certificateRefs: 21 - name: echo-tls Le point essentiel ici est la r√©f√©rence √† un secret contenant le certificat echo-tls. Ce certificat peut √™tre cr√©√© manuellement mais j'ai d√©cid√© pour cet article d'automatiser cela avec Let's Encrypt et cert-manager.\nInfo cert-manager\nAvec cert-manager il est tr√®s simple d'automatiser la cr√©ation et la mise √† jour des certificats expos√©s par la Gateway. Pour cela, il faut permettre au contr√¥lleur d'acc√©der √† route53 afin de r√©soudre un challenge DNS01 (M√©canisme qui permet de s'assurer que les clients peuvent seulement demander des certificats pour des domaines qu'ils poss√®dent).\nUne ressource ClusterIssuer d√©crit la configuration n√©cessaire pour g√©n√©rer des certificats gr√¢ce √† cert-manager.\nEnsuite il suffit d'ajouter une annotation cert-manager.io/cluster-issuer et indiquer le secret Kubernetes o√π sera stock√© le certificat.\n‚ÑπÔ∏è Dans le repo de demo les permissions sont attribu√©es en utilisant Crossplane qui se charge de configurer cela au niveau du Cloud AWS.\nPlus d'informations\nPour que le routage se fasse correctement il faut aussi bien entendu r√©f√©rencer la bonne Gateway mais aussi indiquer le nom de domaine dans la ressource HTTPRoute.\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7 parentRefs: 8 - name: echo 9 namespace: echo 10 hostnames: 11 - \u0026#34;echo.${domain_name}\u0026#34; 12... Il faut patienter quelques minutes le temps que le certificat soit cr√©√©.\n1kubectl get cert -n echo 2NAME READY SECRET AGE 3echo-tls True echo-tls 43m Nous pouvons enfin v√©rifier que le certificat est bien issue de Let's Encrypt comme suit:\n1curl https://echo.cloud.ogenki.io -v 2\u0026gt;\u0026amp;1 | grep -A 6 \u0026#39;Server certificate\u0026#39; 2* Server certificate: 3* subject: CN=echo.cloud.ogenki.io 4* start date: Sep 15 14:43:00 2023 GMT 5* expire date: Dec 14 14:42:59 2023 GMT 6* subjectAltName: host \u0026#34;echo.cloud.ogenki.io\u0026#34; matched cert\u0026#39;s \u0026#34;echo.cloud.ogenki.io\u0026#34; 7* issuer: C=US; O=Let\u0026#39;s Encrypt; CN=R3 8* SSL certificate verify ok. Info GAPI permet aussi de configurer le TLS de bout en bout, jusqu'au conteneur. Cela se fait en configurant la Gateway en Passthrough et en utilisant une ressource TLSRoute. Il faut aussi que le certificat soit port√© par le pod qui fait terminaison TLS.\nUne Gateway partag√©e par plusieurs namespaces Avec GAPI il est possible de router le trafic √† travers les Namespaces. Cela est rendu possible gr√¢ce √† des ressources distinctes pour chaque fonction: Une Gateway qui permet de configurer l'infrastructure et notamment de provisionner une adresse IP, et les *Routes. Ces routes peuvent r√©f√©rencer une Gateway situ√©e dans un autre namespace. Il est ainsi possible pour diff√©rent(e)s √©quipes/projets de partager les m√™mes √©l√©ments d'infrastructure.\nIl est cependant requis de sp√©cifier quelle route est autoris√©e √† r√©f√©rencer la Gateway. Ici nous supposons que nous avons une Gateway d√©di√©e aux outils internes qui s'appelle platform. En utilisant le param√®tre allowedRoutes, nous sp√©cifions explicitement quelles sont les namespaces autoris√©s √† r√©f√©rencer cette Gateway.\ninfrastructure/base/gapi/platform-gateway.yaml\n1... 2 allowedRoutes: 3 namespaces: 4 from: Selector 5 selector: 6 matchExpressions: 7 - key: kubernetes.io/metadata.name 8 operator: In 9 values: 10 - observability 11 - flux-system 12 tls: 13 mode: Terminate 14 certificateRefs: 15 - name: platform-tls Les HTTPRoutes situ√©es dans les namespaces observability et flux-system r√©fer√©ncent cette m√™me Gateway.\n1... 2spec: 3 parentRefs: 4 - name: platform 5 namespace: infrastructure Et utilisent le m√™me r√©partiteur de charge du Clouder\n1NLB_DOMAIN=$(kubectl get svc -n infrastructure cilium-gateway-platform -o jsonpath={.status.loadBalancer.ingress[0].hostname}) 2 3dig +short ${NLB_DOMAIN} 413.36.89.108 5 6dig +short grafana-mycluster-0.cloud.ogenki.io 713.36.89.108 8 9dig +short gitops-mycluster-0.cloud.ogenki.io 1013.36.89.108 Note üîí Ces outils internes ne devraient pas √™tre expos√©s sur Internet mais vous comprendrez qu'il s'agit l√† d'une d√©mo üôè. On pourrait par exemple, utiliser et une Gateway interne (IP priv√©e) en jouant sur les annotations et un moyen de connexion priv√© (VPN, tunnels ...)\nTraffic splitting Il est souvent utile de tester une application sur une portion du trafic lorsqu'une nouvelle version est disponible (A/B testing ou Canary deployment). GAPI permet cela de fa√ßon tr√®s simple en utilisant des poids.\nVoici un exemple permettant de tester sur 5% du trafic vers le service echo-2-echo-server\napps/base/echo/httproute-split.yaml\n1... 2 hostnames: 3 - \u0026#34;split-echo.${domain_name}\u0026#34; 4 rules: 5 - matches: 6 - path: 7 type: PathPrefix 8 value: / 9 backendRefs: 10 - name: echo-1-echo-server 11 port: 80 12 weight: 95 13 - name: echo-2-echo-server 14 port: 80 15 weight: 5 V√©rifions que la r√©partition se fait bien comme attendu:\nscripts/check-split.sh\n1./scripts/check-split.sh https://split-echo.cloud.ogenki.io 2Number of requests for echo-1: 95 3Number of requests for echo-2: 5 Manipulation des ent√™tes HTTP (Headers) Il est aussi possible de jouer avec les ent√™tes HTTP (Headers): en ajouter, modifier ou supprimer. Ces modifications peuvent se faire sur les Headers de requ√™te ou de r√©ponse par le biais de filtres ajout√©s √† la ressource HTTPRoute.\nNous allons par exemple ajouter un Header √† la requ√™te\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7... 8 rules: 9 - matches: 10 - path: 11 type: PathPrefix 12 value: /req-header-add 13 filters: 14 - type: RequestHeaderModifier 15 requestHeaderModifier: 16 add: 17 - name: foo 18 value: bar 19 backendRefs: 20 - name: echo-1-echo-server 21 port: 80 22... La commande suivante permet de v√©rifier que le header est bien pr√©sent.\n1curl -s https://echo.cloud.ogenki.io/req-header-add -sk | jq \u0026#39;.request.headers\u0026#39; 2{ 3 \u0026#34;host\u0026#34;: \u0026#34;echo.cloud.ogenki.io\u0026#34;, 4 \u0026#34;user-agent\u0026#34;: \u0026#34;curl/8.2.1\u0026#34;, 5 \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34;, 6 \u0026#34;x-forwarded-for\u0026#34;: \u0026#34;81.220.234.254\u0026#34;, 7 \u0026#34;x-forwarded-proto\u0026#34;: \u0026#34;https\u0026#34;, 8 \u0026#34;x-envoy-external-address\u0026#34;: \u0026#34;81.220.234.254\u0026#34;, 9 \u0026#34;x-request-id\u0026#34;: \u0026#34;320ba4d2-3bd6-4c2f-8a97-74296a9f3f26\u0026#34;, 10 \u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34; 11} ü™™ Les roles et permissions GAPI offre un mod√®le de partage des permissions claire entre l'infrastructure de routage du trafic (g√©r√©e par les administrateurs de cluster) et les applications (g√©r√©es par les d√©veloppeurs).\nLe fait de disposer de plusieurs ressources nous permet d'utiliser les ressources RBAC dans Kubernetes pour attribuer les droits de fa√ßon d√©clarative. J'ai ajout√© quelques exemples qui n'ont aucun effet dans mon cluster de d√©mo mais qui peuvent vous permettre de vous faire une id√©e.\nLa configuration suivante permet aux membres du groupe developers de g√©rer les HTTPRoutes dans le namespace echo. En revanche ils ne poss√©dent que des droits en lecture sur les Gateways.\n1--- 2apiVersion: rbac.authorization.k8s.io/v1 3kind: Role 4metadata: 5 namespace: echo 6 name: gapi-developer 7rules: 8 - apiGroups: [\u0026#34;gateway.networking.k8s.io\u0026#34;] 9 resources: [\u0026#34;httproutes\u0026#34;] 10 verbs: [\u0026#34;*\u0026#34;] 11 - apiGroups: [\u0026#34;gateway.networking.k8s.io\u0026#34;] 12 resources: [\u0026#34;gateways\u0026#34;] 13 verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] 14--- 15apiVersion: rbac.authorization.k8s.io/v1 16kind: RoleBinding 17metadata: 18 name: gapi-developer 19 namespace: echo 20subjects: 21 - kind: Group 22 name: \u0026#34;developers\u0026#34; 23 apiGroup: rbac.authorization.k8s.io 24roleRef: 25 kind: Role 26 name: gapi-developer 27 apiGroup: rbac.authorization.k8s.io ü§î Un p√©rim√®tre pas √©vident Il ne faut pas confondre GAPI avec ce que l'on nomme couramment une API Gateway. Une section de la FAQ a d'ailleurs √©t√© cr√©√© pour √©claircir ce point. Bien que GAPI offre des fonctionnalit√©s typiquement pr√©sentes dans une API Gateway, il s'agit avant tout d'une impl√©mentation sp√©cifique pour Kubernetes. Cependant, ce choix de d√©nomination peut pr√™ter √† confusion.\nIl est essentiel de mentionner que cet article se concentre uniquement sur le trafic entrant, appel√© north-south, traditionnellement g√©r√© par les Ingress Controllers. Ce trafic repr√©sente le p√©rim√®tre initial de GAPI. Une initiative r√©cente nomm√©e GAMMA vise √† √©galement g√©rer le routage east-west, ce qui permettra de standardiser certaines fonctionnalit√©s des solutions de Service Mesh √† l'avenir. (voir cet article pour plus d'informations).\nüí≠ Derni√®res remarques Pour √™tre honn√™te, j'ai entendu parl√© de Gateway API depuis un petit moment. J'ai lu quelques articles mais jusqu'ici je n'avais pas pris le temps d'approfondir le sujet. Je me disais \u0026quot;Pourquoi? J'arrive √† faire ce que je veux avec mon Ingress Controller ? et puis il faut apprendre √† utiliser de nouvelles ressources\u0026quot;.\nGAPI gagne en maturit√© et nous sommes proche d'une version GA. De nombreux projets l'ont d√©j√† adopt√©, Istio et Linkerd par exemple sont totalement compatibles avec la version 0.8.0 et cette fa√ßon de g√©rer le trafic au sein de Kubernetes deviendra rapidement la norme.\nToujours est-il que j'ai beaucoup aim√© la d√©claration des diff√©rentes configurations que je trouve tr√®s intuitive et explicite ‚ù§Ô∏è. D'autre part le mod√®le de s√©curit√© permet de donner le pouvoir aux developpeurs sans sacrifier la s√©curit√©. Enfin la gestion de l'infrastructure se fait de fa√ßon transparente, nous pouvons rapidement passer d'une impl√©mentation (contr√¥leur sous-jacent) √† une autre sans toucher aux *Routes.\nAlors suis-je pr√™t √† changer mon Ingress Controller pour Cilium aujourd'hui? La r√©ponse courte est Non mais bient√¥t!.\nTout d'abord j'aimerais mettre en √©vidence sur l'√©tendue des possiblit√©s offertes par Cilium: De nombreuses personnes se sentent noy√©es sous les nombreux outils qui gravitent autour de Kubernetes. Cilium permettrait de remplir les fonctionnalit√©s de nombre d'entre eux (metrics, tracing, service-mesh, s√©curit√© et ... Ingress Controller avec GAPI).\nCependant, bien que nous puissions faire du routage HTTP de base, il y √† certains points d'attention:\nLe support de TCP et UDP Le support de GRPC Devoir passer par une r√®gle de mutation pour pouvoir configurer les composants cloud. (Issue Github) De nombreuses fonctionnalit√©s explor√©es sont toujours au stade exp√©rimental. On peut citer les fonctions √©tendues qui support√©s depuis quelques jours: J'ai par exemple tent√© de configurer une redirection HTTP\u0026gt;HTTPS simple mais je suis tomb√© sur ce probl√®me. Je m'attends donc √† ce qu'il y ait des changements dans l'API tr√®s prochainement. Je n'ai pas abord√© toutes les fonctionnalit√©s de l'impl√©mentation Cilium de GAPI (Honn√™tement, cet article est d√©j√† bien fourni üòú). N√©anmoins, je suis vraiment convaincu de son potentiel. J'ai bon espoir qu'on pourra bient√¥t envisager son utilisation en production. Si vous n'avez pas encore envisag√© cette transition, c'est le moment de s'y pencher üòâ ! Toutefois, compte tenu des aspects √©voqu√©s pr√©c√©demment, je conseillerais de patienter un peu.\nüîñ References https://gateway-api.sigs.k8s.io/ https://docs.cilium.io/en/latest/network/servicemesh/gateway-api/gateway-api/#gs-gateway-api https://isovalent.com/blog/post/cilium-gateway-api/ https://isovalent.com/blog/post/tutorial-getting-started-with-the-cilium-gateway-api/ Les labs d'Isovalent permettent de rapidement de tester GAPI et vous pourrez ajouter des badges √† votre collection üòÑ ","link":"https://blog.ogenki.io/fr/post/cilium-gateway-api/","section":"post","tags":["kubernetes","infrastructure","network"],"title":"`Gateway API`: Remplacer mon Ingress Controller avec `Cilium`?"},{"body":"","link":"https://blog.ogenki.io/fr/tags/kubernetes/","section":"tags","tags":null,"title":"Kubernetes"},{"body":" Update 2024-11-23 Ne plus utiliser Weave Gitops pour les ressources Flux. Il existe d√©sormais un plugin Headlamp.\nTerraform est probablement l'outil \u0026quot;Infrastructure As Code\u0026quot; le plus utilis√© pour construire, modifier et versionner les changements d'infrastructure Cloud. Il s'agit d'un projet Open Source d√©velopp√© par Hashicorp et qui utilise le langage HCL pour d√©clarer l'√©tat souhait√© de ressources Cloud. L'√©tat des ressources cr√©√©es est stock√© dans un fichier d'√©tat (terraform state).\nOn peut consid√©rer que Terraform est un outil \u0026quot;semi-d√©claratif\u0026quot; car il n'y a pas de fonctionnalit√© de r√©conciliation automatique int√©gr√©e. Il existe diff√©rentes approches pour r√©pondre √† cette probl√©matique, mais en r√®gle g√©n√©rale, une modification sera appliqu√©e en utilisant terraform apply. Le code est bien d√©crit dans des fichiers de configuration HCL (d√©claratif) mais l'ex√©cution est faite de mani√®re imp√©rative. De ce fait, il peut y avoir de la d√©rive entre l'√©tat d√©clar√© et le r√©el (par exemple, un coll√®gue qui serait pass√© sur la console pour changer un param√®tre üòâ).\n‚ùì‚ùì Alors, comment m'assurer que ce qui est commit dans mon repo git est vraiment appliqu√©. Comment √™tre alert√© s'il y a un changement par rapport √† l'√©tat d√©sir√© et comment appliquer automatiquement ce qui est dans mon code (GitOps) ?\nC'est la promesse de tf-controller, un operateur Kubernetes Open Source de Weaveworks, √©troitement li√© √† Flux (un moteur GitOps de la m√™me soci√©t√©). Flux est l'une des solutions que je pl√©biscite, et je vous invite donc √† lire un pr√©c√©dent article.\nInfo L'ensemble des √©tapes d√©crites ci-dessous sont faites avec ce repo Git\nüéØ Notre objectif En suivant les √©tapes de cet article nous visons les objectifs suivant:\nD√©ployer un cluster Kubernetes qui servira de \u0026quot;Control plane\u0026quot;. Pour r√©sumer il h√©bergera le controlleur Terraform qui nous permettra de d√©clarer tous les √©l√©ments d'infrastructure souhait√©s. Utiliser Flux comme moteur GitOps pour toutes les ressources Kubernetes. Concernant le controleur Terraform, nous allons voir:\nQuelle est le moyen de d√©finir des d√©pendances entre modules Cr√©ation de plusieurs ressources AWS: Zone route53, Certificat ACM, r√©seau, cluster EKS. Les diff√©rentes options de reconciliation (automatique, n√©cessitant une confirmation) Comment sauvegarder et restaurer un fichier d'√©tat (tfstate) üõ†Ô∏è Installer le controleur Terraform ‚ò∏ Le cluster \u0026quot;Control Plane\u0026quot; Afin de pouvoir utiliser le controleur Kubernetes tf-controller, il nous faut d'abord un cluster Kubernetes üòÜ. Nous allons donc cr√©er un cluster control plane en utilisant la ligne de commande terraform et les bonnes pratiques EKS.\nWarning Il est primordial que ce cluster soit r√©siliant, s√©curis√© et supervis√© car il sera responsable de la gestion de l'ensemble des ressources AWS cr√©√©es par la suite.\nSans entrer dans le d√©tail, le cluster \u0026quot;control plane\u0026quot; a √©t√© cr√©√© un utilisant ce code. Cel√†-dit, il est important de noter que toutes les op√©rations de d√©ploiement d'application se font en utilisant Flux.\nInfo En suivant les instructions du README, un cluster EKS sera cr√©√© mais pas uniquement! Il faut en effet donner les permissions au controlleur Terraform pour appliquer les changements d'infrastructure. De plus, Flux doit √™tre install√© et configur√© afin d'appliquer la configuration d√©finie ici.\nAu final on se retrouve donc avec plusieurs √©l√©ments install√©s et configur√©s:\nles addons quasi indispensables que sont aws-loadbalancer-controller et external-dns les roles IRSA pour ces m√™mes composants sont install√©s en utilisant tf-controller La stack de supervision Prometheus / Grafana. external-secrets pour pouvoir r√©cup√©rer des √©l√©ments sensibles depuis AWS secretsmanager. Afin de d√©montrer tout cela au bout de quelques minutes l'interface web pour Flux est accessible via l'URL gitops-\u0026lt;cluster_name\u0026gt;.\u0026lt;domain_name\u0026gt; V√©rifier toute de m√™me que le cluster est accessible et que Flux fonctionne correctement\n1aws eks update-kubeconfig --name controlplane-0 --alias controlplane-0 2Updated context controlplane-0 in /home/smana/.kube/config 1flux check 2... 3‚úî all checks passed 4 5flux get kustomizations 6NAME REVISION SUSPENDED READY MESSAGE 7flux-config main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 8flux-system main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 9infrastructure main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 10security main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 11tf-controller main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 12... üì¶ Le chart Helm et Flux Maintenant que notre cluster \u0026quot;controlplane\u0026quot; est op√©rationnel, l'ajout le contr√¥leur Terraform consiste √† utiliser le chart Helm.\nIl faut tout d'abord d√©clarer la source:\nsource.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: tf-controller 5spec: 6 interval: 30m 7 url: https://weaveworks.github.io/tf-controller Et d√©finir la HelmRelease:\nrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: tf-controller 5spec: 6 releaseName: tf-controller 7 chart: 8 spec: 9 chart: tf-controller 10 sourceRef: 11 kind: HelmRepository 12 name: tf-controller 13 namespace: flux-system 14 version: \u0026#34;0.12.0\u0026#34; 15 interval: 10m0s 16 install: 17 remediation: 18 retries: 3 19 values: 20 resources: 21 limits: 22 memory: 1Gi 23 requests: 24 cpu: 200m 25 memory: 500Mi 26 runner: 27 serviceAccount: 28 annotations: 29 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/tfcontroller_${cluster_name}\u0026#34; Lorsque ce changement est √©crit dans le repo Git, la HelmRelease sera d√©ploy√©e et le contr√¥lleur tf-controller d√©marera\n1kubectl get hr -n flux-system 2NAME AGE READY STATUS 3tf-controller 67m True Release reconciliation succeeded 4 5kubectl get po -n flux-system -l app.kubernetes.io/instance=tf-controller 6NAME READY STATUS RESTARTS AGE 7tf-controller-7ffdc69b54-c2brg 1/1 Running 0 2m6s Dans le repo de demo il y a d√©j√† un certain nombre de ressources AWS d√©clar√©es. Par cons√©quent, au bout de quelques minutes, le cluster se charge de la cr√©ation de celles-cis: Info Bien que la majorit√© des t√¢ches puisse √™tre r√©alis√©e de mani√®re d√©clarative ou via les utilitaires de ligne de commande tels que kubectl et flux, un autre outil existe qui offre la possibilit√© d'interagir avec les ressources terraform : tfctl\nüöÄ Appliquer un changement Parmis les bonnes pratiques avec Terraform, il y a l'usage de modules. Un module est un ensemble de ressources Terraform li√©es logigement afin d'obtenir une seule unit√© r√©utilisable. Cela permet d'abstraire la complexit√©, de prendre des entr√©es, effectuer des actions sp√©cifiques et produire des sorties.\nIl est possible de cr√©er ses propres modules et de les mettre √† disposition dans des Sources ou d'utiliser les nombreux modules partag√©s et maintenus par les communaut√©s. Il suffit alors d'indiquer quelques variables afin de l'adapter au contexte.\nAvec tf-controller, la premi√®re √©tape consiste donc √† indiquer la Source du module. Ici nous allons configurer le socle r√©seau sur AWS (vpc, subnets...) avec le module terraform-aws-vpc.\nsources/terraform-aws-vpc.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1 2kind: GitRepository 3metadata: 4 name: terraform-aws-vpc 5 namespace: flux-system 6spec: 7 interval: 30s 8 ref: 9 tag: v5.0.0 10 url: https://github.com/terraform-aws-modules/terraform-aws-vpc Nous pouvons ensuite cr√©er la ressource Terraform qui en fait usage:\nvpc/dev.yaml\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6 interval: 8m 7 path: . 8 destroyResourcesOnDeletion: true # You wouldn\u0026#39;t do that on a prod env ;) 9 storeReadablePlan: human 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-vpc 13 namespace: flux-system 14 vars: 15 - name: name 16 value: vpc-dev 17 - name: cidr 18 value: \u0026#34;10.42.0.0/16\u0026#34; 19 - name: azs 20 value: 21 - \u0026#34;eu-west-3a\u0026#34; 22 - \u0026#34;eu-west-3b\u0026#34; 23 - \u0026#34;eu-west-3c\u0026#34; 24 - name: private_subnets 25 value: 26 - \u0026#34;10.42.0.0/19\u0026#34; 27 - \u0026#34;10.42.32.0/19\u0026#34; 28 - \u0026#34;10.42.64.0/19\u0026#34; 29 - name: public_subnets 30 value: 31 - \u0026#34;10.42.96.0/24\u0026#34; 32 - \u0026#34;10.42.97.0/24\u0026#34; 33 - \u0026#34;10.42.98.0/24\u0026#34; 34 - name: enable_nat_gateway 35 value: true 36 - name: single_nat_gateway 37 value: true 38 - name: private_subnet_tags 39 value: 40 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 41 \u0026#34;karpenter.sh/discovery\u0026#34;: dev 42 - name: public_subnet_tags 43 value: 44 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 45 writeOutputsToSecret: 46 name: vpc-dev Si l'on devait r√©sumer grossi√®rement: le code terraform provenant de la source terraform-aws-vpc est utilis√© avec les variables vars.\nIl y a ensuite plusieurs param√®tres qui influent sur le fonctionnement de tf-controller. Les principaux param√®tres qui permettent de contr√¥ler la fa√ßon dont sont appliqu√©es les modifications sont .spec.approvePlan et .spec.autoApprove\nüö® D√©tection de la d√©rive D√©finir spec.approvePlan avec une valeur √† disable permet uniquement de notifier que l'√©tat actuel des ressources a d√©riv√© par rapport au code Terraform. Cela permet notamment de choisir le moment et la mani√®re dont l'application des changements sera effectu√©e.\nNote De mon point de vue il manque une section sur les notifications: La d√©rive, les plans en attentes, les probl√®mese de r√©concilation. J'essaye d'identifier les m√©thodes possibles (de pr√©f√©rence avec Prometheus) et de mettre √† jour cet article d√®s que possible.\nüîß Application manuelle L'exemple donn√© pr√©c√©demment (vpc-dev) ne contient pas le param√®tre .spec.approvePlan et h√©rite donc de la valeur par d√©faut qui est false. Par cons√©quent, l'application concr√®te des modifications (apply), n'est pas faite automatiquement.\nUn plan est ex√©cut√© et sera en attente d'une validation:\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system vpc-dev Unknown Plan generated: set approvePlan: \u0026#34;plan-v5.0.0-26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. true 2 minutes Je conseille d'ailleurs de configurer le param√®tre storeReadablePlan √† human. Cela permet de visualiser simplement les modifications en attente en utilisant tfctl:\n1tfctl show plan vpc-dev 2 3Terraform used the selected providers to generate the following execution 4plan. ressource actions are indicated with the following symbols: 5 + create 6 7Terraform will perform the following actions: 8 9 # aws_default_network_acl.this[0] will be created 10 + ressource \u0026#34;aws_default_network_acl\u0026#34; \u0026#34;this\u0026#34; { 11 + arn = (known after apply) 12 + default_network_acl_id = (known after apply) 13 + id = (known after apply) 14 + owner_id = (known after apply) 15 + tags = { 16 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 17 } 18 + tags_all = { 19 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 20 } 21 + vpc_id = (known after apply) 22 23 + egress { 24 + action = \u0026#34;allow\u0026#34; 25 + from_port = 0 26 + ipv6_cidr_block = \u0026#34;::/0\u0026#34; 27 + protocol = \u0026#34;-1\u0026#34; 28 + rule_no = 101 29 + to_port = 0 30 } 31 + egress { 32... 33Plan generated: set approvePlan: \u0026#34;plan-v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. 34To set the field, you can also run: 35 36 tfctl approve vpc-dev -f filename.yaml Apr√®s revue des modifications ci-dessus, il suffit donc d'ajouter l'identifiant du plan √† valider et de pousser le changement sur git comme suit:\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6... 7 approvePlan: plan-v5.0.0-26c38a66f1 8... En quelques instants un runner sera lanc√© qui se chargera d'appliquer les changements:\n1kubectl logs -f -n flux-system vpc-dev-tf-runner 22023/07/01 15:33:36 Starting the runner... version sha 3... 4aws_vpc.this[0]: Creating... 5aws_vpc.this[0]: Still creating... [10s elapsed] 6... 7aws_route_table_association.private[1]: Creation complete after 0s [id=rtbassoc-01b7347a7e9960a13] 8aws_nat_gateway.this[0]: Still creating... [10s elapsed] La r√©conciliation √©ffectu√©e, la ressource passe √† l'√©tat READY: True\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m ü§ñ Application automatique Nous pouvons aussi activer la r√©conciliation automatique. Pour ce faire il faut d√©clarer le param√®tre .spec.autoApprove √† true.\nToutes les ressources IRSA sont configur√©es de la sorte:\nexternal-secrets.yaml\n1piVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: irsa-external-secrets 5spec: 6 approvePlan: auto 7 destroyResourcesOnDeletion: true 8 interval: 8m 9 path: ./modules/iam-role-for-service-accounts-eks 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-iam 13 namespace: flux-system 14 vars: 15 - name: role_name 16 value: ${cluster_name}-external-secrets 17 - name: attach_external_secrets_policy 18 value: true 19 - name: oidc_providers 20 value: 21 main: 22 provider_arn: ${oidc_provider_arn} 23 namespace_service_accounts: [\u0026#34;security:external-secrets\u0026#34;] Donc si je fais le moindre changement sur la console AWS par exemple, celui-ci sera rapidement √©cras√© par celui g√©r√© par tf-controller.\nInfo La politique de suppression d'une ressource Terraform est d√©finie par le param√®tre destroyResourcesOnDeletion. Par d√©faut elles sont conserv√©es et il faut donc que ce param√®tre ait pour valeur true afin de d√©truire les √©l√©ments cr√©es lorsque l'objet Kubernetes est supprim√©.\nIci nous voulons la possibilit√© de supprimer les r√¥les IRSA. Ils sont en effet √©troitement li√©s aux clusters.\nüîÑ Entr√©es et sorties: d√©pendances entre modules Lorsque qu'on utilise Terraform, on a souvent besoin de passer des donn√©es d'un module √† l'autre. G√©n√©ralement ce sont les outputs du module qui exportent ces informations. Il faut donc un moyen de les importer dans un autre module.\nReprenons encore l'exemple donn√© ci-dessus (vpc-dev). Nous notons en bas du YAML la directive suivante:\n1... 2 writeOutputsToSecret: 3 name: vpc-dev Lorsque cette ressource est appliqu√©e nous aurons un message qui confirme que les outputs sont disponibles (\u0026quot;Outputs written\u0026quot;):\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m En effet ce module exporte de nombreuses informations (126):\n1kubectl get secrets -n flux-system vpc-dev 2NAME TYPE DATA AGE 3vpc-dev Opaque 126 15s 4 5kubectl get secret -n flux-system vpc-dev --template=\u0026#39;{{.data.vpc_id}}\u0026#39; | base64 -d 6vpc-0c06a6d153b8cc4db Certains de ces √©l√©ments d'informations sont ensuite utilis√©s pour cr√©er un cluster EKS de dev:\nvpc/dev.yaml\n1... 2 varsFrom: 3 - kind: Secret 4 name: vpc-dev 5 varsKeys: 6 - vpc_id 7 - private_subnets 8... üíæ Sauvegarder et restaurer un tfstate Dans mon cas je ne souhaite pas recr√©er la zone et le certificat √† chaque destruction du controlplane. Voici un exemple des √©tapes √† mener pour que je puisse restaurer l'√©tat de ces ressources lorsque j'utilise cette demo.\nNote Il s'agit l√† d'une proc√©dure manuelle afin de d√©montrer le comportement de tf-controller par rapport aux fichiers d'√©tat. Par d√©faut ces tfstates sont stock√©s dans des secrets mais on pr√©ferera configurer un backend GCS ou S3\nLa cr√©ation initiale de l'environnement de d√©mo m'a permis de sauvegarder les fichiers d'√©tat (tfstate) de cette fa√ßon.\n1WORKSPACE=\u0026#34;default\u0026#34; 2STACK=\u0026#34;route53-cloud-hostedzone\u0026#34; 3BACKUPDIR=\u0026#34;${HOME}/tf-controller-backup\u0026#34; 4 5mkdir -p ${BACKUPDIR} 6 7kubectl get secrets -n flux-system tfstate-${WORKSPACE}-${STACK} -o jsonpath=\u0026#39;{.data.tfstate}\u0026#39; | \\ 8base64 -d | gzip -d \u0026gt; ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate Lorsque le cluster est cr√©√© √† nouveau, tf-controller essaye de cr√©er la zone car le fichier d'√©tat est vide.\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system route53-cloud-hostedzone Unknown Plan generated: set approvePlan: \u0026#34;plan-main@sha1:345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. true 16 minutes 5 6tfctl show plan route53-cloud-hostedzone 7 8Terraform used the selected providers to generate the following execution 9plan. resource actions are indicated with the following symbols: 10 + create 11 12Terraform will perform the following actions: 13 14 # aws_route53_zone.this will be created 15 + resource \u0026#34;aws_route53_zone\u0026#34; \u0026#34;this\u0026#34; { 16 + arn = (known after apply) 17 + comment = \u0026#34;Experimentations for blog.ogenki.io\u0026#34; 18 + force_destroy = false 19 + id = (known after apply) 20 + name = \u0026#34;cloud.ogenki.io\u0026#34; 21 + name_servers = (known after apply) 22 + primary_name_server = (known after apply) 23 + tags = { 24 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 25 } 26 + tags_all = { 27 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 28 } 29 + zone_id = (known after apply) 30 } 31 32Plan: 1 to add, 0 to change, 0 to destroy. 33 34Changes to Outputs: 35 + domain_name = \u0026#34;cloud.ogenki.io\u0026#34; 36 + nameservers = (known after apply) 37 + zone_arn = (known after apply) 38 + zone_id = (known after apply) 39 40Plan generated: set approvePlan: \u0026#34;plan-main@345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. 41To set the field, you can also run: 42 43 tfctl approve route53-cloud-hostedzone -f filename.yaml La proc√©dure de restauration consiste donc √† cr√©er le secret √† nouveau:\n1gzip ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate 2 3cat \u0026lt;\u0026lt;EOF | kubectl apply -f - 4apiVersion: v1 5kind: Secret 6metadata: 7 name: tfstate-${WORKSPACE}-${STACK} 8 namespace: flux-system 9 annotations: 10 encoding: gzip 11type: Opaque 12data: 13 tfstate: $(cat ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate.gz | base64 -w 0) 14EOF Il faudra aussi relancer un plan de fa√ßon explicite pour mettre √† jour l'√©tat de la ressource en question\n1tfctl replan route53-cloud-hostedzone 2Ôò´ Replan requested for flux-system/route53-cloud-hostedzone 3Error: timed out waiting for the condition Nous pouvons alors v√©rifier que le fichier d'√©tat a bien √©t√© mis √† jour\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3flux-system route53-cloud-hostedzone True Outputs written: main@sha1:d0934f979d832feb870a8741ec01a927e9ee6644 false 19 minutes üîç Focus sur certaines fonctionnalit√©s de Flux Oui j'ai un peu menti sur l'agenda üòù. Il me semblait n√©cessaire de mettre en lumi√®re 2 fonctionnalit√©s que je n'avais pas exploit√© jusque l√† et qui sont fort utiles!\nSubstition de variables Lorsque Flux est initilias√© un certain nombre de Kustomization sp√©cifique √† ce cluster sont cr√©√©s. Il est possible d'y indiquer des variables de substitution qui pourront √™tre utilis√©es dans l'ensemble des ressources d√©ploy√©es par cette Kustomization. Cela permet d'√©viter un maximum la d√©duplication de code.\nJ'ai d√©couvert l'efficacit√© de cette fonctionnalit√© tr√®s r√©cemment. Je vais d√©crire ici la fa√ßon dont je l'utilise:\nLe code terraform qui cr√©e un cluster EKS, g√©n√®re aussi une ConfigMap qui contient les variables propres au cluster. On y retrouvera, bien s√ªr, le nom du cluster, mais aussi tous les param√®tres qui varient entre les clusters et qui sont utilis√©s dans les manifests Kubernetes.\nflux.tf\n1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;flux_clusters_vars\u0026#34; { 2 metadata { 3 name = \u0026#34;eks-${var.cluster_name}-vars\u0026#34; 4 namespace = \u0026#34;flux-system\u0026#34; 5 } 6 7 data = { 8 cluster_name = var.cluster_name 9 oidc_provider_arn = module.eks.oidc_provider_arn 10 aws_account_id = data.aws_caller_identity.this.account_id 11 region = var.region 12 environment = var.env 13 vpc_id = module.vpc.vpc_id 14 } 15 depends_on = [flux_bootstrap_git.this] 16} Comme sp√©cifi√© pr√©cedemment, les variables de substition sont d√©finies dans les Kustomization. Prenons un exemple concret. Ci-dessous on d√©finie la Kustomization qui d√©ploie toutes les ressources qui sont consomm√©es par tf-controller On d√©clare ici la ConfigMap eks-controlplane-0-vars qui avait √©t√© g√©n√©r√© √† la cr√©ation du cluster EKS.\ninfrastructure.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1 2kind: Kustomization 3metadata: 4 name: tf-custom-resources 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 path: ./infrastructure/controlplane-0/opentofu/custom-resources 10 postBuild: 11 substitute: 12 domain_name: \u0026#34;cloud.ogenki.io\u0026#34; 13 substituteFrom: 14 - kind: ConfigMap 15 name: eks-controlplane-0-vars 16 - kind: Secret 17 name: eks-controlplane-0-vars 18 optional: true 19 sourceRef: 20 kind: GitRepository 21 name: flux-system 22 dependsOn: 23 - name: tf-controller Enfin voici un exemple de ressource Kubernetes qui en fait usage. Cet unique manifest peut √™tre utilis√© par tous les clusters!.\nexternal-dns/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: external-dns 5spec: 6... 7 values: 8 global: 9 imageRegistry: public.ecr.aws 10 fullnameOverride: external-dns 11 aws: 12 region: ${region} 13 zoneType: \u0026#34;public\u0026#34; 14 batchChangeSize: 1000 15 domainFilters: [\u0026#34;${domain_name}\u0026#34;] 16 logFormat: json 17 txtOwnerId: \u0026#34;${cluster_name}\u0026#34; 18 serviceAccount: 19 annotations: 20 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/${cluster_name}-external-dns\u0026#34; Cela √©limine totalement les overlays qui consistaient √† ajouter les param√®tres sp√©cifiques au cluster.\nWeb UI (Weave GitOps) Dans mon pr√©c√©dent article sur Flux, je mentionnais le fait que l'un des inconv√©nients (si l'on compare avec son principale concurrent: ArgoCD) est le manque d'une interface Web. Bien que je sois un adepte de la ligne de commande, c'est parfois bien utile d'avoir une vue synth√©tique et de pouvoir effectuer certaines op√©ration en quelques clicks \u0026#x1f5b1;\u0026#xfe0f;\nC'est d√©sormais possible avec Weave Gitops! Bien entendu ce n'est pas comparable avec l'UI d'ArgoCD, mais l'essentiel est l√†: Mettre en pause la r√©concilation, visualiser les manifests, les d√©pendances, les √©v√©nements...\nIl existe aussi le plugin VSCode comme alternative.\nüí≠ Remarques Et voil√†, nous arrivons au bout de notre exploration de cet autre outil de gestion d'infrastructure sur Kubernetes. Malgr√© quelques petits soucis rencontr√©s en cours de route, que j'ai partag√© sur le repo Git du projet, l'exp√©rience m'a beaucoup plu. tf-controller offre une r√©ponse concr√®te √† une question fr√©quente : comment g√©rer notre infra comme on g√®re notre code ?\nJ'aime beaucoup l'approche GitOps appliqu√©e √† l'infrastructure, j'avais d'ailleurs √©crit un article sur Crossplane. tf-controller aborde la probl√©matique sous un angle diff√©rent: utiliser du Terraform directement. Cela signifie qu'on peut utiliser nos connaissances actuelles et notre code existant. Pas besoin d'apprendre une nouvelle fa√ßon de d√©clarer nos ressources. C'est un crit√®re √† prendre en compte car migrer vers un nouvel outil lorsque l'on a un existant repr√©sente un √©ffort non n√©gligeable. Cependant j'ajouterais aussi que tf-controller s'adresse aux utilisateurs de Flux uniquement et, de ce fait, restreint le publique cible.\nAujourd'hui, j'utilise une combinaison de Terraform, Terragrunt et RunAtlantis. tf-controller pourrait devenir une alternative viable: Nous avons en effet √©voqu√© l'int√©r√™t de Kustomize associ√© aux substitions de variables pour la factorisation de code. Dans la roadmap du projet il y a aussi l'objectif d'afficher les plans dans les pull-requests. Autre probl√©matique fr√©quente: la n√©cessit√© de passer des √©l√©ments sensibles aux modules. En utilisant une ressource Terraform, on peut injecter des variables depuis des secrets Kubernetes. Ce qui permet d'utiliser certains outils, tels que external-secrets, sealed-secrets ...\nJe vous encourage donc √† essayer tf-controller vous-m√™me, et peut-√™tre m√™me d'y apporter votre contribution üôÇ.\nNote La d√©mo que j'ai faite ici utilise pas mal de ressources, dont certaines assez cruciales (comme le r√©seau). Donc, gardez en t√™te que c'est juste pour la d√©mo ! Je sugg√®re une approche progressive si vous envisagez de le mettre en ouvre: commencez par utiliser la d√©tection de d√©rives, puis cr√©ez des ressources simples. J'ai aussi pris quelques raccourcis en terme de s√©curit√© √† √©viter absolument, notamment le fait de donner les droits admin au contr√¥leur. ","link":"https://blog.ogenki.io/fr/post/terraform-controller/","section":"post","tags":["infrastructure"],"title":"Appliquer les principes de GitOps √† l'infrastructure: Introduction √† `tf-controller`"},{"body":"Kubernetes est d√©sormais la plate-forme privil√©gi√©e pour orchestrer les applications \u0026quot;sans √©tat\u0026quot; aussi appel√© \u0026quot;stateless\u0026quot;. Les conteneurs qui ne stockent pas de donn√©es peuvent √™tre d√©truits et recr√©√©s ailleurs sans impact. En revanche, la gestion d'applications \u0026quot;stateful\u0026quot; dans un environnement dynamique tel que Kubernetes peut √™tre un v√©ritable d√©fi. Malgr√© le fait qu'il existe un nombre croissant de solutions de base de donn√©es \u0026quot;Cloud Native\u0026quot; (comme CockroachDB, TiDB, K8ssandra, Strimzi ...) et il y a de nombreux √©l√©ments √† consid√©rer lors de leur √©valuation:\nQuelle est la maturit√© de l'op√©rateur? (Dynamisme et contributeurs, gouvernance du projet) Quels sont les resources personalis√©es disponibles (\u0026quot;custom resources\u0026quot;), quelles op√©rations permettent t-elles de r√©aliser? Quels sont les type de stockage disponibles: HDD / SSD, stockage local / distant? Que se passe-t-il lorsque quelque chose se passe mal: Quelle est le niveau de r√©silience de la solution? Sauvegarde et restauration: est-il facile d'effectuer et de planifier des sauvegardes? Quelles options de r√©plication et de mise √† l'√©chelle sont disponibles? Qu'en est-il des limites de connexion et de concurrence, les pools de connexion? A propos de la supervision, quelles sont les m√©triques expos√©es et comment les exploiter? J'√©tais √† la recherche d'une solution permettant de g√©rer un serveur PostgreSQL. La base de donn√©es qui y serait h√©berg√©e est n√©cessaire pour un logiciel de r√©servation de billets nomm√© Alf.io. Nous sommes en effet en train d'organiser les Kubernetes Community Days France vous √™tes tous convi√©s! üëê.\nJe cherchais sp√©cifiquement une solution ind√©pendante d'un clouder (cloud agnostic) et l'un des principaux crit√®res √©tait la simplicit√© d'utilisation. Je connaissais d√©j√† plusieurs op√©rateurs Kubernetes, et j'ai fini par √©valuer une solution relativement r√©cente: CloudNativePG.\nCloudNativepg est l'op√©rateur de Kubernetes qui couvre le cycle de vie complet d'un cluster de base de donn√©es PostgreSQL hautement disponible avec une architecture de r√©plication native en streaming.\nCe projet √©t√© cr√©√© par l'entreprise EnterpriseDB et a √©t√© soumis √† la CNCF afin de rejoindre les projets Sandbox.\nüéØ Notre objectif Je vais donner ici une introduction aux principales fonctionnalit√©s de CloudNativePG.\nL'objectif est de:\nCr√©er une base de donn√©es PostgreSQL sur un cluster GKE, Ajouter une instance secondaire (r√©plication) Ex√©cuter quelques tests de r√©silience. Nous verrons √©galement comment tout cela se comporte en terme de performances et quels sont les outils de supervision disponibles. Enfin, nous allons jeter un ≈ìil aux m√©thodes de sauvegarde/restauration.\nInfo Dans cet article, nous allons tout cr√©er et tout mettre √† jour manuellement. Mais dans un environnement de production, il est conseill√© d'utiliser un moteur GitOps, par exemple Flux (sujet couvert dans un article pr√©c√©dent).\nSi vous souhaitez voir un exemple complet, vous pouvez consulter le d√©p√¥t git KCD France infrastructure.\nToutes les resources de cet article sont dans ce d√©p√¥t.\n\u0026#x2611;\u0026#xfe0f; Pr√©requis \u0026#x1f4e5; Outils gcloud SDK: Nous allons d√©ployer sur Google Cloud (en particulier sur GKE) et, pour ce faire, nous devrons cr√©er quelques ressources dans notre projet GCP. Nous aurons donc besoin du SDK et de la CLI Google Cloud. Il est donc n√©cessaire de l'installer en suivant cette documentation.\nkubectl plugin: Pour faciliter la gestion des clusters, il existe un plugin kubectl qui donne des informations synth√©tiques sur l'instance PostgreSQL et permet aussi d'effectuer certaines op√©rations. Ce plugin peut √™tre install√© en utilisant krew:\n1kubectl krew install cnpg ‚òÅÔ∏è Cr√©er les resources Google Cloud Avant de cr√©er notre instance PostgreSQL, nous devons configurer certaines choses:\nNous avons besoin d'un cluster Kubernetes. (Cet article suppose que vous avez d√©j√† pris soin de provisionner un cluster GKE) Nous allons cr√©er un bucket (Google Cloud Storage) pour stocker les sauvegardes et Fichiers WAL. Nous configurerons les permissions pour nos pods afin qu'ils puissent √©crire dans ce bucket. Cr√©er le bucket √† l'aide de CLI gcloud\n1gcloud storage buckets create --location=eu --default-storage-class=coldline gs://cnpg-ogenki 2Creating gs://cnpg-ogenki/... 3 4gcloud storage buckets describe gs://cnpg-ogenki 5[...] 6name: cnpg-ogenki 7owner: 8 entity: project-owners-xxxx0008 9projectNumber: \u0026#39;xxx00008\u0026#39; 10rpo: DEFAULT 11selfLink: https://www.googleapis.com/storage/v1/b/cnpg-ogenki 12storageClass: STANDARD 13timeCreated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; 14updated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; Nous allons maintenant configurer les permissions afin que les pods (PostgreSQL Server) puissent permettant √©crire/lire √† partir du bucket gr√¢ce √† Workload Identity.\nNote Workload Identity doit √™tre activ√© au niveau du cluster GKE. Afin de v√©rifier que le cluster est bien configur√©, vous pouvez lancer la commande suivante:\n1gcloud container clusters describe \u0026lt;cluster_name\u0026gt; --format json --zone \u0026lt;zone\u0026gt; | jq .workloadIdentityConfig 2{ 3 \u0026#34;workloadPool\u0026#34;: \u0026#34;{{ gcp_project }}.svc.id.goog\u0026#34; 4} Cr√©er un compte de service Google Cloud\n1gcloud iam service-accounts create cloudnative-pg --project={{ gcp_project }} 2Created service account [cloudnative-pg]. Attribuer au compte de service la permission storage.admin\n1gcloud projects add-iam-policy-binding {{ gcp_project }} \\ 2--member \u0026#34;serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com\u0026#34; \\ 3--role \u0026#34;roles/storage.admin\u0026#34; 4[...] 5- members: 6 - serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 7 role: roles/storage.admin 8etag: BwXrGA_VRd4= 9version: 1 Autoriser le compte de service (Attention il s'agit l√† du compte de service au niveau Kubernetes) afin d'usurper le compte de service IAM. \u0026#x2139;\u0026#xfe0f; Assurez-vous d'utiliser le format appropri√© serviceAccount:{{ gcp_project }}.svc.id.goog[{{ kubernetes_namespace }}/{{ kubernetes_serviceaccount }}]\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 7 role: roles/iam.workloadIdentityUser 8etag: BwXrGBjt5kQ= 9version: 1 Nous sommes pr√™ts √† cr√©er les ressources Kubernetes \u0026#x1f4aa;\n\u0026#x1f511; Cr√©er les secrets pour les utilisateurs PostgreSQL Nous devons cr√©er les param√®tres d'authentification des utilisateurs qui seront cr√©√©s pendant la phase de \u0026quot;bootstrap\u0026quot; (nous y reviendrons par la suite): le superutilisateur et le propri√©taire de base de donn√©es nouvellement cr√©√©.\n1kubectl create secret generic cnpg-mydb-superuser --from-literal=username=postgres --from-literal=password=foobar --namespace demo 2secret/cnpg-mydb-superuser created 1kubectl create secret generic cnpg-mydb-user --from-literal=username=smana --from-literal=password=barbaz --namespace demo 2secret/cnpg-mydb-user created \u0026#x1f6e0;\u0026#xfe0f; D√©ployer l'op√©rateur CloudNativePG avec Helm Ici nous utiliserons le chart Helm pour d√©ployer CloudNativePG:\n1helm repo add cnpg https://cloudnative-pg.github.io/charts 2 3helm upgrade --install cnpg --namespace cnpg-system \\ 4--create-namespace charts/cloudnative-pg 5 6kubectl get po -n cnpg-system 7NAME READY STATUS RESTARTS AGE 8cnpg-74488f5849-8lhjr 1/1 Running 0 6h17m Cela installe aussi quelques resources personnalis√©es (Custom Resources Definitions)\n1kubectl get crds | grep cnpg.io 2backups.postgresql.cnpg.io 2022-10-08T16:15:14Z 3clusters.postgresql.cnpg.io 2022-10-08T16:15:14Z 4poolers.postgresql.cnpg.io 2022-10-08T16:15:14Z 5scheduledbackups.postgresql.cnpg.io 2022-10-08T16:15:14Z Pour une liste compl√®te des param√®tres possibles, veuillez vous r√©f√©rer √† la doc de l'API.\n\u0026#x1f680; Cr√©er un serveur PostgreSQL Nous pouvons d√©sormais cr√©er notre premi√®re instance en utilisant une resource personnalis√©e Cluster. La d√©finition suivante est assez simple: Nous souhaitons d√©marrer un serveur PostgreSQL, cr√©er automatiquement une base de donn√©es nomm√©e mydb et configurer les informations d'authentification en utilisant les secrets cr√©√©s pr√©c√©demment.\n1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki 5 namespace: demo 6spec: 7 description: \u0026#34;PostgreSQL Demo Ogenki\u0026#34; 8 imageName: ghcr.io/cloudnative-pg/postgresql:14.5 9 instances: 1 10 11 bootstrap: 12 initdb: 13 database: mydb 14 owner: smana 15 secret: 16 name: cnpg-mydb-user 17 18 serviceAccountTemplate: 19 metadata: 20 annotations: 21 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 22 23 superuserSecret: 24 name: cnpg-mydb-superuser 25 26 storage: 27 storageClass: standard 28 size: 10Gi 29 30 backup: 31 barmanObjectStore: 32 destinationPath: \u0026#34;gs://cnpg-ogenki\u0026#34; 33 googleCredentials: 34 gkeEnvironment: true 35 retentionPolicy: \u0026#34;30d\u0026#34; 36 37 resources: 38 requests: 39 memory: \u0026#34;1Gi\u0026#34; 40 cpu: \u0026#34;500m\u0026#34; 41 limits: 42 memory: \u0026#34;1Gi\u0026#34; Cr√©er le namespace o√π notre instance postgresql sera d√©ploy√©e\n1kubectl create ns demo 2namespace/demo created Adapdez le fichier YAML ci-dessus vos besoins et appliquez comme suit:\n1kubectl apply -f cluster.yaml 2cluster.postgresql.cnpg.io/ogenki created Vous remarquerez que le cluster sera en phase Initializing. Nous allons utiliser le plugin CNPG pour la premi√®re fois afin de v√©rifier son √©tat. Cet outil deviendra par la suite notre meilleur ami pour afficher une vue synth√©tique de l'√©tat du cluster.\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Primary server is initializing 4Name: ogenki 5Namespace: demo 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: (switching to ogenki-1) 8Status: Setting up primary Creating primary instance ogenki-1 9Instances: 1 10Ready instances: 0 11 12Certificates Status 13Certificate Name Expiration Date Days Left Until Expiration 14---------------- --------------- -------------------------- 15ogenki-ca 2023-01-13 20:02:40 +0000 UTC 90.00 16ogenki-replication 2023-01-13 20:02:40 +0000 UTC 90.00 17ogenki-server 2023-01-13 20:02:40 +0000 UTC 90.00 18 19Continuous Backup status 20First Point of Recoverability: Not Available 21No Primary instance found 22Streaming Replication status 23Not configured 24 25Instances status 26Name Database Size Current LSN Replication role Status QoS Manager Version Node 27---- ------------- ----------- ---------------- ------ --- --------------- ---- imm√©diatement apr√®s la d√©claration de notre nouveau Cluster, une action de bootstrap est lanc√©e. Dans notre exemple, nous cr√©ons une toute nouvelle base de donn√©es nomm√©e mydb avec un propri√©taire smana dont les informations d'authentification viennent du secret cr√©√© pr√©c√©demment.\n1[...] 2 bootstrap: 3 initdb: 4 database: mydb 5 owner: smana 6 secret: 7 name: cnpg-mydb-user 8[...] 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 0/1 Running 0 55s 4ogenki-1-initdb-q75cz 0/1 Completed 0 2m32s Apr√®s quelques secondes, le cluster change de statut et devient Ready (configur√© et pr√™t √† l'usage) \u0026#x1f44f;\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7154833472216277012 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 1 10Ready instances: 1 11 12[...] 13 14Instances status 15Name Database Size Current LSN Replication role Status QoS Manager Version Node 16---- ------------- ----------- ---------------- ------ --- --------------- ---- 17ogenki-1 33 MB 0/17079F8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xczh Info Il existe de nombreuses fa√ßons de bootstrap un cluster. Par exemple, la restauration d'une sauvegarde dans une toute nouvelle instance ou en ex√©cutant du code SQL ... Plus d'infos ici.\nü©π Instance de secours et r√©silience Info Dans les architectures postgresql traditionnelles, nous trouvons g√©n√©ralement un composant suppl√©mentaire pour g√©rer la haute disponibilit√© (ex: Patroni). Un particularit√© de l'op√©rateur CloudNativePG est qu'il b√©n√©ficie des fonctionnalit√©s de base de Kubernetes et s'appuie sur un composant nomm√© Postgres instance manager.\nAjoutez une instance de secours (\u0026quot;standby\u0026quot;) en d√©finissant le nombre de r√©pliques sur 2.\n1kubectl edit cluster -n demo ogenki 2cluster.postgresql.cnpg.io/ogenki edited 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3[...] 4spec: 5 instances: 2 6[...] L'op√©rateur remarque imm√©diatement le changement, ajoute une instance de secours et d√©marre le processus de r√©plication.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Creating a new replica Creating replica ogenki-2-join 9Instances: 2 10Ready instances: 1 11Current Write LSN: 0/1707A30 (Timeline: 1 - WAL File: 000000010000000000000001) 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 0 3m16s 4ogenki-2-join-xxrwx 0/1 Pending 0 82s Apr√®s un certain temps (qui d√©pend de la quantit√© de donn√©es √† r√©pliquer), l'instance de secours devient op√©rationnelle et nous pouvons voir les statistiques de r√©plication.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 2 10Ready instances: 2 11Current Write LSN: 0/3000060 (Timeline: 1 - WAL File: 000000010000000000000003) 12 13[...] 14 15Streaming Replication status 16Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 17---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 18ogenki-2 0/3000060 0/3000060 0/3000060 0/3000060 00:00:00 00:00:00 00:00:00 streaming async 0 19 20Instances status 21Name Database Size Current LSN Replication role Status QoS Manager Version Node 22---- ------------- ----------- ---------------- ------ --- --------------- ---- 23ogenki-1 33 MB 0/3000060 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 24ogenki-2 33 MB 0/3000060 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc Nous allons d√©sormais √©ffectuer ce que l'on appelle un \u0026quot;Switchover\u0026quot;: Nous allons promouvoir l'instance de secours en instance primaire.\nLe plugin cnpg permet de le faire de fa√ßon imp√©rative, en utilisant la ligne de commande suivante:\n1kubectl cnpg promote ogenki ogenki-2 -n demo 2Node ogenki-2 in cluster ogenki will be promoted Dans mon cas, le basculement √©tait vraiment rapide. Nous pouvons v√©rifier que l'instance ogenki-2 est devenu primaire et que la r√©plication est effectu√©e dans l'autre sens.\n1kubectl cnpg status -n demo ogenki 2[...] 3Status: Switchover in progress Switching over to ogenki-2 4Instances: 2 5Ready instances: 1 6[...] 7Streaming Replication status 8Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 9---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 10ogenki-1 0/4004CA0 0/4004CA0 0/4004CA0 0/4004CA0 00:00:00 00:00:00 00:00:00 streaming async 0 11 12Instances status 13Name Database Size Current LSN Replication role Status QoS Manager Version Node 14---- ------------- ----------- ---------------- ------ --- --------------- ---- 15ogenki-2 33 MB 0/4004CA0 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc 16ogenki-1 33 MB 0/4004CA0 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 Maintenant, provoquons un Failover en supprimant le pod principal\n1kubectl delete po -n demo --grace-period 0 --force ogenki-2 2Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. 3pod \u0026#34;ogenki-2\u0026#34; force deleted 1Cluster Summary 2Name: ogenki 3Namespace: demo 4System ID: 7155095145869606932 5PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 6Primary instance: ogenki-1 7Status: Failing over Failing over from ogenki-2 to ogenki-1 8Instances: 2 9Ready instances: 1 10Current Write LSN: 0/4005D98 (Timeline: 3 - WAL File: 000000030000000000000004) 11 12[...] 13Instances status 14Name Database Size Current LSN Replication role Status QoS Manager Version Node 15---- ------------- ----------- ---------------- ------ --- --------------- ---- 16ogenki-1 33 MB 0/40078D8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 17ogenki-2 - - - pod not available Burstable - gke-kcdfrance-main-np-0e87115b-xszc Quelques secondes plus tard le cluster devient op√©rationnel √† nouveau.\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 13m 2 2 Cluster in healthy state ogenki-1 Jusqu'ici tout va bien, nous avons pu faire quelques tests de la haute disponibilit√© et c'√©tait assez probant üòé.\nüëÅÔ∏è Supervision Nous allons utiliser la Stack Prometheus. Nous ne couvrirons pas son installation dans cet article. Si vous voulez voir comment l'installer avec Flux, vous pouvez jeter un oeil √† cet exemple.\nPour r√©cup√©rer les m√©triques de notre instance, nous devons cr√©er un PodMonitor.\n1apiVersion: monitoring.coreos.com/v1 2kind: PodMonitor 3metadata: 4 labels: 5 prometheus-instance: main 6 name: cnpg-ogenki 7 namespace: demo 8spec: 9 namespaceSelector: 10 matchNames: 11 - demo 12 podMetricsEndpoints: 13 - port: metrics 14 selector: 15 matchLabels: 16 postgresql: ogenki Nous pouvons ensuite ajouter le tableau de bord Grafana disponible ici.\nEnfin, vous souhaiterez peut-√™tre configurer des alertes et vous pouvez cr√©er un PrometheusRule en utilisant ces r√®gles.\n\u0026#x1f525; Performances and benchmark Info Mise √† jour: Il est d√©sormais possible de faire un test de performance avec le plugin cnpg\nAfin de connaitre les limites de votre serveur, vous devriez faire un test de performances et de conserver une base de r√©f√©rence pour de futures am√©liorations.\nNote Au sujet des performances, il existe de nombreux domaines d'am√©lioration sur lesquels nous pouvons travailler.Cela d√©pend principalement de l'objectif que nous voulons atteindre. En effet, nous ne voulons pas perdre du temps et de l'argent pour les performances dont nous n'aurons probablement jamais besoin.\nVoici les principaux √©l√©ments √† analyser:\nTuning de la configuration PostgreSQL Resources syst√®mes (cpu et m√©moire) Types de Disque : IOPS, stockage locale (local-volume-provisioner), Disques d√©di√©es pour les WAL et les donn√©es PG_DATA \u0026quot;Pooling\u0026quot; de connexions PGBouncer. CloudNativePG fourni une resource personnalis√©e Pooler qui permet de configurer cela facilement. Optimisation de la base de donn√©es, analyser les plans d'ex√©cution gr√¢ce √† explain, utiliser l'extension pg_stat_statement ... Tout d'abord, nous ajouterons des \u0026quot;labels\u0026quot; aux n≈ìuds afin d'ex√©cuter la commande pgbench sur diff√©rentes machines de celles h√©bergeant la base de donn√©es.\n1PG_NODE=$(kubectl get po -n demo -l postgresql=ogenki,role=primary -o jsonpath={.items[0].spec.nodeName}) 2kubectl label node ${PG_NODE} workload=postgresql 3node/gke-kcdfrance-main-np-0e87115b-vlzm labeled 4 5 6# Choose any other node different than the ${PG_NODE} 7kubectl label node gke-kcdfrance-main-np-0e87115b-p5d7 workload=pgbench 8node/gke-kcdfrance-main-np-0e87115b-p5d7 labeled Et nous d√©ploierons le chart Helm comme suit\n1git clone git@github.com:EnterpriseDB/cnp-bench.git 2cd cnp-bench 3 4cat \u0026gt; pgbench-benchmark/myvalues.yaml \u0026lt;\u0026lt;EOF 5cnp: 6 existingCluster: true 7 existingHost: ogenki-rw 8 existingCredentials: cnpg-mydb-superuser 9 existingDatabase: mydb 10 11pgbench: 12 # Node where to run pgbench 13 nodeSelector: 14 workload: pgbench 15 initialize: true 16 scaleFactor: 1 17 time: 600 18 clients: 10 19 jobs: 1 20 skipVacuum: false 21 reportLatencies: false 22EOF 23 24helm upgrade --install -n demo pgbench -f pgbench-benchmark/myvalues.yaml pgbench-benchmark/ Info Il existe diff√©rents services selon que vous souhaitez lire et √©crire ou de la lecture seule.\n1kubectl get ep -n demo 2NAME ENDPOINTS AGE 3ogenki-any 10.64.1.136:5432,10.64.1.3:5432 15d 4ogenki-r 10.64.1.136:5432,10.64.1.3:5432 15d 5ogenki-ro 10.64.1.136:5432 15d 6ogenki-rw 10.64.1.3:5432 15d 1kubectl logs -n demo job/pgbench-pgbench-benchmark -f 2Defaulted container \u0026#34;pgbench\u0026#34; out of: pgbench, wait-for-cnp (init), pgbench-init (init) 3pgbench (14.1, server 14.5 (Debian 14.5-2.pgdg110+2)) 4starting vacuum...end. 5transaction type: \u0026lt;builtin: TPC-B (sort of)\u0026gt; 6scaling factor: 1 7query mode: simple 8number of clients: 10 9number of threads: 1 10duration: 600 s 11number of transactions actually processed: 545187 12latency average = 11.004 ms 13initial connection time = 111.585 ms 14tps = 908.782896 (without initial connection time) üíΩ Sauvegarde and restaurations Note Le fait de pouvoir stocker des sauvegarde et fichiers WAL dans le bucket GCP est possible car nous avons attribu√© les autorisations en utilisant une annotation pr√©sente dans le ServiceAccount utilis√© par le cluster\n1serviceAccountTemplate: 2 metadata: 3 annotations: 4 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com Nous pouvons d'abord d√©clencher une sauvegarde on demand √† l'aide de la ressource personnalis√©e Backup\n1apiVersion: postgresql.cnpg.io/v1 2kind: Backup 3metadata: 4 name: ogenki-now 5 namespace: demo 6spec: 7 cluster: 8 name: ogenki 1kubectl apply -f backup.yaml 2backup.postgresql.cnpg.io/ogenki-now created 3 4kubectl get backup -n demo 5NAME AGE CLUSTER PHASE ERROR 6ogenki-now 36s ogenki completed Si vous jetez un ≈ìil au contenu du bucket GCS, vous verrez un nouveau r√©pertoire qui stocke les sauvegardes de base (\u0026quot;base backups\u0026quot;).\n1gcloud storage ls gs://cnpg-ogenki/ogenki/base 2gs://cnpg-ogenki/ogenki/base/20221023T130327/ Mais la plupart du temps, nous pr√©fererons configurer une sauvegarde planifi√©e (\u0026quot;scheduled\u0026quot;). Ci-dessous un exemple pour une sauvegarde quotidienne:\n1apiVersion: postgresql.cnpg.io/v1 2kind: ScheduledBackup 3metadata: 4 name: ogenki-daily 5 namespace: demo 6spec: 7 backupOwnerReference: self 8 cluster: 9 name: ogenki 10 schedule: 0 0 0 * * * Les restaurations ne peuvent √™tre effectu√©es que sur de nouvelles instances. Ici, nous utiliserons la sauvegarde que nous avions cr√©√©e pr√©c√©demment afin d'initialiser une toute nouvelle instance.\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore] 7 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 8 role: roles/iam.workloadIdentityUser 9etag: BwXrs755FPA= 10version: 1 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki-restore 5 namespace: demo 6spec: 7 instances: 1 8 9 serviceAccountTemplate: 10 metadata: 11 annotations: 12 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 13 14 storage: 15 storageClass: standard 16 size: 10Gi 17 18 resources: 19 requests: 20 memory: \u0026#34;1Gi\u0026#34; 21 cpu: \u0026#34;500m\u0026#34; 22 limits: 23 memory: \u0026#34;1Gi\u0026#34; 24 25 superuserSecret: 26 name: cnpg-mydb-superuser 27 28 bootstrap: 29 recovery: 30 backup: 31 name: ogenki-now Nous notons qu'un pod se charge imm√©diatement de la restauration compl√®te (\u0026quot;full recovery\u0026quot;).\n1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 1 (18h ago) 18h 4ogenki-2 1/1 Running 0 18h 5ogenki-restore-1 0/1 Init:0/1 0 0s 6ogenki-restore-1-full-recovery-5p4ct 0/1 Completed 0 51s Le nouveau cluster devient alors op√©rationnel (\u0026quot;Ready\u0026quot;).\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 18h 2 2 Cluster in healthy state ogenki-1 4ogenki-restore 80s 1 1 Cluster in healthy state ogenki-restore-1 \u0026#x1f9f9; Nettoyage Suppression du cluster\n1kubectl delete cluster -n demo ogenki ogenki-restore 2cluster.postgresql.cnpg.io \u0026#34;ogenki\u0026#34; deleted 3cluster.postgresql.cnpg.io \u0026#34;ogenki-restore\u0026#34; deleted Supprimer le service IAM\n1gcloud iam service-accounts delete cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 2You are about to delete service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 3 4Do you want to continue (Y/n)? y 5 6deleted service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com] üí≠ Conclusion Je viens tout juste de d√©couvrir CloudNativePG et je n'ai fait qu'en percevoir la surface, mais une chose est s√ªre: la gestion d'une instance PostgreSQL est vraiment facilit√©e. Cependant, le choix d'une solution de base de donn√©es est une d√©cision complexe. Il faut prendre en compte le cas d'usage, les contraintes de l'entreprise, la criticit√© de l'application et les comp√©tences des √©quipes op√©rationnelles. Il existe de nombreuses options: bases de donn√©es g√©r√©es par le fournisseur Cloud, installation traditionnelle sur serveur baremetal, solutions distribu√©es ...\nNous pouvons √©galement envisager d'utiliser Crossplane et une Composition pour fournir un niveau d'abstraction suppl√©mentaire afin de d√©clarer des bases de donn√©es des fournisseurs Cloud, mais cela n√©cessite plus de configuration.\nCloudNativePG sort du lot par sa simplicit√©: Super facile √† ex√©cuter et √† comprendre. De plus, la documentation est excellente (l'une des meilleures que j'aie jamais vues!), Surtout pour un si jeune projet open source (cela aidera peut √™tre pour √™tre accept√© en tant que projet \u0026quot;Sandbox\u0026quot; CNCF ü§û).\nSi vous voulez en savoir plus, il y avait une pr√©sentation √† ce sujet √† KubeCon NA 2022.\n","link":"https://blog.ogenki.io/fr/post/cnpg/","section":"post","tags":["data"],"title":"`CloudNativePG`: et PostgreSQL devient facile sur Kubernetes"},{"body":"","link":"https://blog.ogenki.io/fr/tags/data/","section":"tags","tags":null,"title":"Data"},{"body":"In a previous article, we've seen how to use Crossplane so that we can manage cloud resources the same way as our applications. \u0026#x2764;\u0026#xfe0f; Declarative approach! There were several steps and command lines in order to get everything working and reach our target to provision a dev Kubernetes cluster.\nHere we'll achieve exactly the same thing but we'll do that in the GitOps way. According to the OpenGitOps working group there are 4 GitOps principles:\nThe desired state of our system must be expressed declaratively. This state must be stored in a versioning system. Changes are pulled and applied automatically in the target platform whenever the desired state changes. If, for any reason, the current state is modified, it will be automatically reconciled with the desired state. There are several GitOps engine options. The most famous ones are ArgoCD and Flux. We won't compare them here. I chose Flux because I like its composable architecture with different controllers, each one handling a core Flux feature (GitOps toolkit).\nLearn more about GitOps toolkit components here.\nüéØ Our target Here we want to declare our desired infrastructure components only by adding git changes. By the end of this article you'll get a GKE cluster provisioned using a local Crossplane instance. We'll discover Flux basics and how to use it in order to build a complete GitOps CD workflow.\n\u0026#x2611;\u0026#xfe0f; Requirements \u0026#x1f4e5; Install required tools First of all we need to install a few tools using asdf\nCreate a local file .tool-versions\n1cd ~/sources/devflux/ 2 3cat \u0026gt; .tool-versions \u0026lt;\u0026lt;EOF 4flux2 0.31.3 5kubectl 1.24.3 6kubeseal 0.18.1 7kustomize 4.5.5 8EOF 1for PLUGIN in $(cat .tool-versions | awk \u0026#39;{print $1}\u0026#39;); do asdf plugin-add $PLUGIN; done 2 3asdf install 4Downloading ... 100.0% 5Copying Binary 6... Check that all the required tools are actually installed.\n1asdf current 2flux2 0.31.3 /home/smana/sources/devflux/.tool-versions 3kubectl 1.24.3 /home/smana/sources/devflux/.tool-versions 4kubeseal 0.18.1 /home/smana/sources/devflux/.tool-versions 5kustomize 4.5.5 /home/smana/sources/devflux/.tool-versions \u0026#x1f511; Create a Github personal access token In this article the git repository is hosted in Github. In order to be able to use the flux bootstrap a personnal access token is required.\nPlease follow this procedure.\nWarning Store the Github token in a safe place for later use\n\u0026#x1f9d1;\u0026zwj;\u0026#x1f4bb; Clone the devflux repository All the files used for the upcoming steps can be retrieved from this repository. You should clone it, that will be easier to copy them into your own repository.\n1git clone https://github.com/Smana/devflux.git \u0026#x1f680; Bootstrap flux in the Crossplane cluster As we will often be using the flux CLI you may want to configure the bash|zsh completion\n1source \u0026lt;(flux completion bash) Warning Here we consider that you already have a local k3d instance. If not you may want to either go through the whole previous article or just run the local cluster creation.\nEnsure that you're working in the right context\n1kubectl config current-context 2k3d-crossplane Run the bootstrap command that will basically deploy all Flux's components in the namespace flux-system. Here I'll create a repository named devflux using my personal Github account.\n1export GITHUB_USER=\u0026lt;YOUR_ACCOUNT\u0026gt; 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/k3d-crossplane 6‚ñ∫ cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git\u0026#34; 7... 8‚úî configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/k3d-crossplane\u0026#34; for \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux\u0026#34; 9... 10‚úî all components are healthy Check that all the pods are running properly and that the kustomization flux-system has been successfully reconciled.\n1kubectl get po -n flux-system 2NAME READY STATUS RESTARTS AGE 3helm-controller-5985c795f8-gs2pc 1/1 Running 0 86s 4notification-controller-6b7d7485fc-lzlpg 1/1 Running 0 86s 5kustomize-controller-6d4669f847-9x844 1/1 Running 0 86s 6source-controller-5fb4888d8f-wgcqv 1/1 Running 0 86s 7 8flux get kustomizations 9NAME REVISION SUSPENDED READY MESSAGE 10flux-system main/33ebef1 False True Applied revision: main/33ebef1 Running the bootstap command actually creates a github repository if it doesn't exist yet. Clone it now for our upcoming changes. You'll notice that the first commit has been made by Flux.\n1git clone https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git 2Cloning into \u0026#39;devflux\u0026#39;... 3 4cd devflux 5 6git log -1 7commit 2beb6aafea67f3386b50cbc706fb34575844040d (HEAD -\u0026gt; main, origin/main, origin/HEAD) 8Author: Flux \u0026lt;\u0026gt; 9Date: Thu Jul 14 17:13:27 2022 +0200 10 11 Add Flux sync manifests 12 13ls clusters/k3d-crossplane/flux-system/ 14gotk-components.yaml gotk-sync.yaml kustomization.yaml \u0026#x1f4c2; Flux repository structure There are several options for organizing your resources in the Flux configuration repository. Here is a proposition for the sake of this article.\n1tree -d -L 2 2. 3‚îú‚îÄ‚îÄ apps 4‚îÇ¬†‚îú‚îÄ‚îÄ base 5‚îÇ¬†‚îî‚îÄ‚îÄ dev-cluster 6‚îú‚îÄ‚îÄ clusters 7‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 8‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 9‚îú‚îÄ‚îÄ infrastructure 10‚îÇ¬†‚îú‚îÄ‚îÄ base 11‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 12‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 13‚îú‚îÄ‚îÄ observability 14‚îÇ¬†‚îú‚îÄ‚îÄ base 15‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 16‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 17‚îî‚îÄ‚îÄ security 18 ‚îú‚îÄ‚îÄ base 19 ‚îú‚îÄ‚îÄ dev-cluster 20 ‚îî‚îÄ‚îÄ k3d-crossplane Directory Description Example /apps our applications Here we'll deploy a demo application \u0026quot;online-boutique\u0026quot; /infrastructure base infrastructure/network components Crossplane as it will be used to provision cloud resources but we can also find CSI/CNI/EBS drivers... /observability All metrics/apm/logging tools Prometheus of course, Opentelemetry ... /security Any component that enhance our security level SealedSecrets (see below) Info For the upcoming steps please refer to the demo repository here\nLet's use this structure and begin to deploy applications \u0026#x1f680;.\n\u0026#x1f510; SealedSecrets There are plenty of alternatives when it comes to secrets management in Kubernetes. In order to securely store secrets in a git repository the GitOps way we'll make use of SealedSecrets. It uses a custom resource definition named SealedSecrets in order to encrypt the Kubernetes secret at the client side then the controller is in charge of decrypting and generating the expected secret in the cluster.\n\u0026#x1f6e0;\u0026#xfe0f; Deploy the controller using Helm The first thing to do is to declare the kustomization that handles all the security tools.\nclusters/k3d-crossplane/security.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: security 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 sourceRef: 10 kind: GitRepository 11 name: flux-system 12 path: ./security/k3d-crossplane 13 healthChecks: 14 - apiVersion: helm.toolkit.fluxcd.io/v1beta1 15 kind: HelmRelease 16 name: sealed-secrets 17 namespace: kube-system Info A Kustomization is a custom resource that comes with Flux. It basically points to a set of Kubernetes resources managed with kustomize The above security kustomization points to a local directory where the kustomize resources are.\n1... 2spec: 3 path: ./security/k3d-crossplane 4... Note This is worth noting that there are two types on kustomizations. That can be confusing when you start playing with Flux.\nOne managed by flux's kustomize controller. Its API is kustomization.kustomize.toolkit.fluxcd.io The other kustomization.kustomize.config.k8s.io is for the kustomize overlay The kustomization.yaml file is always used for the kustomize overlay. Flux itself doesn't need this overlay in all cases, but if you want to use features of a Kustomize overlay you will occasionally need to create it in order to access them. It provides instructions to the Kustomize CLI.\nWe will deploy SealedSecrets using the Helm chart. So we need to declare the source of this chart. Using the kustomize overlay system, we'll first create the base files that will be inherited at the cluster level.\nsecurity/base/sealed-secrets/source.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: sealed-secrets 5 namespace: flux-system 6spec: 7 interval: 30m 8 url: https://bitnami-labs.github.io/sealed-secrets Then we'll define the HelmRelease which references the above source. Put the values you want to apply to the Helm chart under spec.values\nsecurity/base/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 releaseName: sealed-secrets 8 chart: 9 spec: 10 chart: sealed-secrets 11 sourceRef: 12 kind: HelmRepository 13 name: sealed-secrets 14 namespace: flux-system 15 version: \u0026#34;2.4.0\u0026#34; 16 interval: 10m0s 17 install: 18 remediation: 19 retries: 3 20 values: 21 fullnameOverride: sealed-secrets-controller 22 resources: 23 requests: 24 cpu: 80m 25 memory: 100Mi If you're starting your repository from scratch you'll need to generate the kustomization.yaml file (kustomize overlay).\n1kustomize create --autodetect security/base/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4- helmrelease.yaml 5- source.yaml Now we declare the sealed-secret kustomization at the cluster level. Just for the example we'll overwrite a value at the cluster level using kustomize's overlay system.\nsecurity/k3d-crossplane/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 values: 8 resources: 9 requests: 10 cpu: 100m security/k3d-crossplane/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3bases: 4 - ../../base 5patches: 6 - helmrelease.yaml Pushing our changes is the only thing to do in order to get sealed-secrets deployed in the target cluster.\n1git commit -m \u0026#34;security: deploy sealed-secrets in k3d-crossplane\u0026#34; 2[security/sealed-secrets 283648e] security: deploy sealed-secrets in k3d-crossplane 3 6 files changed, 66 insertions(+) 4 create mode 100644 clusters/k3d-crossplane/security.yaml 5 create mode 100644 security/base/sealed-secrets/helmrelease.yaml 6 create mode 100644 security/base/sealed-secrets/kustomization.yaml 7 create mode 100644 security/base/sealed-secrets/source.yaml 8 create mode 100644 security/k3d-crossplane/sealed-secrets/helmrelease.yaml 9 create mode 100644 security/k3d-crossplane/sealed-secrets/kustomization.yaml After a few seconds (1 minutes by default) a new kustomization will appear.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3flux-system main/d36a33c False True Applied revision: main/d36a33c 4security main/d36a33c False True Applied revision: main/d36a33c And all the resources that we declared in the flux repository should be available and READY.\n1flux get sources helm 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee False True stored artifact for revision \u0026#39;4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee\u0026#39; 1flux get helmrelease -n kube-system 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 2.2.0 False True Release reconciliation succeeded üß™ A first test SealedSecret Let's use the CLI kubeseal to test it out. We'll create a SealedSecret that will be decrypted by the sealed-secrets controller in the cluster and create the expected secret foobar\n1kubectl create secret generic foobar -n default --dry-run=client -o yaml --from-literal=foo=bar \\ 2| kubeseal --namespace default --format yaml | kubectl apply -f - 3sealedsecret.bitnami.com/foobar created 4 5kubectl get secret -n default foobar 6NAME TYPE DATA AGE 7foobar Opaque 1 3m13s 8 9kubectl delete sealedsecrets.bitnami.com foobar 10sealedsecret.bitnami.com \u0026#34;foobar\u0026#34; deleted \u0026#x2601;\u0026#xfe0f; Deploy and configure Crossplane \u0026#x1f511; Create the Google service account secret The first thing we need to do in order to get Crossplane working is to create the GCP serviceaccount. The steps have been covered here in the previous article. We'll create a SealedSecret gcp-creds that contains the serviceaccount file crossplane.json.\ninfrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml\n1kubectl create secret generic gcp-creds --context k3d-crossplane -n crossplane-system --from-file=creds=./crossplane.json --dry-run=client -o yaml \\ 2| kubeseal --format yaml --namespace crossplane-system - \u0026gt; infrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml üîÑ Crossplane dependencies Now we will deploy Crossplane with Flux. I won't put the manifests here you'll find all of them in this repository. However it's important to understand that, in order to deploy and configure Crossplane properly we need to do that in a specific order. Indeed several CRD's (custom resource definitions) are required:\nFirst of all we'll install the crossplane controller. Then we'll configure the provider because the custom resource is now available thanks to the crossplane controller installation. Finally a provider installation deploys several CRDs that can be used to configure the provider itself and cloud resources. The dependencies between kustomizations can be controlled using the parameters dependsOn. Looking at the file clusters/k3d-crossplane/infrastructure.yaml, we can see for example that the kustomization infrastructure-custom-resources depends on the kustomization crossplane_provider which itself depends on crossplane-configuration....\n1--- 2apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 3kind: Kustomization 4metadata: 5 name: crossplane-provider 6spec: 7... 8 dependsOn: 9 - name: crossplane-core 10--- 11apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 12kind: Kustomization 13metadata: 14 name: crossplane-configuration 15spec: 16... 17 dependsOn: 18 - name: crossplane-provider 19--- 20apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 21kind: Kustomization 22metadata: 23 name: infrastructure-custom-resources 24spec: 25... 26 dependsOn: 27 - name: crossplane-configuration Commit and push the changes for the kustomisations to appear. Note that they'll be reconciled in the defined order.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3infrastructure-custom-resources False False dependency \u0026#39;flux-system/crossplane-configuration\u0026#39; is not ready 4crossplane-configuration False False dependency \u0026#39;flux-system/crossplane-provider\u0026#39; is not ready 5security main/666f85a False True Applied revision: main/666f85a 6flux-system main/666f85a False True Applied revision: main/666f85a 7crossplane-core main/666f85a False True Applied revision: main/666f85a 8crossplane-provider main/666f85a False True Applied revision: main/666f85a Then all Crossplane components will be deployed, we can have a look to the HelmRelease status for instance.\n1kubectl describe helmrelease -n crossplane-system crossplane 2... 3Status: 4 Conditions: 5 Last Transition Time: 2022-07-15T19:12:04Z 6 Message: Release reconciliation succeeded 7 Reason: ReconciliationSucceeded 8 Status: True 9 Type: Ready 10 Last Transition Time: 2022-07-15T19:12:04Z 11 Message: Helm upgrade succeeded 12 Reason: UpgradeSucceeded 13 Status: True 14 Type: Released 15 Helm Chart: crossplane-system/crossplane-system-crossplane 16 Last Applied Revision: 1.9.0 17 Last Attempted Revision: 1.9.0 18 Last Attempted Values Checksum: 056dc1c6029b3a644adc7d6a69a93620afd25b65 19 Last Release Revision: 2 20 Observed Generation: 1 21Events: 22 Type Reason Age From Message 23 ---- ------ ---- ---- ------- 24 Normal info 20m helm-controller HelmChart \u0026#39;crossplane-system/crossplane-system-crossplane\u0026#39; is not ready 25 Normal info 20m helm-controller Helm upgrade has started 26 Normal info 19m helm-controller Helm upgrade succeeded And our GKE cluster should also be created because we defined a bunch of crossplane custom resources in infrastructure/k3d-crossplane/custom-resources/crossplane\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RUNNING 34.x.x.190 europe-west9-a 22m \u0026#x1f680; Bootstrap flux in the dev cluster Our local Crossplane cluster is now ready and it created our dev cluster and we also want it to be managed with Flux. So let's configure Flux for this dev cluster using the same bootstrap command.\nAuthenticate to the newly created cluster. The following command will automatically change your current context.\n1gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project \u0026lt;your_project\u0026gt; 2Fetching cluster endpoint and auth data. 3kubeconfig entry generated for dev-cluster. 4 5kubectl config current-context 6gke_\u0026lt;your_project\u0026gt;_europe-west9-a_dev-cluster Run the bootstrap command for the dev-cluster.\n1export GITHUB_USER=Smana 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/dev-cluster 6‚ñ∫ cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/Smana/devflux.git\u0026#34; 7... 8‚úî configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/dev-cluster\u0026#34; for \u0026#34;https://github.com/Smana/devflux\u0026#34; 9... 10‚úî all components are healthy Note It's worth noting that each Kubernetes cluster generates its own sealing keys. That means that if you recreate the dev-cluster, you must regenerate all the sealedsecrets. In our example we declared a secret in order to set the Grafana credentials. Here's the command you need to run in order to create a new version of the sealedsecret and don't forget to use the proper context \u0026#x1f609;.\n1kubectl create secret generic kube-prometheus-stack-grafana \\ 2--from-literal=admin-user=admin --from-literal=admin-password=\u0026lt;yourpassword\u0026gt; --namespace observability --dry-run=client -o yaml \\ 3| kubeseal --namespace observability --format yaml \u0026gt; observability/dev-cluster/kube-prometheus-stack/sealedsecrets.yaml After a few seconds we'll get the following kustomizations deployed.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3apps main/1380eaa False True Applied revision: main/1380eaa 4flux-system main/1380eaa False True Applied revision: main/1380eaa 5observability main/1380eaa False True Applied revision: main/1380eaa 6security main/1380eaa False True Applied revision: main/1380eaa Here we configured the prometheus stack and deployed a demo microservices stack named \u0026quot;online-boutique\u0026quot; This demo application exposes the frontend through a service of type LoadBalancer.\n1kubectl get svc -n demo frontend-external 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3frontend-external LoadBalancer 10.140.174.201 34.155.121.2 80:31943/TCP 7m44s Use the EXTERNAL_IP\n\u0026#x1f575;\u0026#xfe0f; Troubleshooting The cheatsheet in Flux's documentation contains many ways for troubleshooting when something goes wrong. Here I'll just give a sample of my favorite command lines.\nObjects that aren't ready\n1flux get all -A --status-selector ready=false Checking the logs of a given kustomization\n1flux logs --kind kustomization --name infrastructure-custom-resources 22022-07-15T19:38:52.996Z info Kustomization/infrastructure-custom-resources.flux-system - server-side apply completed 32022-07-15T19:38:53.016Z info Kustomization/infrastructure-custom-resources.flux-system - Reconciliation finished in 66.12266ms, next run in 4m0s 42022-07-15T19:11:34.697Z info Kustomization/infrastructure-custom-resources.flux-system - Discarding event, no alerts found for the involved object Show how a given pod is managed by Flux.\n1flux trace -n crossplane-system pod/crossplane-5dc8d888d7-g95qx 2 3Object: Pod/crossplane-5dc8d888d7-g95qx 4Namespace: crossplane-system 5Status: Managed by Flux 6--- 7HelmRelease: crossplane 8Namespace: crossplane-system 9Revision: 1.9.0 10Status: Last reconciled at 2022-07-15 21:12:04 +0200 CEST 11Message: Release reconciliation succeeded 12--- 13HelmChart: crossplane-system-crossplane 14Namespace: crossplane-system 15Chart: crossplane 16Version: 1.9.0 17Revision: 1.9.0 18Status: Last reconciled at 2022-07-15 21:11:36 +0200 CEST 19Message: pulled \u0026#39;crossplane\u0026#39; chart with version \u0026#39;1.9.0\u0026#39; 20--- 21HelmRepository: crossplane 22Namespace: crossplane-system 23URL: https://charts.crossplane.io/stable 24Revision: 362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d 25Status: Last reconciled at 2022-07-15 21:11:35 +0200 CEST 26Message: stored artifact for revision \u0026#39;362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d\u0026#39; If you want to check what would be the changes before pushing your commit. In thi given example I just increased the cpu requests for the sealed-secrets controller.\n1flux diff kustomization security --path security/k3d-crossplane 2‚úì Kustomization diffing... 3‚ñ∫ HelmRelease/kube-system/sealed-secrets drifted 4 5metadata.generation 6 ¬± value change 7 - 6 8 + 7 9 10spec.values.resources.requests.cpu 11 ¬± value change 12 - 100m 13 + 120m 14 15‚ö†Ô∏è identified at least one change, exiting with non-zero exit code \u0026#x1f9f9; Cleanup Don't forget to delete the Cloud resources if you don't want to have a bad suprise \u0026#x1f4b5;! Just comment the file infrastructure/k3d-crossplane/custom-resources/crossplane/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 # - cluster.yaml 5 - network.yaml \u0026#x1f44f; Achievements With our current setup everything is configured using the GitOps approach:\nWe can manage infrastructure resources using Crossplane. Our secrets are securely stored in our git repository. We have a dev-cluster that we can enable or disable just but commenting a yaml file. Our demo application can be deployed from scratch in seconds. üí≠ final thoughts Flux is probably the tool I'm using the most on a daily basis. It's really amazing!\nWhen you get familiar with its concepts and the command line it becomes really easy to use and troubleshoot. You can use either Helm when a chart is available or Kustomize.\nHowever we faced a few issues:\nIt's not straightforward to find an efficient structure depending on the company needs. Especially when you have several Kubernetes controllers that depend on other CRDs. The Helm controller doesn't maintain a state of the Kubernetes resources deployed by the Helm chart. That means that if you delete a resource which has been deployed through a Helm chart, it won't be reconciled (It will change soon. Being discussed here) Flux doesn't provide itself a web UI and switching between CLIs (kubectl, flux ...) can be annoying from a developer perspective. (I'm going to test weave-gitops ) I've been using Flux in production for more than a year and we configured it with the image automation so that the only thing a developer has to do is to merge a pull request and the new version of the application is automatically deployed in the target cluster.\nI should probably give another try to ArgoCD in order to be able to compare these precisely ü§î.\n","link":"https://blog.ogenki.io/fr/post/devflux/","section":"post","tags":["gitops","devxp"],"title":"100% `GitOps` using Flux"},{"body":"Qui suis-je? Je suis un ing√©nieur syst√®me / SRE senior. Je porte un int√©r√™t particulier aux conteneurs Linux et les technologies d√Ætes \u0026quot;Cloud Native\u0026quot;. Au fil de ma carri√®re, j'ai eu l'occasion de collaborer avec une vari√©t√© d'entreprises, des startups dynamiques aux grandes structures, o√π j'ai contribu√© √† optimiser la fiabilit√© et la disponibilit√© des plateformes, tout en am√©liorant l'exp√©rience des d√©veloppeurs. Mon parcours inclut l'accompagnement de plusieurs soci√©t√©s dans leur migration vers des solutions Cloud. J'ai √©galement pu diriger des √©quipes SRE/DevOps compos√©es de profils vari√©s, incluant d√©veloppeurs et ing√©nieurs SRE, avec pour objectif un engagement collectif vers des objectifs communs.\nEn parall√®le de mon parcours professionnel, je m'investis dans l'organisation du meetup Cloud Native Computing √† Paris ainsi que les Kubernetes Community Days France , refl√©tant mon engagement continu dans l'√©cosyst√®me du Cloud.\nAutres centre d'int√©ret : Lecture de romans de science-fiction, Kickboxing, Surf, Longboard et Rollers..\nLes images sympas (miniatures) de chaque article ont √©t√© g√©n√©r√©es avec DALL-E\n","link":"https://blog.ogenki.io/fr/about/","section":"","tags":null,"title":"About"},{"body":"La cible de cette documentation est de pouvoir cr√©er et g√©rer un cluster GKE en utilisant Crossplane.\nCrossplane exploite les principes de base de Kubernetes afin de fournir des ressources cloud et bien plus encore: une ** approche d√©clarative ** avec ** Drift Detections ** et ** r√©conciliations ** Utilisation de boucles de contr√¥le: Exploseding_head:.En d'autres termes, nous d√©clarons les ressources cloud que nous voulons et Crossplane garantit que l'√©tat cible correspond √† celui appliqu√© via l'API Kubernetes.\nVoici les √©tapes que nous suivrons afin d'obtenir un cluster Kubernetes pour le d√©veloppement et les cas d'utilisation des exp√©rimentations.\n\u0026#x1f433; Create the local k3d cluster for Crossplane's control plane k3d est un cluster Kubernetes l√©ger qui exploite K3S qui s'ex√©cute dans notre ordinateur portable local. Il existe plusieurs mod√®les de d√©ploiement pour Crossplane, nous pourrions par exemple d√©ployer le plan de contr√¥le sur un cluster de gestion sur Kubernetes ou un plan de contr√¥le par cluster Kubernetes. Ici, j'ai choisi une m√©thode simple qui est bien pour un cas d'utilisation personnelle: une instance Kubernetes locale ** dans laquelle je vais d√©ployer Crossplane.\nLet's install k3d using asdf.\n1asdf plugin-add k3d 2 3asdf install k3d $(asdf latest k3d) 4* Downloading k3d release 5.4.1... 5k3d 5.4.1 installation was successful! Create a single node Kubernetes cluster.\n1k3d cluster create crossplane 2... 3INFO[0043] You can now use it like this: 4kubectl cluster-info 5 6k3d cluster list 7crossplane 1/1 0/0 true Check that the cluster is reachable using the kubectl CLI.\n1kubectl cluster-info 2Kubernetes control plane is running at https://0.0.0.0:40643 3CoreDNS is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4Metrics-server is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy We only need a single node for our Crossplane use case.\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k3d-crossplane-server-0 Ready control-plane,master 26h v1.22.7+k3s1 \u0026#x2601;\u0026#xfe0f; Generate the Google Cloud service account Warning Store the downloaded crossplane.json credentials file in a safe place.\nCreate a service account\n1GCP_PROJECT=\u0026lt;your_project\u0026gt; 2gcloud iam service-accounts create crossplane --display-name \u0026#34;Crossplane\u0026#34; --project=${GCP_PROJECT} 3Created service account [crossplane]. Assign the proper permissions to the service account.\nCompute Network Admin Kubernetes Engine Admin Service Account User 1SA_EMAIL=$(gcloud iam service-accounts list --filter=\u0026#34;email ~ ^crossplane\u0026#34; --format=\u0026#39;value(email)\u0026#39;) 2 3gcloud projects add-iam-policy-binding \u0026#34;${GCP_PROJECT}\u0026#34; --member=serviceAccount:\u0026#34;${SA_EMAIL}\u0026#34; \\ 4--role=roles/container.admin --role=roles/compute.networkAdmin --role=roles/iam.serviceAccountUser 5Updated IAM policy for project [\u0026lt;project\u0026gt;]. 6bindings: 7- members: 8 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 9 role: roles/compute.networkAdmin 10- members: 11 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 12... 13version: 1 Download the service account key (json format)\n1gcloud iam service-accounts keys create crossplane.json --iam-account ${SA_EMAIL} 2created key [ea2eb9ce2939127xxxxxxxxxx] of type [json] as [crossplane.json] for [crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com] \u0026#x1f6a7; Deploy and configure Crossplane Now that we have a credentials file for Google Cloud, we can deploy the Crossplane operator and configure the provider-gcp provider.\nInfo Most of the following steps are issued from the official documentation\nWe'll first use Helm in order to install the operator\n1helm repo add crossplane-master https://charts.crossplane.io/master/ 2\u0026#34;crossplane-master\u0026#34; has been added to your repositories 3 4helm repo update 5...Successfully got an update from the \u0026#34;crossplane-master\u0026#34; chart repository 6 7helm install crossplane --namespace crossplane-system --create-namespace \\ 8--version 1.18.1 crossplane-stable/crossplane 9 10NAME: crossplane 11LAST DEPLOYED: Mon Jun 6 22:00:02 2022 12NAMESPACE: crossplane-system 13STATUS: deployed 14REVISION: 1 15TEST SUITE: None 16NOTES: 17Release: crossplane 18... Check that the operator is running properly.\n1kubectl get po -n crossplane-system 2NAME READY STATUS RESTARTS AGE 3crossplane-rbac-manager-54d96cd559-222hc 1/1 Running 0 3m37s 4crossplane-688c575476-lgklq 1/1 Running 0 3m37s Info All the files used for the upcoming steps are stored within this blog repository. So you should clone and change the current directory:\n1git clone https://github.com/Smana/smana.github.io.git 2 3cd smana.github.io/content/resources/crossplane_k3d Now we'll configure Crossplane so that it will be able to create and manage GCP resources. This is done by configuring the provider provider-gcp as follows.\nprovider.yaml\n1apiVersion: pkg.crossplane.io/v1 2kind: Provider 3metadata: 4 name: crossplane-provider-gcp 5spec: 6 package: crossplane/provider-gcp:v0.21.0 1kubectl apply -f provider.yaml 2provider.pkg.crossplane.io/crossplane-provider-gcp created 3 4kubectl get providers 5NAME INSTALLED HEALTHY PACKAGE AGE 6crossplane-provider-gcp True True crossplane/provider-gcp:v0.21.0 10s Create the Kubernetes secret that holds the GCP credentials file created above\n1kubectl create secret generic gcp-creds -n crossplane-system --from-file=creds=./crossplane.json 2secret/gcp-creds created Then we need to create a resource named ProviderConfig and reference the newly created secret.\nprovider-config.yaml\n1apiVersion: gcp.crossplane.io/v1beta1 2kind: ProviderConfig 3metadata: 4 name: default 5spec: 6 projectID: ${GCP_PROJECT} 7 credentials: 8 source: Secret 9 secretRef: 10 namespace: crossplane-system 11 name: gcp-creds 12 key: creds 1kubectl apply -f provider-config.yaml 2providerconfig.gcp.crossplane.io/default created Info If the serviceaccount has the proper permissions we can create resources in GCP. In order to learn about all the available resources and parameters we can have a look to the provider's API reference.\nThe first resource we'll create is the network that will host our Kubernetes cluster.\nnetwork.yaml\n1apiVersion: compute.gcp.crossplane.io/v1beta1 2kind: Network 3metadata: 4 name: dev-network 5 labels: 6 service: vpc 7 creation: crossplane 8spec: 9 forProvider: 10 autoCreateSubnetworks: false 11 description: \u0026#34;Network used for experimentations and POCs\u0026#34; 12 routingConfig: 13 routingMode: REGIONAL 1kubectl get network 2NAME READY SYNCED 3dev-network True True You can even get more details by describing this resource. For instance if something fails you would see the message returned by the Cloud provider in the events.\n1kubectl describe network dev-network | grep -A 20 \u0026#39;^Status:\u0026#39; 2Status: 3 At Provider: 4 Creation Timestamp: 2022-06-28T09:45:30.703-07:00 5 Id: 3005424280727359173 6 Self Link: https://www.googleapis.com/compute/v1/projects/${GCP_PROJECT}/global/networks/dev-network 7 Conditions: 8 Last Transition Time: 2022-06-28T16:45:31Z 9 Reason: Available 10 Status: True 11 Type: Ready 12 Last Transition Time: 2022-06-30T16:36:59Z 13 Reason: ReconcileSuccess 14 Status: True 15 Type: Synced \u0026#x1f680; Create a GKE cluster Everything is ready so that we can create our GKE cluster. Applying the file cluster.yaml will create a cluster and attach a node group to it.\ncluster.yaml\n1--- 2apiVersion: container.gcp.crossplane.io/v1beta2 3kind: Cluster 4metadata: 5 name: dev-cluster 6spec: 7 forProvider: 8 description: \u0026#34;Kubernetes cluster for experimentations and POCs\u0026#34; 9 initialClusterVersion: \u0026#34;1.24\u0026#34; 10 releaseChannel: 11 channel: \u0026#34;RAPID\u0026#34; 12 location: europe-west9-a 13 addonsConfig: 14 gcePersistentDiskCsiDriverConfig: 15 enabled: true 16 networkPolicyConfig: 17 disabled: false 18 networkRef: 19 name: dev-network 20 ipAllocationPolicy: 21 createSubnetwork: true 22 useIpAliases: true 23 defaultMaxPodsConstraint: 24 maxPodsPerNode: 110 25 networkPolicy: 26 enabled: false 27 writeConnectionSecretToRef: 28 namespace: default 29 name: gke-conn 30--- 31apiVersion: container.gcp.crossplane.io/v1beta1 32kind: NodePool 33metadata: 34 name: main-np 35spec: 36 forProvider: 37 initialNodeCount: 1 38 autoscaling: 39 autoprovisioned: false 40 enabled: true 41 maxNodeCount: 4 42 minNodeCount: 1 43 clusterRef: 44 name: dev-cluster 45 config: 46 machineType: n2-standard-2 47 diskSizeGb: 120 48 diskType: pd-standard 49 imageType: cos_containerd 50 preemptible: true 51 labels: 52 environment: dev 53 managed-by: crossplane 54 oauthScopes: 55 - \u0026#34;https://www.googleapis.com/auth/devstorage.read_only\u0026#34; 56 - \u0026#34;https://www.googleapis.com/auth/logging.write\u0026#34; 57 - \u0026#34;https://www.googleapis.com/auth/monitoring\u0026#34; 58 - \u0026#34;https://www.googleapis.com/auth/servicecontrol\u0026#34; 59 - \u0026#34;https://www.googleapis.com/auth/service.management.readonly\u0026#34; 60 - \u0026#34;https://www.googleapis.com/auth/trace.append\u0026#34; 61 metadata: 62 disable-legacy-endpoints: \u0026#34;true\u0026#34; 63 shieldedInstanceConfig: 64 enableIntegrityMonitoring: true 65 enableSecureBoot: true 66 management: 67 autoRepair: true 68 autoUpgrade: true 69 maxPodsConstraint: 70 maxPodsPerNode: 60 71 locations: 72 - \u0026#34;europe-west9-a\u0026#34; 1kubectl apply -f cluster.yaml 2cluster.container.gcp.crossplane.io/dev-cluster created 3nodepool.container.gcp.crossplane.io/main-np created Note that it takes around 10 minutes for the Kubernetes API and the nodes to be available. The STATE will transition from PROVISIONING to RUNNING and when a change is being applied the cluster status is RECONCILING\n1watch \u0026#39;kubectl get cluster,nodepool\u0026#39; 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3cluster.container.gcp.crossplane.io/dev-cluster False True PROVISIONING 34.155.122.6 europe-west9-a 3m15s 4 5NAME READY SYNCED STATE CLUSTER-REF AGE 6nodepool.container.gcp.crossplane.io/main-np False False dev-cluster 3m15s When the column READY switches to True you can download the cluster's credentials.\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RECONCILING 34.42.42.42 europe-west9-a 6m23s 4 5gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project ${GCP_PROJECT} 6Fetching cluster endpoint and auth data. 7kubeconfig entry generated for dev-cluster. For better readability you may want to rename the context id for the newly created cluster\n1kubectl config rename-context gke_${GCP_PROJECT}_europe-west9-a_dev-cluster dev-cluster 2Context \u0026#34;gke_${GCP_PROJECT}_europe-west9-a_dev-cluster\u0026#34; renamed to \u0026#34;dev-cluster\u0026#34;. 3 4kubectl config get-contexts 5CURRENT NAME CLUSTER AUTHINFO NAMESPACE 6* dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster 7 k3d-crossplane k3d-crossplane admin@k3d-crossplane Check that you can call our brand new GKE API\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3gke-dev-cluster-main-np-d0d978f9-5fc0 Ready \u0026lt;none\u0026gt; 10m v1.24.1-gke.1400 That's great \u0026#x1f389; we know have a GKE cluster up and running.\nüí≠ final thoughts I've been using Crossplane for a few months now in a production environment.\nEven if I'm conviced about the declarative approach using the Kubernetes API, we decided to move with caution with it. It clearly doesn't have Terraform's community and maturity. We're still declaring our resources using the deletionPolicy: Orphan so that even if something goes wrong on the controller side the resource won't be deleted.\nFurthermore we limited to a specific list of usual AWS resources requested by our developers. Nevertheless our target has always been to empower developers and we had really positive feedback from them. That's the best indicator for us. As the project matures, we'll move more and more resources from Terraform to Crossplane.\nIMHO the key success of Crossplane depends on the providers maintenance and evolution. The Cloud providers interest and involvement is really important.\nIn our next article we'll see how to use a GitOps engine to run all the above steps.\n","link":"https://blog.ogenki.io/fr/post/crossplane_k3d/","section":"post","tags":["kubernetes","infrastructure"],"title":"Mon cluster Kubernetes (GKE) avec `Crossplane`"},{"body":"Afin d'installer des binaires et de pouvoir passer d'une version √† une autre, j'aime utiliser asdf.\n\u0026#x1f4e5; Installation L'installation recommand√©e consiste √† utiliser Git comme suit\n1git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.10.0 Il y a quelques √©tapes suppl√©mentaires qui d√©pendent de votre shell. Voici celles que j'utilise pour bash:\n1. $HOME/.asdf/asdf.sh Vous voudrez probablement configurer la completion du shell comme suit\n1. $HOME/.asdf/completions/asdf.bash \u0026#x1f680; Prenons un exemple Listons tous les plugins disponibles pour trouver k3d\n1asdf plugin-list-all | grep k3d 2k3d https://github.com/spencergilbert/asdf-k3d.git Installons k3d\n1asdf plugin-add k3d V√©rifier les versions disponibles\n1asdf list-all k3d| tail -n 3 25.4.0-dev.3 35.4.0 45.4.1 Nous installerons la derni√®re version\n1asdf install k3d latest 2* Downloading k3d release 5.4.1... 3k3d 5.4.1 installation was successful! Enfin, nous pouvons passer d'une version √† une autre. Nous pouvons d√©finir une version \u0026quot;globale\u0026quot; qui serait utilis√©e sur tous les r√©pertoires.\n1asdf global k3d 5.4.1 ou utilisez une version locale en fonction du r√©pertoire actuel\n1cd /tmp 2asdf local k3d 5.4.1 3 4asdf current k3d 5k3d 5.4.1 /tmp/.tool-versions \u0026#x1f9f9; Faire le m√©nage D√©sinstaller une version donn√©e\n1asdf uninstall k3d 5.4.1 Retirer un plugin\n1asdf plugin remove k3d ","link":"https://blog.ogenki.io/fr/post/asdf/asdf/","section":"post","tags":["tooling","local"],"title":"G√©rer les versions d'outils avec `asdf`"},{"body":"","link":"https://blog.ogenki.io/fr/tags/local/","section":"tags","tags":null,"title":"Local"},{"body":"","link":"https://blog.ogenki.io/fr/tags/tooling/","section":"tags","tags":null,"title":"Tooling"},{"body":"","link":"https://blog.ogenki.io/fr/categories/","section":"categories","tags":null,"title":"Categories"}]