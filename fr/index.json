[{"body":"","link":"https://blog.ogenki.io/fr/","section":"","tags":null,"title":""},{"body":"","link":"https://blog.ogenki.io/fr/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"https://blog.ogenki.io/fr/tags/observability/","section":"tags","tags":null,"title":"Observability"},{"body":"","link":"https://blog.ogenki.io/fr/series/observability/","section":"series","tags":null,"title":"Observability"},{"body":"","link":"https://blog.ogenki.io/fr/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://blog.ogenki.io/fr/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://blog.ogenki.io/fr/tags/","section":"tags","tags":null,"title":"Tags"},{"body":" Comment se portent nos applications? ğŸ‘ï¸ Une fois que notre application est dÃ©ployÃ©e, il est primordial de disposer d'indicateurs permettant d'identifier d'Ã©ventuels problÃ¨mes ainsi que suivre les Ã©volutions de performance. Parmi ces Ã©lÃ©ments, les mÃ©triques et les logs jouent un rÃ´le essentiel en fournissant des informations prÃ©cieuses sur le fonctionnement de l'application. En complÃ©ment, il est souvent utile de mettre en place un tracing dÃ©taillÃ© pour suivre prÃ©cisÃ©ment toutes les actions rÃ©alisÃ©es par l'application.\nDans cette sÃ©rie d'articles, nous allons explorer les diffÃ©rents aspects liÃ©s Ã  la supervision applicative. L'objectif Ã©tant d'analyser en dÃ©tail l'Ã©tat de nos applications, afin d'amÃ©liorer leur disponibilitÃ© et leurs performances, tout en garantissant une expÃ©rience utilisateur optimale.\nCe premier volet est consacrÃ© Ã  la collecte et la visualisation des mÃ©triques. Nous allons dÃ©ployer une solution performante et Ã©volutive pour acheminer ces mÃ©triques vers un systÃ¨me de stockage fiable et pÃ©renne. Puis nous allons voir comment les visualiser afin de les analyser.\nâ“ Qu'est ce qu'une mÃ©trique Definition Avant de collecter cette dite \u0026quot;mÃ©trique\u0026quot;, penchons-nous d'abord sur sa dÃ©finition et ses spÃ©cificitÃ©s: Une mÃ©trique est une donnÃ©e mesurable qui permet de suivre l'Ã©tat et les performances d'une application. Ces donnÃ©es sont gÃ©nÃ©ralement des chiffres collectÃ©s Ã  intervals rÃ©guliers, on peut citer par exemple le nombre de requÃªtes, la quantitÃ© de mÃ©moire ou le taux d'erreurs.\nEt quand on s'intÃ©resse au domaine de la supervision, il est difficile de passer Ã  cotÃ© de Prometheus. Ce projet a notamment permis le l'Ã©mergence d'un standard qui dÃ©finit la maniÃ¨re dont on expose des mÃ©triques appelÃ© OpenMetrics dont voici le format.\nTime Series: Une time series unique est la combinaison du nom de la mÃ©trique ainsi que ses labels, par consÃ©quent request_total{code=\u0026quot;200\u0026quot;} et request_total{code=\u0026quot;500\u0026quot;} sont bien 2 time series distinctes.\nLabels: On peut associer des labels Ã  une mÃ©trique afin de la caractÃ©riser plus prÃ©cisÃ©ment. Ils sont ajoutÃ©es Ã  la suite du nom de la mÃ©trique en utilisant des accolades. Bien qu'ils soient optionnels, nous les retrouverons trÃ¨s souvent, notamment dans sur un cluster Kubernetes (pod, namespace...).\nValue: La value reprÃ©sente une donnÃ©e numÃ©rique recueillie Ã  un moment donnÃ© pour une time series spÃ©cifique. Selon le type de mÃ©trique, il s'agit d'une valeur qui peut Ãªtre mesurÃ©e ou comptÃ©e afin de suivre l'Ã©volution d'un indicateur dans le temps.\nTimestamp: Indique quand la donnÃ©e a Ã©tÃ© collectÃ©e (format epoch Ã  la milliseconde). S'il n'est pas prÃ©sent, Il est ajoutÃ© au moment oÃ¹ la mÃ©trique est rÃ©cupÃ©rÃ©e.\nCette ligne complÃ¨te reprÃ©sente ce que l'on appelle un raw sample.\nAttention Ã  la cardinalitÃ©! Plus il y a de labels, plus les combinaisons possibles augmentent, et par consÃ©quent, le nombre de timeseries. Le nombre total de combinaisons est appelÃ© cardinalitÃ©. Une cardinalitÃ© Ã©levÃ©e peut avoir un impact significatif sur les performances, notamment en termes de consommation de mÃ©moire et de ressources de stockage.\nUne cardinalitÃ© Ã©levÃ©e se produit Ã©galement lorsque de nouvelles mÃ©triques sont crÃ©Ã©es frÃ©quemment. Ce phÃ©nomÃ¨ne, appelÃ© churn rate, indique le rythme auquel des mÃ©triques apparaissent puis disparaissent dans un systÃ¨me. Dans le contexte de Kubernetes, oÃ¹ des pods sont rÃ©guliÃ¨rement crÃ©Ã©s et supprimÃ©s, ce churn rate peut contribuer Ã  l'augmentation rapide de la cardinalitÃ©.\nLa collecte en bref Maintenant que l'on sait ce qu'est une mÃ©trique, voyons comment elles sont collectÃ©es. La plupart des solutions modernes exposent un endpoint qui permet de \u0026quot;scraper\u0026quot; les mÃ©triques, c'est-Ã -dire de les interroger Ã  intervalle rÃ©gulier. Par exemple, grÃ¢ce au SDK Prometheus, disponible dans la plupart des langages de programmation, il est facile d'intÃ©grer cette collecte dans nos applications.\nIl est d'ailleurs important de souligner que Prometheus utilise, en rÃ¨gle gÃ©nÃ©rale, un modÃ¨le de collecte en mode \u0026quot;Pull\u0026quot;, oÃ¹ le serveur interroge pÃ©riodiquement les services pour rÃ©cupÃ©rer les mÃ©triques via ces endpoints exposÃ©s. Cette approche permet de mieux contrÃ´ler la frÃ©quence de collecte des donnÃ©es et d'Ã©viter de surcharger les systÃ¨mes. On distinguera donc le mode \u0026quot;Push\u0026quot; oÃ¹ ce sont les applications qui envoient directement les informations.\nIllustrons cela concrÃ¨tement avec un serveur web Nginx. Ce serveur est installÃ© Ã  partir du chart Helm en activant le support de Prometheus. Ici le paramÃ¨tre metrics.enabled=true permet d'ajouter un chemin qui expose les mÃ©triques.\n1helm install ogenki-nginx bitnami/nginx --set metrics.enabled=true Ainsi, nous pouvons par exemple rÃ©cupÃ©rer via un simple appel http un nombre de mÃ©triques important\n1kubectl port-forward svc/ogenki-nginx metrics \u0026amp; 2Forwarding from 127.0.0.1:9113 -\u0026gt; 9113 3 4curl -s localhost:9113/metrics 5... 6# TYPE promhttp_metric_handler_requests_total counter 7promhttp_metric_handler_requests_total{code=\u0026#34;200\u0026#34;} 257 8... La commande curl Ã©tait juste un exemple, La collecte est, en effet rÃ©alisÃ©e par un systÃ¨me dont la responsabilitÃ© est de stocker ces donnÃ©es pour pouvoir ensuite les exploiter et les analyser. â„¹ï¸ Quand on utilise Prometheus, un composant supplÃ©mentaire est nÃ©cessaire pour pouvoir pousser des mÃ©triques depuis les applications: PushGateway.\nDans cet article, j'ai choisi de vous faire dÃ©couvrir VictoriaMetrics.\nâœ¨ VictoriaMetrics: Un hÃ©ritier de Prometheus Tout comme Prometheus, VictoriaMetrics est une base de donnÃ©es Time Series (TSDB). Celles-cis sont conÃ§ues pour suivre et stocker des Ã©vÃ©nements qui Ã©voluent au fil du temps. MÃªme si VictoriaMetrics est apparue quelques annÃ©es aprÃ¨s Prometheus, elles partagent pas mal de points communs : ce sont toutes deux des bases de donnÃ©es open-source sous licence Apache 2.0, dÃ©diÃ©es au traitement des time series. VictoriaMetrics reste entiÃ¨rement compatible avec Prometheus, en utilisant le mÃªme format de mÃ©triques, OpenMetrics, et un support total du langage de requÃªtes PromQL.\nCes deux projets sont dâ€™ailleurs trÃ¨s actifs, avec des communautÃ©s dynamiques et des contributions rÃ©guliÃ¨res venant de nombreuses entreprises comme on peut le voir ici.\nExplorons maintenant les principales diffÃ©rences et les raisons qui pourraient pousser Ã  choisir VictoriaMetrics :\nStockage et compression efficace : C'est probablement l'un des arguments majeurs, surtout quand on gÃ¨re un volume important de donnÃ©es ou qu'on souhaite les conserver Ã  long terme. Avec Prometheus, il faut ajouter un composant supplÃ©mentaire, comme Thanos, pour cela. VictoriaMetrics, en revanche, dispose d'un moteur de stockage optimisÃ© qui regroupe et optimise les donnÃ©es avant de les Ã©crire sur disque. De plus, il utilise des algorithmes de compression trÃ¨s puissants, offrant une utilisation de l'espace disque bien plus efficace que Prometheus.\nEmpreinte mÃ©moire : VictoriaMetrics consommerait jusqu'Ã  7 fois moins de mÃ©moire qu'une solution basÃ©e sur Prometheus. Cela dit, les benchmarks disponibles en ligne commencent Ã  dater, et Prometheus a bÃ©nÃ©ficiÃ© de nombreuses optimisations de mÃ©moire.\nMetricsQL : VictoriaMetrics Ã©tend le langage PromQL avec de nouvelles fonctions. Ce language est aussi conÃ§u pour Ãªtre plus performant, notamment sur un large dataset.\nArchitecture modulaire: VictoriaMetrics peut Ãªtre dÃ©ployÃ© en 2 modes: \u0026quot;Single\u0026quot; ou \u0026quot;Cluster\u0026quot;. Selon le besoin on pourra aller bien plus loin: On verra cela dans la suite de l'article.\nEt bien d'autres...: Les arguments ci-dessus sont ceux que j'ai retenu mais il y en a d'autres. VictoriaMetrics peut aussi Ãªtre utilisÃ© en mode Push, configurÃ© pour du multitenant et d'autres fonctions que l'on retrouvera dans la version entreprise.\nCase studies: ce qu\u0026#39;ils en disent Sur le site de VictoriaMetrics, on trouve de nombreux tÃ©moignages et retours d'expÃ©rience d'entreprises ayant migrÃ© depuis d'autres systÃ¨mes (comme Thanos, InfluxDB, etc.). Certains exemples sont particuliÃ¨rement instructifs, notamment ceux de Roblox, Razorpay ou Criteo, qui gÃ¨rent un volume trÃ¨s important de mÃ©triques.\nğŸ” Une architecture modulaire et scalable DÃ©ploiement: GitOps et OpÃ©rateurs Kubernetes Le reste de cet article est issu d'un ensemble de configurations que vous pouvez retrouver dans le repository Cloud Native Ref. Il y est fait usage de nombreux opÃ©rateurs et notamment ceux pour VictoriaMetrics et pour Grafana.\nL'ambition de ce projet est de pouvoir dÃ©marrer rapidement une plateforme complÃ¨te qui applique les bonnes pratiques en terme d'automatisation, de supervision, de sÃ©curitÃ© etc. Les commentaires et contributions sont les bienvenues ğŸ™ VictoriaMetrics peut Ãªtre dÃ©ployÃ© de diffÃ©rentes maniÃ¨res: Le mode par dÃ©faut est appelÃ© Single et, comme son nom l'indique, il s'agit de dÃ©ployer une instance unique qui gÃ¨re la lecture, l'Ã©criture et le stockage. Il est d'ailleurs recommandÃ© de commencer par celui-ci car il est optimisÃ© et rÃ©pond Ã  la plupart des cas d'usage comme le prÃ©cise ce paragraphe.\nLe mode Single La mÃ©thode de dÃ©ploiement choisie dans cet article fait usage du chart Helm victoria-metrics-k8s-stack qui configure de nombreuses ressources (VictoriaMetrics, Grafana, Alertmanager, quelques dashboards...). Voici un extrait de configuration Flux pour un mode Single\nobservability/base/victoria-metrics-k8s-stack/helmrelease-vmsingle.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2 2kind: HelmRelease 3metadata: 4 name: victoria-metrics-k8s-stack 5 namespace: observability 6spec: 7 releaseName: victoria-metrics-k8s-stack 8 chart: 9 spec: 10 chart: victoria-metrics-k8s-stack 11 sourceRef: 12 kind: HelmRepository 13 name: victoria-metrics 14 namespace: observability 15 version: \u0026#34;0.25.15\u0026#34; 16... 17 values: 18 vmsingle: 19 spec: 20 retentionPeriod: \u0026#34;1d\u0026#34; # Minimal retention, for tests only 21 replicaCount: 1 22 storage: 23 accessModes: 24 - ReadWriteOnce 25 resources: 26 requests: 27 storage: 10Gi 28 extraArgs: 29 maxLabelsPerTimeseries: \u0026#34;50\u0026#34; Lorsque l'ensemble des manifests Kubernetes sont appliquÃ©s, on obtient l'architecture suivante:\nğŸ”’ AccÃ¨s privÃ©: MÃªme si cela ne fait pas vraiment partie des composants liÃ©s Ã  la collecte des mÃ©triques, j'ai souhaitÃ© mettre en avant la faÃ§on dont on accÃ¨de aux diffÃ©rentes interfaces. J'ai en effet choisi de capitaliser sur Gateway API, que j'utilise depuis quelques temps et qui a fait l'objet de prÃ©cÃ©dents articles. Une alternative serait d'utiliser un composant de VictoriaMetrics, VMAuth, qui peut servir de proxy pour l'autorisation et le routage des accÃ¨s mais Je n'ai pas retenu cette option pour le moment.\nğŸ‘· VMAgent: Un agent trÃ¨s lÃ©ger, dont la fonction principale est de rÃ©cupÃ©rer les mÃ©triques et de les acheminer vers une base de donnÃ©es compatible avec Prometheus. Par ailleurs, cet agent peut appliquer des filtres ou des transformations aux mÃ©triques avant de les transmettre. En cas d'indisponibilitÃ© de la destination ou en cas de manque de ressources, il peut mettre en cache les donnÃ©es sur disque. VMAgent dispose aussi d'une interface Web permettant de lister les \u0026quot;Targets\u0026quot; (Services qui sont scrapÃ©s)\nğŸ”¥ VMAlert \u0026amp; VMAlertManager: Ce sont les composants chargÃ©s de notifier en cas de problÃ¨mes, d'anomalies. Je ne vais volontairement pas approfondir le sujet car cela fera l'objet d'un future acticle.\nâš™ï¸ VMsingle: Il s'agit de la base de donnÃ©es VictoriaMetrics dÃ©ployÃ©e sous forme d'un pod unique qui prend en charge l'ensemble des opÃ©rations (lecture, Ã©criture et persistence des donnÃ©es).\nLorsque tous les pods sont dÃ©marrÃ©s, on peut accÃ©der Ã  l'interface principale de VictoriaMetrics: VMUI. Elle permet de visualiser un grand nombre d'informations: Ã‰videmment nous pourrons parcourir les mÃ©triques scrapÃ©es, les requÃªtes les plus utilisÃ©es, les statistiques relatives Ã  la cardinalitÃ© et bien d'autres.\nYour browser does not support the video tag. La Haute disponibilitÃ© Pour ne jamais perdre de vue ce qui se passe sur nos applications, la solution de supervision doit toujours rester opÃ©rationnelle. Pour cela, tous les composants de VictoriaMetrics peuvent Ãªtre configurÃ©s en haute disponibilitÃ©. En fonction du niveau de redondance souhaitÃ©, plusieurs options s'offrent Ã  nous.\nLa plus simple est d'envoyer les donnÃ©es Ã  deux instances Single, les donnÃ©es sont ainsi dupliquÃ©es Ã  2 endroits. De plus, on peut envisager de dÃ©ployer ces instances dans deux rÃ©gions diffÃ©rentes.\nIl est aussi recommandÃ© de redonder les agents VMAgent qui vont scraper les mÃªmes services, afin de s'assurer qu'aucune donnÃ©e ne soit perdue.\nBien configurer la De-duplication Dans une telle architecture, Ã©tant donnÃ© que plusieurs VMAgents envoient des donnÃ©es et scrappent les mÃªmes services, on se retrouve avec des mÃ©triques en double. La De-duplication dans VictoriaMetrics permet de ne conserver qu'une seule version lorsque deux raw samples sont identiques. Un paramÃ¨tre mÃ©rite une attention particuliÃ¨re : -dedup.minScrapeInterval: Seule la version la plus rÃ©cente sera conservÃ©e lorsque raw samples identiques sont trouvÃ©s dans cet intervale de temps.\nIl est aussi recommandÃ© de :\nConfigurer ce paramÃ¨tre avec une valeur Ã©gale au scrape_interval que l'on dÃ©finit dans la configuration Prometheus. Garder une valeur de scrape_interval identique pour tous les services scrappÃ©s. Le schÃ©ma ci-dessous montre l'une des nombreuses combinaisons possibles pour assurer une disponibilitÃ© optimale. âš ï¸ Cependant, il faut tenir compte du surcoÃ»t, non seulement pour le stockage et le calcul, mais aussi pour les transferts rÃ©seau entre zones/rÃ©gions. Il est parfois plus judicieux d'avoir une bonne stratÃ©gie de sauvegarde et restauration ğŸ˜….\nLe mode Cluster Comme mentionnÃ© plus tÃ´t, dans la plupart des cas, le mode Single est largement suffisant. Il a l'avantage d'Ãªtre simple Ã  maintenir et, avec du scaling vertical, il permet de rÃ©pondre Ã  quasiment tous les cas d'usage. Il existe aussi un mode Cluster, qui n'est pertinent que dans deux cas prÃ©cis :\nBesoin de multitenant. Par exemple pour isoler plusieurs Ã©quipes ou clients. Si les limites du scaling vertical sont atteintes. Ma configuration permet de choisir entre l'un ou l'autre des modes:\nobservability/base/victoria-metrics-k8s-stack/kustomization.yaml\n1resources: 2... 3 4 - vm-common-helm-values-configmap.yaml 5 # Choose between single or cluster helm release 6 7 # VM Single 8 - helmrelease-vmsingle.yaml 9 - httproute-vmsingle.yaml 10 11 # VM Cluster 12 # - helmrelease-vmcluster.yaml 13 # - httproute-vmcluster.yaml Dans ce mode, on va sÃ©parer les fonctions de lecture, Ã©criture et de stockage en 3 services bien distincts.\nâœï¸ VMInsert: RÃ©partit les donnÃ©es sur les instances de VMStorage en utilisant du consistent hashing basÃ© sur la time series (combinaison du nom de la mÃ©trique et de ses labels).\nğŸ’¾ VMStorage: Est chargÃ© d'Ã©crire les donnÃ©es sur disque et de retourner les donnÃ©es demandÃ©es par VMSelect.\nğŸ“– VMSelect: Pour chaque requÃªte va rÃ©cupÃ©rer les donnÃ©es sur les VMStorages.\nL'intÃ©rÃªt principal de ce mode est Ã©videmment de pouvoir adapter le scaling en fonction du besoin. Par exemple, si on a besoin de plus de capacitÃ© en Ã©criture on va ajouter des replicas VMInsert.\nLe paramÃ¨tre initial, qui permet d'avoir un niveau de redondance minimum est replicationFactor Ã  2. Voici un extrait des values Helm pour le mode cluster.\nobservability/base/victoria-metrics-k8s-stack/helmrelease-vmcluster.yaml\n1 vmcluster: 2 enabled: true 3 spec: 4 retentionPeriod: \u0026#34;10d\u0026#34; 5 replicationFactor: 2 6 vmstorage: 7 storage: 8 volumeClaimTemplate: 9 storageClassName: \u0026#34;gp3\u0026#34; 10 spec: 11 resources: 12 requests: 13 storage: 10Gi 14 resources: 15 limits: 16 cpu: \u0026#34;1\u0026#34; 17 memory: 1500Mi 18 affinity: 19 podAntiAffinity: 20 requiredDuringSchedulingIgnoredDuringExecution: 21 - labelSelector: 22 matchExpressions: 23 - key: \u0026#34;app.kubernetes.io/name\u0026#34; 24 operator: In 25 values: 26 - \u0026#34;vmstorage\u0026#34; 27 topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; 28 topologySpreadConstraints: 29 - labelSelector: 30 matchLabels: 31 app.kubernetes.io/name: vmstorage 32 maxSkew: 1 33 topologyKey: topology.kubernetes.io/zone 34 whenUnsatisfiable: ScheduleAnyway 35 vmselect: 36 storage: 37 volumeClaimTemplate: 38 storageClassName: \u0026#34;gp3\u0026#34; \u0026#x2139;\u0026#xfe0f; On notera que certains paramÃ¨tres font partie des bonnes pratiques Kubernetes, notamment lorsque l'on utilise Karpenter: topologySpreadConstraints permet de rÃ©partir sur diffÃ©rentes zones, podAntiAffinity pour Ã©viter que 2 pods pour le mÃªme service se retrouvent sur le mÃªme noeud.\nğŸ› ï¸ La configuration Ok, c'est cool, VictoriaMetrics est maintenant dÃ©ployÃ© ğŸ‘. Il est temps de configurer la supervision de nos applications, et pour Ã§a, on va s'appuyer sur le pattern opÃ©rateur de Kubernetes. ConcrÃ¨tement, cela signifie que l'on va dÃ©clarer des ressources personnalisÃ©es (Custom Resources) qui seront interprÃ©tÃ©es par VictoriaMetrics Operator pour configurer et gÃ©rer VictoriaMetrics.\nLe Helm chart quâ€™on a utilisÃ© ne dÃ©ploie pas directement VictoriaMetrics, mais il installe principalement lâ€™opÃ©rateur. Cet opÃ©rateur se charge ensuite de crÃ©er et de gÃ©rer des custom resources comme VMSingle ou VMCluster, qui dÃ©terminent comment VictoriaMetrics est dÃ©ployÃ© et configurÃ© en fonction des besoins.\nLe rÃ´le de VMServiceScrape est de dÃ©finir oÃ¹ aller chercher les mÃ©triques pour un service donnÃ©. On sâ€™appuie sur les labels Kubernetes pour identifier le bon service et le bon port.\nobservability/base/victoria-metrics-k8s-stack/vmservicecrapes/karpenter.yaml\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMServiceScrape 3metadata: 4 name: karpenter 5 namespace: karpenter 6spec: 7 selector: 8 matchLabels: 9 app.kubernetes.io/name: karpenter 10 endpoints: 11 - port: http-metrics 12 path: /metrics 13 namespaceSelector: 14 matchNames: 15 - karpenter Nous pouvons vÃ©rifier que les paramÃ¨tres sont bien configurÃ©s grÃ¢ce Ã  kubectl\n1kubectl get services -n karpenter --selector app.kubernetes.io/name=karpenter -o yaml | grep -A 4 ports 2 ports: 3 - name: http-metrics 4 port: 8000 5 protocol: TCP 6 targetPort: http-metrics Parfois il n'y pas de service, nous pouvons alors indiquer comment identifier les pods directement avec VMPodScrape.\nobservability/base/flux-config/observability/vmpodscrape.yaml\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMPodScrape 3metadata: 4 name: flux-system 5 namespace: flux-system 6spec: 7 namespaceSelector: 8 matchNames: 9 - flux-system 10 selector: 11 matchExpressions: 12 - key: app 13 operator: In 14 values: 15 - helm-controller 16 - source-controller 17 - kustomize-controller 18 - notification-controller 19 - image-automation-controller 20 - image-reflector-controller 21 podMetricsEndpoints: 22 - targetPort: http-prom Toutes nos applications ne sont pas forcÃ©ment dÃ©ployÃ©es sur Kubernetes. La ressource VMScrapeConfig dans VictoriaMetrics permet d'utiliser plusieurs mÃ©thodes de \u0026quot;Service Discovery\u0026quot;. Cette ressource offre la flexibilitÃ© de dÃ©finir comment scrapper les cibles via diffÃ©rents mÃ©canismes de dÃ©couverte, tels que les instances EC2 (AWS), les services Cloud ou d'autres systÃ¨mes. Dans l'exemple ci-dessous, on utilise le tag personnalisÃ© observability:node-exporter, et on applique des transformations de labels. Ce qui nous permet de rÃ©cupÃ©rer les mÃ©triques exposÃ©es par les node-exporters installÃ©s sur ces instances.\nobservability/base/victoria-metrics-k8s-stack/vmscrapeconfigs/ec2.yaml\n1apiVersion: operator.victoriametrics.com/v1beta1 2kind: VMScrapeConfig 3metadata: 4 name: aws-ec2-node-exporter 5 namespace: observability 6spec: 7 ec2SDConfigs: 8 - region: ${region} 9 port: 9100 10 filters: 11 - name: tag:observability:node-exporter 12 values: [\u0026#34;true\u0026#34;] 13 relabelConfigs: 14 - action: replace 15 source_labels: [__meta_ec2_tag_Name] 16 target_label: ec2_name 17 - action: replace 18 source_labels: [__meta_ec2_tag_app] 19 target_label: ec2_application 20 - action: replace 21 source_labels: [__meta_ec2_availability_zone] 22 target_label: ec2_az 23 - action: replace 24 source_labels: [__meta_ec2_instance_id] 25 target_label: ec2_id 26 - action: replace 27 source_labels: [__meta_ec2_region] 28 target_label: ec2_region â„¹ï¸ Si on utilisait dÃ©jÃ  le Prometheus Operator, la migration vers VictoriaMetrics est trÃ¨s simple car il est compatible avec les CRDs dÃ©finies par le Prometheus Operator.\nğŸ“ˆ Visualiser nos mÃ©triques avec l'opÃ©rateur Grafana Il est facile de deviner Ã  quoi sert le Grafana Operator: Utiliser des ressources Kubernetes pour configurer Grafana ğŸ˜. Il permet de dÃ©ployer des instances Grafana, d'ajouter des datasources, d'importer des dashboards de diffÃ©rentes Ã  partir de diffÃ©rentes sources (URL, JSON), de les classer dans des rÃ©pertoires etc... Il s'agit d'une alternative au fait de tout dÃ©finir dans le chart Helm ou d'utiliser des configmaps et, selon moi, offre une meilleure lecture. Dans cet exemple, je regroupe l'ensemble des ressources relatives Ã  la supervision de Cilium\n1tree infrastructure/base/cilium/ 2infrastructure/base/cilium/ 3â”œâ”€â”€ grafana-dashboards.yaml 4â”œâ”€â”€ grafana-folder.yaml 5â”œâ”€â”€ httproute-hubble-ui.yaml 6â”œâ”€â”€ kustomization.yaml 7â”œâ”€â”€ vmrules.yaml 8â””â”€â”€ vmservicescrapes.yaml La dÃ©finition du rÃ©pertoire est super simple\nobservability/base/infrastructure/cilium/grafana-folder.yaml\n1apiVersion: grafana.integreatly.org/v1beta1 2kind: GrafanaFolder 3metadata: 4 name: cilium 5spec: 6 allowCrossNamespaceImport: true 7 instanceSelector: 8 matchLabels: 9 dashboards: \u0026#34;grafana\u0026#34; Puis voici une ressource Dashboard qui va chercher la configuration Ã  partir d'un lien HTTP. Nous pouvons aussi utiliser les dashboards disponibles depuis le site de Grafana, en indiquant l'ID appropriÃ© ou carrÃ©ment mettre la dÃ©finition au format JSON.\nobservability/base/infrastructure/cilium/grafana-dashboards.yaml\n1apiVersion: grafana.integreatly.org/v1beta1 2kind: GrafanaDashboard 3metadata: 4 name: cilium-cilium 5spec: 6 folderRef: \u0026#34;cilium\u0026#34; 7 allowCrossNamespaceImport: true 8 datasources: 9 - inputName: \u0026#34;DS_PROMETHEUS\u0026#34; 10 datasourceName: \u0026#34;VictoriaMetrics\u0026#34; 11 instanceSelector: 12 matchLabels: 13 dashboards: \u0026#34;grafana\u0026#34; 14 url: \u0026#34;https://raw.githubusercontent.com/cilium/cilium/main/install/kubernetes/cilium/files/cilium-agent/dashboards/cilium-dashboard.json\u0026#34; Notez que j'ai choisi de ne pas utiliser l'opÃ©rateur Grafana pour dÃ©ployer l'instance, mais de garder celle qui a Ã©tÃ© installÃ©e via le Helm chart de VictoriaMetrics. Il faut donc simplement fournir Ã  l'opÃ©rateur Grafana les paramÃ¨tres d'authentification pour qu'il puisse appliquer les modifications sur cette instance.\nobservability/base/grafana-operator/grafana-victoriametrics.yaml\n1apiVersion: grafana.integreatly.org/v1beta1 2kind: Grafana 3metadata: 4 name: grafana-victoriametrics 5 labels: 6 dashboards: \u0026#34;grafana\u0026#34; 7spec: 8 external: 9 url: http://victoria-metrics-k8s-stack-grafana 10 adminPassword: 11 name: victoria-metrics-k8s-stack-grafana-admin 12 key: admin-password 13 adminUser: 14 name: victoria-metrics-k8s-stack-grafana-admin 15 key: admin-user Enfin nous pouvons utiliser Grafana et explorer nos diffÃ©rents dashboards ğŸ‰!\nYour browser does not support the video tag. ğŸ’­ DerniÃ¨res remarques Si l'on se rÃ©fÃ¨re aux diffÃ©rents articles consultÃ©s, l'une des principales raisons pour lesquelles migrer ou choisir VictoriaMetrics serait une meilleure performance en rÃ¨gle gÃ©nÃ©rale. Cependant il est judicieux de rester prudent car les rÃ©sultats des benchmarks dÃ©pendent de plusieurs facteurs, ainsi que de l'objectif recherchÃ©. C'est pourquoi il est fortement conseillÃ© de lancer des tests soit mÃªme. VictoriaMetrics propose un jeu de test qui peut Ãªtre rÃ©alisÃ© sur les TSDB compatibles avec Prometheus.\nVous l'aurez compris, aujourd'hui mon choix se porte sur VictoriaMetrics pour la collecte des mÃ©triques, car j'apprÃ©cie l'architecture modulaire avec une multitude de combinaisons possibles en fonction de l'Ã©volution du besoin. Cependant, une solution utilisant l'opÃ©rateur Prometheus fonctionne trÃ¨s bien dans la plupart des cas et a l'intÃ©rÃªt d'Ãªtre gouvernÃ© par une fondation.\nPar aileurs, il est important de noter que certaines fonctionnalitÃ©s ne sont disponibles qu'en version Entreprise, notamment le downsampling qui est fort utile lorsque l'on veut garder une grosse quantitÃ© de donnÃ©es sur du long terme.\nDans cet article nous avons surtout pu mettre en Ã©vidence la facilitÃ© de mise en oeuvre pour obtenir une solution qui permette, d'une part de collecter efficacement les mÃ©triques, et de les visualiser. Ceci, toujours en utilisant le pattern operateur Kubernetes qui permet de faire du GitOps, et de dÃ©clarer les diffÃ©rents types de ressources au travers de Custom resources. Ainsi Un dÃ©veloppeur peut trÃ¨s bien inclure Ã  ses manifests, un VMServiceScrape et une VMRule et, ainsi, inclure la culture de l'observabilitÃ© dans les processes de livraisons applicative.\nDisposer de mÃ©triques c'est bien bien, mais est-ce suffisant? On va essayer d'y rÃ©pondre dans les prochains articles ...\nğŸ”– References Articles sur VictoriaMetrics ","link":"https://blog.ogenki.io/fr/post/series/observability/metrics/","section":"post","tags":["observability"],"title":"Une solution complÃ¨te et performante pour gÃ©rer vos mÃ©triques avec les opÃ©rateurs `VictoriaMetrics` et `Grafana`!"},{"body":"Dagger est un projet open source qui promet de rÃ©volutionner la faÃ§on de dÃ©finir les pipelines d'intÃ©gration continue (CI). Il a Ã©tÃ© crÃ©Ã© par les fondateurs de Docker qui se sont basÃ©s sur une Ã©tude des difficultÃ©s courantes dans les entreprises. Il ont alors identifiÃ© un manque d'outillage efficaces le long du cycle de dÃ©veloppement jusqu'au passage en production.\nIl y a notamment un manque d'homogÃ©nÃ©itÃ© entre les environnements d'exÃ©cution, vous avez probablement dÃ©jÃ  entendu votre voisin(e) se plaindre avec un truc du genre: \u0026quot;Erf, Ã§a marchait bien sur ma machine! C'est quoi cette erreur sur la CI?\u0026quot; ğŸ˜†\nOffrant une mÃ©thode commune et centralisÃ©e, Dagger serait LA rÃ©ponse Ã  cette problÃ©matique et permettrait, par ailleurs, d'amÃ©liorer l'expÃ©rience dÃ©veloppeur locale, la collaboration et ainsi d'accÃ©lerer le cycle de dÃ©veloppement.\nBeaucoup d'entre nous ont dÃ©jÃ  utilisÃ© des scripts bash, des Makefiles et d'autres mÃ©thodes traditionnelles pour automatiser certaines actions. Cependant, ces solutions peuvent vite devenir complexes et difficiles Ã  maintenir. Dagger propose une alternative moderne et simplifiÃ©e, permettant de standardiser et d'uniformiser nos pipelines, peu importe l'environnement.\nMais alors, quelles sont les principales fonctionnalitÃ©s de Dagger, et comment l'utiliser efficacement?\nğŸ¯ Notre objectif Voici les points que nous allons aborder dans cet article :\nTout d'abord, nous allons comprendre le fonctionnement de Dagger et faire nos premiers pas.\nEnsuite, nous prendrons des cas concrets pour sa mise en Å“uvre. Nous verrons comment transformer un projet existant, et je vous prÃ©senterai Ã©galement un module que j'utilise dÃ©sormais quotidiennement.\nEnfin, nous dÃ©crirons une solution de mise en cache efficace, qui nous permettra de nous projeter dans la mise Ã  l'Ã©chelle avec Dagger.\nğŸ” La dÃ©couverte En gros, Dagger est un outil qui nous permet de dÃ©finir des tÃ¢ches dans notre langage prÃ©fÃ©rÃ© et de rendre ce code portable. Autrement dit, ce que j'exÃ©cute sur ma machine sera exÃ©cutÃ© de la mÃªme maniÃ¨re sur la CI ou sur l'ordinateur de mon/ma collÃ¨gue.\nIl y a 2 composants principaux qui entrent en jeu\nLa CLI Dagger: Notre point d'accÃ¨s principal pour interagir avec les diffÃ©rentes fonctions et modules, les tÃ©lÃ©charger et afficher le rÃ©sultat de leur exÃ©cution. Le moteur Dagger: Toutes les opÃ©rations effectuÃ©es avec la CLI passent par une API GraphQL exposÃ©e par un moteur Dagger. Chaque client initie sa propre session avec l'API Core qui dispose des fonctionnalitÃ©s de base. Elles peuvent ensuite Ãªtre Ã©tendues grÃ¢ce Ã  des modules. CommenÃ§ons par installer la CLI. Si vous avez parcouru mes prÃ©cÃ©dents articles, vous savez que j'affectionne particuliÃ¨rement asdf\n1asdf plugin-add dagger 2 3asdf install dagger 0.12.1 4Downloading dagger from https://github.com/dagger/dagger/releases/download/v0.12.1/dagger_v0.12.1_linux_amd64.tar.gz 5 6asdf global dagger 0.12.1 7dagger version 8dagger v0.12.1 (registry.dagger.io/engine) linux/amd64 Entrons dans le vif du sujet, nous pouvons tout de suite exÃ©cuter un module fournie par la communautÃ©. Supposons que l'on veuille scanner un repo git et une image Docker avec trivy.\nLe Daggerverse Le Daggerverse est une plateforme permettant a quiconque de partager des modules. Lorsque vous avez un besoin, il est conseillÃ© de regarder ce qui est dÃ©jÃ  proposÃ© par d'autres. Faites le test en recherchant par example golangci, black, gptscript, wolfi...\nNous pouvons consulter les fonctions disponibles dans le module en utilisant l'argument functions\n1TRIVY_MODULE=\u0026#34;github.com/purpleclay/daggerverse/trivy@c3f44e0c8a396b2adf024bb862714037ae4cc8e7\u0026#34; 2 3dagger functions -m ${TRIVY_MODULE} 4Name Description 5filesystem Scan a filesystem for any vulnerabilities 6image Scan a published (or remote) image for any vulnerabilities 7image-local Scan a locally exported image for any vulnerabilities Les functions peuvent aussi prendre divers paramÃ¨tres\n1dagger call -m ${TRIVY_MODULE} filesystem --help 2... 3ARGUMENTS 4 --dir Directory the path to directory to scan [required] 5 --exit-code int the returned exit code when vulnerabilities are detected (0) 6 --format string the type of format to use when generating the compliance report (table) 7 --ignore-unfixed filter out any vulnerabilities without a known fix 8 --scanners string the types of scanner to execute (vuln,secret) 9 --severity string the severity of security issues to detect (UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL) 10 --template string a custom go template to use when generating the compliance report 11 --vuln-type string the types of vulnerabilities to scan for (os,library) Analysons donc le niveau de sÃ©curitÃ© de mon repository local ğŸ•µï¸\n1dagger call -m ${TRIVY_MODULE} filesystem --dir \u0026#34;.\u0026#34; 2 3scan/go.mod (gomod) 4=================== 5Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0) 6 7â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 8â”‚ Library â”‚ Vulnerability â”‚ Severity â”‚ Status â”‚ Installed Version â”‚ Fixed Version â”‚ Title â”‚ 9â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ 10â”‚ github.com/vektah/gqlparser/v2 â”‚ CVE-2023-49559 â”‚ MEDIUM â”‚ fixed â”‚ 2.5.11 â”‚ 2.5.14 â”‚ gqlparser denial of service vulnerability via the â”‚ 11â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ parserDirectives function â”‚ 12â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ https://avd.aquasec.com/nvd/cve-2023-49559 â”‚ 13â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Oups! il semble qu'il y ait une vulnÃ©rabilitÃ© critique dans mon image ğŸ˜¨.\n1dagger call -m ${TRIVY_MODULE} image --ref smana/dagger-cli:v0.12.1 --severity CRITICAL 2 3smana/dagger-cli:v0.12.1 (ubuntu 23.04) 4======================================= 5Total: 0 (CRITICAL: 0) 6 7 8usr/local/bin/dagger (gobinary) 9=============================== 10Total: 1 (CRITICAL: 1) 11 12â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 13â”‚ Library â”‚ Vulnerability â”‚ Severity â”‚ Status â”‚ Installed Version â”‚ Fixed Version â”‚ Title â”‚ 14â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ 15â”‚ stdlib â”‚ CVE-2024-24790 â”‚ CRITICAL â”‚ fixed â”‚ 1.22.3 â”‚ 1.21.11, 1.22.4 â”‚ golang: net/netip: Unexpected behavior from Is methods for â”‚ 16â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ IPv4-mapped IPv6 addresses â”‚ 17â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ https://avd.aquasec.com/nvd/cve-2024-24790 â”‚ 18â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ C'est dÃ©jÃ  super cool de pouvoir bÃ©nÃ©ficier de nombreuses sources ğŸ¤© ! Ces modules peuvent donc Ãªtre utilisÃ©s directement ou devenir une source d'inspiration prÃ©cieuse pour nos futurs pipelines.\nUn module est une collection de fonctions qui prend des paramÃ¨tres en entrÃ©e et nous renvoie une rÃ©ponse sous diffÃ©rentes formes : du texte en sortie, l'exÃ©cution d'un terminal, le lancement d'un service, etc. Notons aussi que toutes les fonctions sont exÃ©cutÃ©es dans des conteneurs.\nAprÃ¨s cette courte intro, passons Ã  des cas d'usage rÃ©els en commenÃ§ant par ajouter des tÃ¢ches/fonctions Ã  un projet existant.\nğŸ¦‹ Daggeriser une application existante Prenons un projet de dÃ©monstration existant, un simple serveur web avec une fonction de stockage de mots dans une base de donnÃ©es. Nous allons progressivement transformer ce projet en y injectant du Dagger ğŸ’‰. Cette approche itÃ©rative, Ã©tape par Ã©tape, peut Ã©galement Ãªtre appliquÃ©e Ã  des projets plus importants pour les Daggeriser progressivement. PremiÃ¨re fonction ğŸ‘¶ Notre prioritÃ© va Ãªtre de tester le code en utilisant la commande go test.\nCommenÃ§ons donc par initialiser le repo git afin de gÃ©nÃ©rer l'arborescence requise pour l'exÃ©cution des fonctions Dagger.\n1git clone https://github.com/Smana/golang-helloworld.git 2cd golang-helloworld 3dagger init --sdk=go 1ls -l dagger* 2.rw-r--r-- 101 smana 28 Jun 21:54 dagger.json 3 4dagger: 5.rw------- 25k smana 28 Jun 21:54 dagger.gen.go 6drwxr-xr-x - smana 28 Jun 21:54 internal 7.rw------- 1.4k smana 28 Jun 21:54 main.go La commande d'initialisation gÃ©nÃ¨re donc un fichier main.go qui contient des fonctions d'exemple que nous allons totalement remplacer par le code suivant:\n1package main 2 3import ( 4\t\u0026#34;context\u0026#34; 5) 6 7type GolangHelloworld struct{} 8 9// Test runs the tests for the GolangHelloworld project 10func (m *GolangHelloworld) Test(ctx context.Context, source *Directory) (string, error) { 11\tctr := dag.Container().From(\u0026#34;golang:1.22\u0026#34;) 12\treturn ctr. 13\tWithWorkdir(\u0026#34;/src\u0026#34;). 14\tWithMountedDirectory(\u0026#34;/src\u0026#34;, source). 15\tWithExec([]string{\u0026#34;go\u0026#34;, \u0026#34;test\u0026#34;, \u0026#34;./...\u0026#34;}). 16\tStdout(ctx) 17} Il s'agit lÃ  d'une fonction trÃ¨s simple:\nCette fonction, appelÃ©e Test, prend en paramÃ¨tre un rÃ©pertoire source. Nous utilisons une image golang:1.22. Le code du rÃ©pertoire donnÃ© en paramÃ¨tre est montÃ© dans le dossier /src du conteneur. Ensuite, nous exÃ©cutons la commande go test ./... sur le rÃ©pertoire source. Enfin, nous rÃ©cupÃ©rons le rÃ©sultat des tests (stdout). Mise Ã  jour de l\u0026#39;environnement de dev Il est rÃ©guliÃ¨rement nÃ©cessaire de lancer la commande suivante afin de mettre Ã  jour les fichiers Dagger (dÃ©pendances etc...)\n1dagger develop C'est parti, testons notre code!\n1dagger call test --source \u0026#34;.\u0026#34; 2? helloworld/cmd/helloworld [no test files] 3? helloworld/dagger [no test files] 4? helloworld/dagger/internal/dagger [no test files] 5? helloworld/dagger/internal/querybuilder [no test files] 6? helloworld/dagger/internal/telemetry [no test files] 7ok helloworld/internal/server 0.004s \u0026#x2139;\u0026#xfe0f; La premiÃ¨re exÃ©cution prend du temps car elle construit tÃ©lÃ©charge l'image et installe les dÃ©pendances Go, mais les exÃ©cutions suivantes sont beaucoup plus rapides. Nous aborderons le sujet de la mise en cache plus tard dans cet article.\nEt mon docker-compose alors? ğŸ³ Le projet initial permet de lancer un environnement de test local en utilisant Docker Compose\nLa commande docker-compose up --build effectue plusieurs actions : elle construit l'image Docker de l'application en se basant sur le Dockerfile local, puis lance deux conteneurs : un pour l'application et un pour la base de donnÃ©es. Elle permet Ã©galement la communication entre ces deux conteneurs.\n1docker ps 2CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3a1673d56f9c8 golang-helloworld-app \u0026#34;/app/main\u0026#34; 3 seconds ago Up 3 seconds 0.0.0.0:8080-\u0026gt;8080/tcp, :::8080-\u0026gt;8080/tcp golang-helloworld-app-1 4bb3dee1305dc postgres:16 \u0026#34;docker-entrypoint.sâ€¦\u0026#34; 3 seconds ago Up 3 seconds 0.0.0.0:5432-\u0026gt;5432/tcp, :::5432-\u0026gt;5432/tcp golang-helloworld-database-1 Il est ensuite possible d'accÃ©der Ã  l'application et de stocker des mots dans la base de donnÃ©es.\n1curl -X POST -d \u0026#39;{\u0026#34;word\u0026#34;:\u0026#34;foobar\u0026#34;}\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; http://localhost:8080/store 2 3curl http://localhost:8080/list 4[\u0026#34;foobar\u0026#34;] Comment rÃ©aliser la mÃªme chose avec Dagger?\nTout d'abord nous allons construire l'image:\n1// Build the Docker container 2func (m *GolangHelloworld) Build(ctx context.Context, source *Directory) *Container { 3\t// build the binary 4\tbuilder := dag.Container(). 5\tFrom(golangImage). 6\tWithDirectory(\u0026#34;/src\u0026#34;, source). 7\tWithWorkdir(\u0026#34;/src\u0026#34;). 8\tWithEnvVariable(\u0026#34;CGO_ENABLED\u0026#34;, \u0026#34;0\u0026#34;). 9\tWithExec([]string{\u0026#34;go\u0026#34;, \u0026#34;build\u0026#34;, \u0026#34;-o\u0026#34;, \u0026#34;helloworld\u0026#34;, \u0026#34;cmd/helloworld/main.go\u0026#34;}) 10 11\t// Create the target image with the binary 12\ttargetImage := dag.Container(). 13\tFrom(alpineImage). 14\tWithFile(\u0026#34;/bin/helloworld\u0026#34;, builder.File(\u0026#34;/src/helloworld\u0026#34;), ContainerWithFileOpts{Permissions: 0700, Owner: \u0026#34;nobody\u0026#34;}). 15\tWithUser(\u0026#34;nobody:nobody\u0026#34;). 16\tWithEntrypoint([]string{\u0026#34;/bin/helloworld\u0026#34;}) 17 18\treturn targetImage 19} Ce code dÃ©montre l'utilisation du \u0026quot;multi-stage build\u0026quot; pour optimiser la sÃ©curitÃ© et la taille de l'image. Cette mÃ©thode permet de n'inclure que ce qui est nÃ©cessaire dans l'image finale, rÃ©duisant ainsi la surface d'attaque et la taille de l'image.\nEnsuite nous avons besoin d'une instance PostgreSQL. Ã‡a tombe bien il y a un module pour Ã§a Â®!\nNous allons donc installer cette dÃ©pendance pour pouvoir utiliser ses fonctions directement dans notre code.\n1dagger install github.com/quartz-technology/daggerverse/postgres@v0.0.3 La fonction Database() permet de lancer un conteneur Postgres.\n1... 2\topts := PostgresOpts{ 3\tDbName: dbName, 4\tCache: cache, 5\tVersion: \u0026#34;13\u0026#34;, 6\tConfigFile: nil, 7\tInitScript: initScriptDir, 8\t} 9 10... 11\tpgCtr := dag.Postgres(pgUser, pgPass, pgPortInt, opts).Database() Ensuite, nous devons crÃ©er un lien entre les deux conteneurs. Ci-dessous, nous rÃ©cupÃ©rons les informations du service exposÃ© par le conteneur Postgres pour les utiliser dans notre application.\n1... 2\tpgSvc := pgCtr.AsService() 3 4\tpgHostname, err := pgSvc.Hostname(ctx) 5\tif err != nil { 6\treturn nil, fmt.Errorf(\u0026#34;could not get postgres hostname: %w\u0026#34;, err) 7\t} 8 9\treturn ctr. 10\tWithSecretVariable(\u0026#34;PGPASSWORD\u0026#34;, pgPass). 11\tWithSecretVariable(\u0026#34;PGUSER\u0026#34;, pgUser). 12\tWithEnvVariable(\u0026#34;PGHOST\u0026#34;, pgHostname). 13\tWithEnvVariable(\u0026#34;PGDATABASE\u0026#34;, opts.DbName). 14\tWithEnvVariable(\u0026#34;PGPORT\u0026#34;, pgPort). 15\tWithServiceBinding(\u0026#34;database\u0026#34;, pgSvc). 16\tWithExposedPort(8080), nil 17... Les secrets ğŸ”’ Les informations sensibles peuvent Ãªtre passÃ©es lors de l'appel aux fonctions Dagger de plusieurs faÃ§on: Des variables d'environnement, lecture du contenu de fichiers ou la sortie d'une ligne de commande. Dans cet article nous avons privilÃ©giÃ© les variables d'environnement mais nous aurions trÃ¨s bien pu utiliser une commande vault. (Article prÃ©cÃ©dent sur Vault)\nup permet de transfÃ©rer les appels locaux aux services exposÃ©s par le conteneur.\n1export PGUSER=\u0026#34;user\u0026#34; 2export PGPASS=\u0026#34;password\u0026#34; 3dagger call serve --pg-user=env:PGUSER --pg-pass=env:PGPASS --source \u0026#34;.\u0026#34; as-service up 4 5... 6 â— start /bin/helloworld 30.7s 7 â”ƒ 2024/06/30 08:27:50 Starting server on :8080 8 â”ƒ 2024/06/30 08:27:50 Starting server on :8080 Et voilÃ ! nous pouvons dÃ©sormais tester notre application en local.\nD\u0026#39;autres fonctions J'ai volontairement tronquÃ© ces derniers extraits, mais je vous invite Ã  consulter la configuration complÃ¨te ici. Vous y trouverez notamment la possibilitÃ© de publier l'image dans un registry.\nDe plus, je vous conseille de parcourir le Cookbook dans la documentation Dagger, oÃ¹ vous trouverez de nombreux exemples.\nğŸ§© Le module Kubeconform Je suis parti d'un rÃ©el cas d'usage: J'utilise depuis quelques annÃ©es un script bash pour valider les manifests Kubernetes/Kustomize ainsi que la configuration Flux. L'idÃ©e est donc de rÃ©pondre Ã  ce mÃªme besoin mais aussi d'aller un peu plus loin...\nL'initialisation d'un module se fait de la faÃ§on suivante:\n1dagger init --name=kubeconform --sdk=go kubeconform Il faut ensuite dÃ©cider des paramÃ¨tres d'entrÃ©e. Par exemple je souhaite pouvoir choisir la version du binaire Kubeconform.\n1... 2\t// Kubeconform version to use for validation. 3\t// +optional 4\t// +default=\u0026#34;v0.6.6\u0026#34; 5\tversion string, 6... Les commentaires ci-dessus sont importants: La description sera affichÃ©e Ã  l'utilisateur et nous pouvons faire en sorte que ce paramÃ¨tre ne soit pas requis avec une version par dÃ©fault.\n1dagger call -m github.com/Smana/daggerverse/kubeconform@v0.1.0 validate --help 2Validate the Kubernetes manifests in the provided directory and optional source CRDs directories 3... 4 --version string Kubeconform version to use for validation. (default \u0026#34;v0.6.6\u0026#34;) L'objectif est de pouvoir partager ce module, donc tous les Ã©lÃ©ments de contexte doivent Ãªtre clairs et comprÃ©hensibles.\nEn dÃ©veloppant ce module, j'ai suis passÃ© par plusieurs itÃ©rations et j'ai obtenu des infos trÃ¨s utiles sur le Discord de Dagger. C'est un super moyen d'Ã©changer avec la communautÃ©.\nAnalysons par exemple ceci:\n1kubeconformBin := dag.Arc(). 2 Unarchive(dag.HTTP(fmt.Sprintf(\u0026#34;https://github.com/yannh/kubeconform/releases/download/%s/kubeconform-linux-amd64.tar.gz\u0026#34;, kubeconform_version)). 3 WithName(\u0026#34;kubeconform-linux-amd64.tar.gz\u0026#34;)).File(\u0026#34;kubeconform-linux-amd64/kubeconform\u0026#34;) J'utilise le module Arc pour dÃ©compresser un fichier rÃ©cupÃ©rÃ© avec la fonction HTTP et je ne prends que le binaire inclus dans cette archive. PlutÃ´t efficace !\nDans cet autre exemple j'utilise le module Apko pour construire l'image initial, y installer des packages...\n1ctr := dag.Apko().Wolfi([]string{\u0026#34;bash\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;kustomize\u0026#34;, \u0026#34;git\u0026#34;, \u0026#34;python3\u0026#34;, \u0026#34;py3-pip\u0026#34;, \u0026#34;yq\u0026#34;}). 2 WithExec([]string{\u0026#34;pip\u0026#34;, \u0026#34;install\u0026#34;, \u0026#34;pyyaml\u0026#34;}) Au moment oÃ¹ j'Ã©cris cet article, le module Kubeconform inclut aussi un bout de script bash, essentiellement pour parcourir l'arborescence efficacement et exÃ©cuter kubeconform\n1\tscriptContent := `#!/bin/bash 2... 3` 4 5\t// Add the manifests and the script to the container 6\tctr = ctr. 7\tWithMountedDirectory(\u0026#34;/work\u0026#34;, manifests). 8\tWithNewFile(\u0026#34;/work/run_kubeconform.sh\u0026#34;, ContainerWithNewFileOpts{ 9\tPermissions: 0750, 10\tContents: scriptContent, 11\t}) 12 13\t// Execute the script 14\tkubeconform_command := []string{\u0026#34;bash\u0026#34;, \u0026#34;/work/run_kubeconform.sh\u0026#34;} 15... Pour tester et corriger le module nous pouvons l'exÃ©cuter localement sur un repo qui contient des manifests Kubernetes.\n1dagger call validate --manifests ~/Sources/demo-cloud-native-ref/clusters --catalog 2... 3Summary: 1 resource found in 1 file - Valid: 1, Invalid: 0, Errors: 0, Skipped: 0 4Validation successful for ./mycluster-0/crds.yaml 5Processing file: ./mycluster-0/flux-config.yaml 6Summary: 1 resource found in 1 file - Valid: 1, Invalid: 0, Errors: 0, Skipped: 0 7Validation successful for ./mycluster-0/flux-config.yaml Nous pouvons aussi augmenter le niveau de verbositÃ©. Le niveau le plus Ã©levÃ© Ã©tant -vvv --debug\n1dagger call validate --manifests ~/Sources/demo-cloud-native-ref/clusters --catalog -vvv --debug 2... 309:32:07 DBG new end old=\u0026#34;2024-07-06 09:32:07.436103097 +0200 CEST\u0026#34; new=\u0026#34;2024-07-06 09:32:07.436103273 +0200 CEST\u0026#34; 409:32:07 DBG recording span span=telemetry.LogsSource/Subscribe id=b3fc48ec7900f581 509:32:07 DBG recording span child span=telemetry.LogsSource/Subscribe parent=ae535768bb2be9d7 child=b3fc48ec7900f581 609:32:07 DBG new end old=\u0026#34;2024-07-06 09:32:07.436103273 +0200 CEST\u0026#34; new=\u0026#34;2024-07-06 09:32:07.438699251 +0200 CEST\u0026#34; 709:32:07 DBG recording span span=\u0026#34;/home/smana/.asdf/installs/dagger/0.12.1/bin/dagger call -m github.com/Smana/daggerverse/kubeconform@v0.1.0 validate --manifests /home/smana/Sources/demo-cloud-native-ref/clusters --catalog -vvv --debug\u0026#34; id=ae535768bb2be9d7 809:32:07 DBG frontend exporting logs logs=4 909:32:07 DBG exporting log span=0xf62760 body=\u0026#34;\u0026#34; 1009:32:07 DBG got EOF 1109:32:07 DBG run finished err=\u0026lt;nil\u0026gt; 12 13âœ” 609fcdee60c94c07 connect 0.6s 14 âœ” c873c2d69d2b7ce7 starting engine 0.5s 15 âœ” 5f48c41bd0a948ca create 0.5s 16 âœ” dbd62c92c3db105f exec docker start dagger-engine-ceb38152f96f1298 0.0s 17 â”ƒ dagger-engine-ceb38152f96f1298 18 âœ” 4db8303f1d7ec940 connecting to engine 0.1s 19 â”ƒ 09:32:03 DBG connecting runner=docker-image://registry.dagger.io/engine:v0.12.1 client=5fa0kn1nc4qlku1erer3868nj 20 â”ƒ 09:32:03 DBG subscribing to telemetry remote=docker-image://registry.dagger.io/engine:v0.12.1 21 â”ƒ 09:32:03 DBG subscribed to telemetry elapsed=19.095Âµs Ã€ partir de la version v0.12.x, Dagger propose un mode interactif. En utilisant le paramÃ¨tre -i ou --interactive, il est possible de lancer automatiquement un terminal lorsque le code rencontre une erreur. Cela permet de rÃ©aliser des vÃ©rifications et des opÃ©rations directement dans le conteneur.\nDe plus, il est possible d'ajouter l'exÃ©cution de Terminal() Ã  n'importe quel endroit dans la dÃ©finition du conteneur pour entrer en mode interactif Ã  ce moment prÃ©cis.\n1... 2\tstdout, err := ctr.WithExec(kubeconform_command). 3\tTerminal(). 4\tStdout(ctx) 5... Avec ce module j'ai pu ajouter aussi des fonctionnalitÃ©s manquantes qui sont fort utiles:\nConvertir toutes les CRDs en JSONSchemas afin de valider 100% des manifests Kubernetes Le rendre compatible avec les substitutions de variables de Flux. Enfin, j'ai pu le partager dans le Daggerverse et mettre Ã  jour mes workflows de CI sur Github Actions.\nğŸš€ ItÃ©ration rapide et collaboration grÃ¢ce Ã  un cache partagÃ© Utiliser un cache permet de ne pas rÃ©exÃ©cuter les Ã©tapes dont le code n'a pas changÃ©. Lors de la premiÃ¨re exÃ©cution, toutes les Ã©tapes seront exÃ©cutÃ©es, mais les suivantes ne reprendront que les Ã©tapes modifiÃ©es, ce qui permet de gagner un temps considÃ©rable.\nDagger permet de mettre en cache, Ã  chaque exÃ©cution, les opÃ©rations de manipulation des fichiers, la construction des conteneurs, l'exÃ©cution des tests, la compilation du code, ainsi que les volumes qui doivent Ãªtre explicitement dÃ©finis dans le code.\nPar dÃ©faut, le moteur Dagger est disponible en local, et utilise du cache local.\n1docker ps 2CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 33cec5bf51843 registry.dagger.io/engine:v0.12.1 \u0026#34;dagger-entrypoint.sâ€¦\u0026#34; 8 days ago Up 2 hours dagger-engine-ceb38152f96f1298 La proposition suivante vise Ã  dÃ©finir un cache partagÃ© et distant, accessible Ã  tous les collaborateurs ainsi que depuis la CI. L'objectif est d'accÃ©lÃ©rer les exÃ©cutions ultÃ©rieures, peu importe oÃ¹ Dagger est exÃ©cutÃ©.\nNous allons voir comment mettre cela en pratique avec:\nDes Github Runners exÃ©cutÃ©s en privÃ©, sur notre plateforme (Self-Hosted) Un moteur Dagger centralisÃ© Cloud Native Reference Cette solution de CI sur EKS est dÃ©ployÃ©e en utilisant le repository Cloud Native Ref. Je vous encourage vivement Ã  le consulter, car j'y aborde de nombreux sujets relatifs aux technlogies Cloud Native. L'idÃ©e initial de ce projet est de pouvoir dÃ©marrer rapidement une plateforme qui applique les bonnes pratiques en terme d'automatisation, de supervision, de sÃ©curitÃ© etc. Les commentaires et contributions sont les bienvenues ğŸ™ Voici comment les composants de CI interagissent, avec Dagger jouant un rÃ´le central grÃ¢ce au cache partagÃ©.\nMaintenant que nous avons une vue d'ensemble de Dagger et de son utilisation, nous allons explorer comment optimiser son utilisation en entreprise en utilisant un cache partagÃ©.\nğŸ¤– Github Self Hosted Runners: AccÃ¨s au cache Dagger s'intÃ¨gre bien avec la plupart des solutions de CI. Il suffit en effet de lancer une commande dagger. Dans cet article nous faisons usage de l'Action pour Github Actions.\n1 kubernetes-validation: 2 name: Kubernetes validation â˜¸ 3 runs-on: ubuntu-latest 4 steps: 5 - name: Checkout 6 uses: actions/checkout@v4 7 8 - name: Validate Flux clusters manifests 9 uses: dagger/dagger-for-github@v6 10 with: 11 version: \u0026#34;latest\u0026#34; 12 verb: call 13 module: github.com/Smana/daggerverse/kubeconform@kubeconform/v0.1.0 14 args: validate --manifests \u0026#34;./clusters\u0026#34; --catalog Ce job tÃ©lÃ©charge le code source du repo git et exÃ©cute le module kubeconform. Bien que cela fonctionne trÃ¨s bien, il faut noter que ce job est exÃ©cutÃ© sur les runners fournis par Github sur leur infrastructure.\nLes GitHub self-hosted runners sont des machines que vous configurez pour exÃ©cuter des workflows GitHub Actions sur votre propre infrastructure, plutÃ´t que d'utiliser les runners hÃ©bergÃ©s par GitHub. Ils offrent plus de contrÃ´le et de flexibilitÃ©, permettant de personnaliser l'environnement d'exÃ©cution selon vos besoins spÃ©cifiques. Cela peut conduire Ã  des performances amÃ©liorÃ©es et permet un accÃ¨s sÃ©curisÃ© Ã  des ressources privÃ©es.\nUn Scale set est un group de runners Github qui partage une configuration commune: .github/workflows/ci.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2 2kind: HelmRelease 3metadata: 4 name: dagger-gha-runner-scale-set 5spec: 6 releaseName: dagger-gha-runner-scale-set 7... 8 values: 9 runnerGroup: \u0026#34;default\u0026#34; 10 githubConfigUrl: \u0026#34;https://github.com/Smana/demo-cloud-native-ref\u0026#34; 11 githubConfigSecret: gha-runner-scale-set 12 maxRunners: 5 13 14 containerMode: 15 type: \u0026#34;dind\u0026#34; Ce scale set est configurÃ© pour le repo Cloud Native Ref Il faut lui indiquer un secret dans lequel est configurÃ© les paramÃ¨tres de la Github App dind indique le mode utilisÃ© pour lancer les conteneurs. âš ï¸ Attention cependant en termes de sÃ©curitÃ© : Dagger doit s'exÃ©cuter en tant qu'utilisateur root et avoir des permissions Ã©levÃ©es pour contrÃ´ler les conteneurs, volumes, rÃ©seaux, etc. (Plus d'informations ici). â˜¸ï¸ ConsidÃ©ration spÃ©cifiques Ã  EKS Il existe plusieurs approches lorsqu'il s'agit de l'optimisation du cache, chacune prÃ©sentant des avantages et inconvÃ©nients. Cela fait d'ailleurs l'objet de discussions trÃ¨s intÃ©ressantes ici. J'ai fait quelques choix qui, selon moi, sont un bon compromis entre disponibilitÃ© et performances dont voici les principales lignes:\nLe moteur Dagger: Un unique pod expose un service HTTP.\nNodePool spÃ©cifique : Un node pool avec des contraintes permettant d'obtenir des disques NVME locaux.\n1 - key: karpenter.k8s.aws/instance-local-nvme 2 operator: Gt 3 values: [\u0026#34;100\u0026#34;] 4 - key: karpenter.k8s.aws/instance-category 5 operator: In 6 values: [\u0026#34;c\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;r\u0026#34;] 7 taints: 8 - key: ogenki/io 9 value: \u0026#34;true\u0026#34; 10 effect: NoSchedule Les points de montages des conteneurs: Lorsqu'un nÅ“ud du nodepool io dÃ©marre, il exÃ©cute la commande /usr/bin/setup-local-disks raid0. Cette commande prÃ©pare les disques en crÃ©ant un array en raid0 et monte les systÃ¨mes de fichiers des conteneurs dessus. Ainsi, tout cet espace est directement accessible depuis le pod !\nâš ï¸ Notez que c'est un volume Ã©phÃ©mÃ¨re : les donnÃ©es sont perdues lorsque le pod est arrÃªtÃ©. C'est cet espace que nous utilisons pour le cache Dagger.\n1... 2 - name: varlibdagger 3 ephemeral: 4 volumeClaimTemplate: 5 spec: 6 accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] 7 resources: 8 requests: 9 storage: 10Gi 10 - name: varrundagger 11 ephemeral: 12 volumeClaimTemplate: 13 spec: 14 accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] 15 resources: 16 requests: 17 storage: 90Gi 18... 1kubectl exec -ti -n tooling dagger-engine-c746bd8b8-b2x6z -- /bin/sh 2/ # df -h | grep nvme 3/dev/nvme3n1 9.7G 128.0K 9.7G 0% /var/lib/dagger 4/dev/nvme2n1 88.0G 24.0K 88.0G 0% /run/buildkit Bonnes pratiques avec Karpenter: Afin d'optimiser la disponibilitÃ© du moteur Dagger, nous l'avons configurÃ© avec un Pod Disruption Budget ainsi que l'annotation karpenter.sh/do-not-disrupt: \u0026quot;true\u0026quot;. Par ailleurs il est prÃ©fÃ©rable d'utiliser des instances On-demand, que nous pourrions envisager de rÃ©server auprÃ¨s de AWS afin d'obtenir un discount.\nNetwork policies: Ã‰tant donnÃ© que les runners peuvent exÃ©cuter n'importe quel code, il est fortement recommandÃ© de limiter les flux rÃ©seaux au strict nÃ©cessaire, que ce soit pour les self-hosted runners ou le moteur dagger.\nAfin de tester cela, nous allons lancer un job qui crÃ©e un conteneur en installer de nombreux paquets relativement lourds. L'idÃ©e etant que cela prenne en peu de temps.\n.github/workflows/ci.yaml\n1 test-cache: 2 name: Testing in-cluster cache 3 runs-on: dagger-gha-runner-scale-set 4 container: 5 image: smana/dagger-cli:v0.12.1 6 env: 7 _EXPERIMENTAL_DAGGER_RUNNER_HOST: \u0026#34;tcp://dagger-engine:8080\u0026#34; 8 cloud-token: ${{ secrets.DAGGER_CLOUD_TOKEN }} 9 10 steps: 11 - name: Simulate a build with heavy packages 12 uses: dagger/dagger-for-github@v6 13 with: 14 version: \u0026#34;latest\u0026#34; 15 verb: call 16 module: github.com/shykes/daggerverse.git/wolfi@dfb1f91fa463b779021d65011f0060f7decda0ba 17 args: container --packages \u0026#34;python3,py3-pip,go,rust,clang\u0026#34; â„¹ï¸ AccÃ©der au moteur Dagger distant se fait en utilisant la variable d'environnement _EXPERIMENTAL_DAGGER_RUNNER_HOST\nLors de la premiÃ¨re exÃ©cution, le job met 3min et 37secs\nEn revanche tout autre exÃ©cution ultÃ©rieure sera beaucoup plus rapide (10secs)! ğŸ‰ ğŸš€ ğŸ¥³\nEn local ğŸ’» , je peux aussi bÃ©nÃ©ficier de ce cache en configurant mon environnement comme cela:\n1kubectl port-forward -n tooling svc/dagger-engine 8080 2_EXPERIMENTAL_DAGGER_RUNNER_HOST=\u0026#34;tcp://127.0.0.1:8080\u0026#34; Mes tests locaux seront aussi accessible par la CI et un autre dÃ©veloppeur reprenant mon travail n'aura pas Ã  tout reconstruire de zÃ©ro.\nAttention â• Cette solution a l'avantage non nÃ©gigeable de disposer d'un stockage ulta rapide! De plus l'architecture est on ne peut plus simple: un seul moteur Dagger avec un stockage local qui expose un service.\nâ– âš ï¸ C'est pourtant loin d'Ãªtre parfait: il faut, en effet accÃ©pter que ce cache soit Ã©phÃ©mÃ¨re malgrÃ© les prÃ©cautions prises pour garantir un niveau de disponibilitÃ© Ã©levÃ©. Par ailleurs il faut aussi prendre en compte le coÃ»t d'une instance qui tourne tout le temps, le scaling ne peut se faire qu'en prenant une machine plus grosse.\nDagger Cloud Dagger Cloud est une solution pour les entreprises permettant une visualisation trÃ¨s claire de l'exÃ©cution des pipelines, avec la possibilitÃ© de parcourir toutes les Ã©tapes et d'identifier rapidement les Ã©ventuelles problÃ¨mes. C'est gratuit pour un usage individuel et je vous encourage Ã  tester. Cette offre fourni Ã©galement une alternative Ã  la solution proposÃ©e ci-dessus: un cache distribuÃ©, gÃ©rÃ© par Dagger. (Plus d'informations ici) Your browser does not support the video tag. ğŸ’­ DerniÃ¨res remarques Dagger est un projet assez rÃ©cent qui Ã©volue vite, soutenu par une communautÃ© toujours plus grande et active. Les questions de scaling abordÃ©es dans cet article seront probablement amÃ©liorÃ©es dans le futur.\nPour les modules disponibles dans le Daggerverse, il est parfois difficile de juger leur qualitÃ©. Il n'y a pas de modules \u0026quot;validÃ©s\u0026quot; ou \u0026quot;officiels\u0026quot;, il faut donc souvent en tester plusieurs, analyser le code et parfois en crÃ©er un soi-mÃªme.\nCet article vous a permis de dÃ©couvrir Dagger et ses principales fonctions que j'ai utilisÃ©es. Mon expÃ©rience s'est limitÃ©e au SDK Golang, mais l'expÃ©rience devrait Ãªtre similaire avec d'autres langages. Je dÃ©couvre des choses nouvelles chaque jour. La prise en main initiale n'est pas Ã©vidente pour ceux qui, comme moi, ne dÃ©veloppent pas quotidiennement, mais plus je l'utilise sur des cas concrets, plus je me sens Ã  l'aise. J'ai mÃªme migrÃ© les quelques jobs de mon repo Ã  100% sur Dagger.\nJe suis passÃ© de Makefile Ã  Task et j'espÃ¨re maintenant aller plus loin avec Dagger. J'ai l'ambition de construire des pipelines plus complexes, comme la restauration et la vÃ©rification d'un backup Vault ou encore la crÃ©ation et les tests d'un cluster EKS avant de le dÃ©truire. Quoi qu'il en soit, Dagger fait maintenant partie de ma boÃ®te Ã  outils ! \u0026#x2705;\nğŸ”– References Doc Discord Youtube ","link":"https://blog.ogenki.io/fr/post/dagger-intro/","section":"post","tags":["devxp"],"title":"`Dagger`: la piÃ¨ce manquante de l'expÃ©rience dÃ©veloppeur?"},{"body":"","link":"https://blog.ogenki.io/fr/tags/devxp/","section":"tags","tags":null,"title":"Devxp"},{"body":"Le chiffrement TLS est un standard incontournable dans la sÃ©curisation des services et applications, que ce soit sur Internet ou au sein mÃªme de l'entreprise. Sur Internet, le recours Ã  un certificat TLS, validÃ© par une autoritÃ© de certification reconnue, est essentiel pour garantir la confidentialitÃ© des Ã©changes de donnÃ©es.\nEn ce qui concerne les communications internes, la PKI privÃ©e (Private Public Key Infrastructure) joue un rÃ´le crucial dans la distribution et la validation des certificats nÃ©cessaires au chiffrement des communications au sein de l'entreprise, assurant ainsi une sÃ©curitÃ© renforcÃ©e.\nDans cet article, nous allons plonger dans le vif du sujet : la mise en place d'une gestion efficace et robuste des certificats TLS au sein d'une entreprise. Nous explorerons les meilleures pratiques, les outils et les stratÃ©gies pour une infrastructure de certificats fiable.\nğŸ¯ Notre objectif Afin que les utilisateurs puissent accÃ©der aux applications, nous utiliserons le standard Gateway API. (Je vous invite Ã  lire mon prÃ©cÃ©dent article sur le sujet.) Dans l'implÃ©mentation prÃ©sentÃ©e ci-dessus, un composant joue un rÃ´le majeur: Cert-manager. Il s'agit en effet du moteur central qui se chargera de gÃ©nÃ©rer et de renouveler les certificats. Pour les applications destinÃ©es Ã  rester internes et non exposÃ©es sur Internet, nous opterons pour la gÃ©nÃ©ration de certificats via une PKI privÃ©e avec Vault d'Hashicorp. Quant aux applications publiques, elles utiliseront des certificats dÃ©livrÃ©s par Let's Encrypt. ğŸ›‚ A propos de Let's Encrypt BasÃ© sur le protocole ACME (Automatic Certificate Management Environment), cette solution permet une installation et un renouvellement automatiques des certificats.\nLet's Encrypt est simple Ã  mettre en oeuvre, gratuit et amÃ©liore la sÃ©curitÃ©. Cependant, il est important de noter que la durÃ©e des certificats est courte, et nÃ©cessite donc des renouvellements frÃ©quents.\nPour en savoir plus sur son fonctionnement, vous pouvez vous rÃ©fÃ©rer Ã  cette documentation. ğŸ” Une PKI privÃ©e avec Vault Une PKI privÃ©e, ou Infrastructure Ã  ClÃ© Publique privÃ©e, est un systÃ¨me cryptographique utilisÃ© au sein d'une organisation pour sÃ©curiser les donnÃ©es et les communications. Elle repose sur une AutoritÃ© de Certification (CA) interne qui Ã©met des certificats TLS spÃ©cifiques Ã  l'organisation.\nCe systÃ¨me permet Ã  une organisation de :\nContrÃ´ler entiÃ¨rement les procÃ©dures de vÃ©rification de l'identitÃ© et de l'authentification, et d'Ã©mettre des certificats pour des domaines internes, ce qui n'est pas possible avec Let's Encrypt. SÃ©curiser les communications et les donnÃ©es internes avec une authentification et un chiffrement forts au sein de l'organisation. Cependant, la mise en Å“uvre de ce type d'infrastructure requiert une attention particuliÃ¨re et une gestion de plusieurs composants. Ici, nous allons explorer une des fonctionnalitÃ©s principales de Vault, qui est initialement un outil de gestion de secrets mais qui peut aussi faire office de PKI interne.\nUne plateforme Cloud Native de rÃ©fÃ©rence Toutes les actions rÃ©alisÃ©es dans cet article proviennent de ce dÃ©pÃ´t git\nOn y trouve le code Opentofu permettant de dÃ©ployer et configurer Vault mais aussi de nombreuses sources qui me permettent de construire mes articles de blog. N'hÃ©sitez pas Ã  me faire des retours, ouvrir des issues si nÃ©cessaire ... ğŸ™\nâœ… PrÃ©requis Three-tier PKI Une infrastructure Ã  trois niveaux de PKI (Three-tier PKI) comprend une AutoritÃ© de Certification Racine (CA) au sommet, des AutoritÃ©s de Certification IntermÃ©diaires au milieu, et des EntitÃ©s Finales Ã  la base. L'AutoritÃ© de Certification Racine dÃ©livre des certificats aux AutoritÃ©s de Certification IntermÃ©diaires, qui Ã  leur tour Ã©mettent des certificats aux utilisateurs finaux ou aux dispositifs. Cette structure renforce la sÃ©curitÃ© en rÃ©duisant l'exposition de l'AutoritÃ© de Certification Racine et simplifie la gestion et la rÃ©vocation des certificats, offrant une solution Ã©volutive et flexible pour la sÃ©curitÃ© numÃ©rique. Pour renforcer la sÃ©curitÃ© le systÃ¨me de gestion de certificats, il est recommandÃ© de crÃ©er une AutoritÃ© de Certification Racine (AC Racine) hors ligne. Nous devons donc rÃ©aliser, au prÃ©alable, les Ã©tapes suivantes :\nGÃ©nÃ©rer l'AutoritÃ© de Certification Racine hors ligne : Cette approche minimise les risques de sÃ©curitÃ© en isolant l'AC Racine du rÃ©seau.\nCrÃ©er une AutoritÃ© de Certification IntermÃ©diaire : Elle agit sous l'autoritÃ© de l'AC Racine et est utilisÃ©e pour Ã©mettre des certificats, permettant une gestion plus flexible et sÃ©curisÃ©e.\nGÃ©nÃ©rer le certificat pour le serveur Vault depuis l'AC IntermÃ©diaire : Cela assure une chaÃ®ne de confiance depuis l'AC Racine jusqu'aux certificats utilisateurs finaux, en passant par l'AC IntermÃ©diaire.\nEn suivant la procÃ©dure dÃ©crite ici vous devriez obtenir les fichiers suivants qui seront utilisÃ©s dans le reste de cet article. Il s'agit lÃ  d'une proposition basÃ© sur openssl, et vous pouvez utiliser la mÃ©thode qui vous convient pour parvenir au mÃªme rÃ©sultat\n1cd opentofu/vault/cluster 2 3ls .tls/*.pem 4.tls/bundle.pem .tls/ca-chain.pem .tls/intermediate-ca-key.pem .tls/intermediate-ca.pem .tls/root-ca-key.pem .tls/root-ca.pem .tls/vault-key.pem .tls/vault.pem ğŸ—ï¸ Construire le cluster Il existe plusieurs mÃ©thodes pour dÃ©ployer un cluster Vault mais je n'ai pas trouvÃ© celle qui me convenait, je l'ai donc construite en prenant les dÃ©cisions suivantes:\nStockage intÃ©grÃ© basÃ© sur le protocole Raft, qui est particuliÃ¨rement adaptÃ© aux systÃ¨mes distribuÃ©s et garantit une rÃ©silience Ã©levÃ©e. Voici un tableau illustrant la tolÃ©rance aux pannes en fonction de la taille du cluster :\nCluster size Failure tolerance 1 0 3 1 5 2 7 3 Par consÃ©quent notre cluster Vault sera composÃ© de 5 membres, ce qui nous permettra de tolÃ©rer la dÃ©faillance de 2 nÅ“uds.\nStratÃ©gie de nÅ“uds Ã©phÃ©mÃ¨res avec instances SPOT : L'architecture est constituÃ©e exclusivement d'instances SPOT pour une efficacitÃ© optimale en termes de coÃ»t. Ce groupe est configurÃ© avec trois pools d'instances Spot distincts, chacun exploitant un type d'instance diffÃ©rent. Cette diversification stratÃ©gique vise Ã  pallier toute dÃ©faillance potentielle liÃ©e Ã  une pÃ©nurie spÃ©cifique de type d'instance SPOT, assurant ainsi une haute disponibilitÃ© et une continuitÃ© de service ininterrompue, tout en maximisant l'efficience des coÃ»ts.\nFonctionnalitÃ© de dÃ©verrouillage automatique de Vault (Unseal) : Cette fonction est essentielle compte tenu de la nature Ã©phÃ©mÃ¨re de nos nÅ“uds. Elle permet de minimiser les temps d'arrÃªt et d'Ã©liminer le besoin d'interventions manuelles pour le dÃ©verrouillage de Vault.\nCet article n'a pas pour but de dÃ©crire toutes les Ã©tapes qui sont disponibles dans la documentation du repo Github. Le fichier de variables Opentofu contient la configuration souhaitÃ©e.\n1name = \u0026#34;ogenki-vault\u0026#34; 2leader_tls_servername = \u0026#34;vault.priv.cloud.ogenki.io\u0026#34; 3domain_name = \u0026#34;priv.cloud.ogenki.io\u0026#34; 4env = \u0026#34;dev\u0026#34; 5mode = \u0026#34;ha\u0026#34; 6region = \u0026#34;eu-west-3\u0026#34; 7enable_ssm = true 8 9# Use hardened AMI 10ami_owner = \u0026#34;xxx\u0026#34; # Compte AWS oÃ¹ se trouve l\u0026#39;AMI 11ami_filter = { 12 \u0026#34;name\u0026#34; = [\u0026#34;*hardened-ubuntu-*\u0026#34;] 13} AprÃ¨s avoir exÃ©cutÃ© l'ensemble des Ã©tapes, Vault peut Ãªtre utilisÃ© et nous obtenons un cluster constituÃ© de 5 noeuds.\nğŸ› ï¸ Configuration Le dÃ©ploiement d'une plateforme complÃ¨te se fait par Ã©tapes distinctes car certaines opÃ©rations doivent Ãªtre faites manuellement afin de garantir une sÃ©curitÃ© optimale: La gÃ©nÃ©ration du certificat racine qui doit Ãªtre conservÃ© hors ligne et l'initialisation de Vault avec le token root initial.\nIl faut bien entendu tous les composants rÃ©seaux afin d'y dÃ©ployer des machines, puis le cluster Vault peut Ãªtre installÃ© et configurÃ© avant de considÃ©rer l'ajout d'autres Ã©lÃ©ments d'infrastructure, qui dÃ©pendront probablement des informations sensibles stockÃ©es dans Vault.\nLa configuration de Vault est appliquÃ©e grÃ¢ce au provider Terraform dont l'authentification se fait via un token gÃ©nÃ©rÃ© depuis l'instance Vault. La proposition ici dÃ©montre comment configurer la PKI et autoriser les applications internes Ã  interagir avec l'API de Vault et, en particulier, comment configurer Cert-Manager.\nIl suffit donc de dÃ©clarer les variables propre Ã  votre organisation\n1domain_name = \u0026#34;priv.cloud.ogenki.io\u0026#34; 2pki_common_name = \u0026#34;Ogenki Vault Issuer\u0026#34; 3pki_country = \u0026#34;France\u0026#34; 4pki_organization = \u0026#34;Ogenki\u0026#34; 5pki_domains = [ 6 \u0026#34;cluster.local\u0026#34;, 7 \u0026#34;priv.cloud.ogenki.io\u0026#34; 8] AprÃ¨s avoir suivi la procÃ©dure, la PKI est configurÃ©e et il est alors possible de gÃ©nÃ©rer des certificats.\nInstaller la CA privÃ©e sur les machines Contrairement aux PKI publiques, oÃ¹ les certificats sont automatiquement approuvÃ©s par les logiciels clients, dans une PKI privÃ©e, les certificats doivent Ãªtre approuvÃ©s manuellement par les utilisateurs ou dÃ©ployÃ©s sur tous les appareils par l'administrateur de domaineâ€‹\nUbuntu Archlinux macOS Windows Server ğŸ’¾ Sauvegardes planifiÃ©es Comme toute solution contenant des donnÃ©es, il est indispensable de les sauvegarder. Notons aussi la sensibilitÃ© de celles stockÃ©es dans Vault. Il nous faut donc une sauvegarde rÃ©guliÃ¨re dans lieu sÃ©curisÃ©. La solution proposÃ©e ici est tout simplement une Cronjob. Elle utilise Crossplane pour construire les ressources AWS et se dÃ©compose comme suit:\nUn bucket S3 oÃ¹ seront stockÃ©s les snapshots Une polique de rÃ©tention pour ne conserver que les 30 derniÃ¨res sauvegardes. Le bucket est chiffrÃ© avec une clÃ© KMS spÃ©cifique. Un external-secret pour pouvoir rÃ©cupÃ©rer les paramÃ¨tres d'authentificatinon de l'Approle spÃ©cifique Ã  la Cronjob. Une Cronjob qui exÃ©cute le script disponible dans le repo et qui Ã©ffectue un snapshot tel que dÃ©crit dans la doc d'Hashicorp. Un rÃ´le IRSA qui donne les permissions au pod d'Ã©crire les snapshots sur S3. ğŸš€ En pratique avec Gateway API! L'objectif de cet article est de dÃ©montrer une utilisation concrÃ¨te avec Gateway-API et, en fonction du protocole utilisÃ©, plusieurs options sont possibles pour sÃ©curiser les connexions avec du TLS. Nous pouvons notamment faire du Passthrough et faire en sorte que la terminaison TLS se fasse sur l'upstream (exposÃ© directement par le pod). En revanche pour notre cas d'usage, nous allons utiliser le cas le plus commun: HTTPS au niveau de la Gateway. Voici un exemple simple car il suffit uniquement d'indiquer le secret Kubernetes contenant le certificat\n1listeners: 2- protocol: HTTPS 3 port: 443 4 tls: 5 mode: Terminate 6 certificateRefs: 7 - name: foobar-tls Voyons cela en dÃ©tail, car il y a certains Ã©lÃ©ments Ã  prÃ©parer afin de pouvoir obtenir ces secrets ğŸ”.\nâ˜ï¸ Un certificat publique Info Cert-Manager est un outil open source permettant de gÃ©rer les certificats TLS dans Kubernetes. Il s'agit, en fait, d'un opÃ©rateur Kubernetes qui est contrÃ´lÃ© par l'usage de CRDs (Custom Resources Definitions): il est en effet possible de gÃ©nÃ©rer des certificats en crÃ©ant des resources de type certificate. Cert-manager se charge ensuite de vÃ©rifier qu'ils sont toujours valides et dÃ©clenche un renouvellement lorsque c'est nÃ©cessaire. Il peut Ãªtre intÃ©grÃ© avec un nombre grandissant d'autoritÃ© de certifications comme Let's Encrypt, Venafi, Google, Vault ... Dans la mise en place de cert-manager avec Let's Encrypt, on utilise un Issuer pour configurer la gÃ©nÃ©ration de certificats dans un namespace spÃ©cifique. Par contre, un ClusterIssuer Ã©tend cette capacitÃ© Ã  tous les namespaces du cluster, offrant ainsi une solution plus globale et flexible pour la gestion des certificats.\nsecurity/base/cert-manager/le-clusterissuer-prod.yaml\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: letsencrypt-prod 5spec: 6 acme: 7 email: mymail@domain.tld 8 server: https://acme-v02.api.letsencrypt.org/directory 9 privateKeySecretRef: 10 name: ogenki-issuer-account-key 11 solvers: 12 - selector: 13 dnsZones: 14 - \u0026#34;cloud.ogenki.io\u0026#34; 15 dns01: 16 route53: 17 region: eu-west-3 Nous utilisons ici l'instance de prod de Let's Encrypt qui est soumise Ã  certaines rÃ¨gles et il est recommandÃ© de commencer vos tests sur l'instance de staging. L'adresse email est utilisÃ©e pour recevoir des notifications, comme la nÃ©cessitÃ© de renouveler Une clÃ© ogenki-issuer-account-key est gÃ©nÃ©rÃ©e et est utilisÃ©e pour s'authentifier auprÃ¨s du serveur ACME. Le mÃ©canisme qui permet de prouver la lÃ©gitimitÃ© d'une demande de certificat est faite grÃ¢ce Ã  une rÃ©solution DNS. A prÃ©sent, comment pouvons-nous faire appel Ã  ce ClusterIssuer depuis une resource Gateway-API? Figurez-vous qu'il y a une intÃ©gration trÃ¨s simple par l'usage d'une annotation au niveau de la Gateway. Cette solution est expÃ©rimentale et requiert un paramÃ¨tre spÃ©cifique lors du dÃ©ploiment de cert-manager.\nsecurity/base/cert-manager/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta2 2kind: HelmRelease 3metadata: 4 name: cert-manager 5 namespace: security 6spec: 7 values: 8... 9 featureGates: ExperimentalGatewayAPISupport=true Il est aussi nÃ©cessaire de donner les permissions au contrÃ´leur Cert-manager d'interagir avec Route53 pour pouvoir complÃ©ter le challenge DNS. Ici j'utilise une Composition Crossplane. (â„¹ï¸ Si vous souhaitez creuser le sujet c'est par ici.)\nPuis il faut ajouter l'annotation dans la Gateway et indiquer le secret cible.\ninfrastructure/base/gapi/platform-public-gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: platform-public 5 annotations: 6 cert-manager.io/cluster-issuer: letsencrypt-prod 7spec: 8 gatewayClassName: cilium 9 listeners: 10 - name: http 11 hostname: \u0026#34;*.${domain_name}\u0026#34; 12... 13 tls: 14 mode: Terminate 15 certificateRefs: 16 - name: platform-public-tls Lorsque la Gateway est crÃ©Ã©, un certificat est gÃ©nÃ©rÃ©. Ce certificat utilise le ClusterIssuer letsencrypt-prod indiquÃ© ci-dessus.\n1kubectl describe certificate -n infrastructure platform-public-tls 2Name: platform-public-tls 3Namespace: infrastructure 4API Version: cert-manager.io/v1 5Kind: Certificate 6... 7Spec: 8 Dns Names: 9 *.cloud.ogenki.io 10 Issuer Ref: 11 Group: cert-manager.io 12 Kind: ClusterIssuer 13 Name: letsencrypt-prod 14 Secret Name: platform-public-tls 15 Usages: 16 digital signature 17 key encipherment 18Status: 19 Conditions: 20 Last Transition Time: 2024-01-24T20:43:26Z 21 Message: Certificate is up to date and has not expired 22 Observed Generation: 1 23 Reason: Ready 24 Status: True 25 Type: Ready 26 Not After: 2024-04-23T19:43:24Z 27 Not Before: 2024-01-24T19:43:25Z 28 Renewal Time: 2024-03-24T19:43:24Z 29 Revision: 1 Enfin, au bout de quelques secondes, un secret Kubernetes est crÃ©Ã© et contient le certificat. Il s'agit d'un secret de type TLS contenant les fichiers tls.crt tls.key et ca.crt\nLe plugin view-cert Les certificats gÃ©nÃ©rÃ©s par cert-manager sont stockÃ©s dans des secrets Kubernetes. Bien qu'il soit possible de les extraire Ã  coup de commandes base64 et openssl. Pourquoi ne pas se simplifier la vie? Je suis un adepte de la ligne de commande et j'utilise pour ma part rÃ©guliÃ¨rement le plugin view-cert qui permet d'afficher une synthÃ¨se des secrets de type tls.\n1kubectl view-cert -n infrastructure platform-public-tls 2[ 3 { 4 \u0026#34;SecretName\u0026#34;: \u0026#34;platform-public-tls\u0026#34;, 5 \u0026#34;Namespace\u0026#34;: \u0026#34;infrastructure\u0026#34;, 6 \u0026#34;Version\u0026#34;: 3, 7 \u0026#34;SerialNumber\u0026#34;: \u0026#34;35f659ad03e437805fbf48111b74738efe3\u0026#34;, 8 \u0026#34;Issuer\u0026#34;: \u0026#34;CN=R3,O=Let\u0026#39;s Encrypt,C=US\u0026#34;, 9 \u0026#34;Validity\u0026#34;: { 10 \u0026#34;NotBefore\u0026#34;: \u0026#34;2024-01-28T09:41:35Z\u0026#34;, 11 \u0026#34;NotAfter\u0026#34;: \u0026#34;2024-04-27T09:41:34Z\u0026#34; 12 }, 13 \u0026#34;Subject\u0026#34;: \u0026#34;CN=*.cloud.ogenki.io\u0026#34;, 14 \u0026#34;IsCA\u0026#34;: false 15 } 16] Il peut Ãªtre installÃ© en utilisant krew\n1kubectl krew install view-cert ğŸ  Un certificat privÃ©e Pour un certificat privÃ© avec Vault, nous devons aussi dÃ©clarer un ClusterIssuer mais sa dÃ©finition diffÃ©re lÃ©gerement:\nsecurity/base/cert-manager/vault-clusterissuer.yaml\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: vault 5 namespace: security 6spec: 7 vault: 8 server: https://vault.priv.cloud.ogenki.io:8200 9 path: pki_private_issuer/sign/ogenki 10 caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0... 11 auth: 12 appRole: 13 path: approle 14 roleId: f8363d0f-b7db-9b08-67ab-8425ab527587 15 secretRef: 16 name: cert-manager-vault-approle 17 key: secretId L'URL indiquÃ©e est celle du serveur Vault. Elle doit Ãªtre accessible depuis les pods dans Kubernetes Le path dans Vault fait partie de la phase de configuration de Vault. Il s'agit du rÃ´le autorisÃ© Ã  gÃ©nÃ©rÃ© des certificats. Nous utilisons ici une authentification via un Approle. Pour plus de dÃ©tails sur l'ensemble des actions nÃ©cessaires Ã  la configuration de Cert-Manager avec Vault, vous rÃ©fÃ©rer Ã  cette procÃ©dure.\nLa principale diffÃ©rence avec la mÃ©thode utilisÃ©e pour Let's Encrypt rÃ©side dans le faut que le certificat doit Ãªtre crÃ©Ã© explicitement. En effet, la mÃ©thode prÃ©cÃ©dente permettait de le faire automatiquement avec une annotation.\ninfrastructure/base/gapi/platform-private-gateway-certificate.yaml\n1apiVersion: cert-manager.io/v1 2kind: Certificate 3metadata: 4 name: private-gateway-certificate 5spec: 6 secretName: private-gateway-tls 7 duration: 2160h # 90d 8 renewBefore: 360h # 15d 9 commonName: private-gateway.priv.cloud.ogenki.io 10 dnsNames: 11 - gitops-${cluster_name}.priv.${domain_name} 12 - grafana-${cluster_name}.priv.${domain_name} 13 - harbor.priv.${domain_name} 14 issuerRef: 15 name: vault 16 kind: ClusterIssuer 17 group: cert-manager.io Comme on peut le voir, ce certificat pourra Ãªtre utilisÃ© pour accÃ©der aux applications weave-gitops, grafana et harbor. Il a une durÃ©e de validitÃ© de 90 jours et sera renouvelÃ© automatiquement 15 jours avant son expiration.\nQuelques secondes aprÃ¨s la crÃ©ation de la resource certificate, un secret Kubernetes est gÃ©nÃ©rÃ©.\n1kubectl describe certificates -n infrastructure private-gateway-certificate 2Name: private-gateway-certificate 3Namespace: infrastructure 4API Version: cert-manager.io/v1 5Kind: Certificate 6... 7Spec: 8 Common Name: private-gateway.priv.cloud.ogenki.io 9 Dns Names: 10 gitops-mycluster-0.priv.cloud.ogenki.io 11 grafana-mycluster-0.priv.cloud.ogenki.io 12 harbor.priv.cloud.ogenki.io 13 Duration: 2160h0m0s 14 Issuer Ref: 15 Group: cert-manager.io 16 Kind: ClusterIssuer 17 Name: vault 18 Renew Before: 360h0m0s 19 Secret Name: private-gateway-tls 20Status: 21 Conditions: 22 Last Transition Time: 2024-01-27T19:54:57Z 23 Message: Certificate is up to date and has not expired 24 Observed Generation: 1 25 Reason: Ready 26 Status: True 27 Type: Ready 28 Not After: 2024-04-26T19:54:57Z 29 Not Before: 2024-01-27T19:54:27Z 30 Renewal Time: 2024-04-11T19:54:57Z 31 Revision: 1 32Events: 33 Type Reason Age From Message 34 ---- ------ ---- ---- ------- 35 Normal Issuing 41m cert-manager-certificates-trigger Issuing certificate as Secret does not exist 36 Normal Generated 41m cert-manager-certificates-key-manager Stored new private key in temporary Secret resource \u0026#34;private-gateway-certificate-jggkv\u0026#34; 37 Normal Requested 41m cert-manager-certificates-request-manager Created new CertificateRequest resource \u0026#34;private-gateway-certificate-1\u0026#34; 38 Normal Issuing 38m cert-manager-certificates-issuing The certificate has been successfully issued Enfin, Il suffit d'utiliser ce secret dans la dÃ©claration de la Gateway privÃ©e.\ninfrastructure/base/gapi/platform-private-gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: platform-private 5spec: 6 gatewayClassName: cilium 7 listeners: 8 - name: http 9 hostname: \u0026#34;*.priv.${domain_name}\u0026#34; 10... 11 tls: 12 mode: Terminate 13 certificateRefs: 14 - name: private-gateway-tls Nous pouvons vÃ©rifier l'autoritÃ© de certification en utilisant la commande curl:\n1curl --verbose -k https://gitops-mycluster-0.priv.cloud.ogenki.io 2\u0026gt;\u0026amp;1 | grep \u0026#39;issuer:\u0026#39; 2* issuer: O=Ogenki; CN=Ogenki Vault Issuer ğŸ’­ DerniÃ¨res remarques â“ Qui n'a pas subit un incident liÃ© au renouvellement d'un certificat? â“ Comment obtenir un niveau de sÃ©curitÃ© correspondant aux Ã©xigences de l'entreprise? â“ Comment peut-on se simplifier les tÃ¢ches opÃ©rationnelles liÃ©es Ã  la maintenance des certificats TLS?\nNous avons pu explorer une rÃ©ponse concrÃ¨te Ã  ces questions dans cet article. L'automatisation mise en oeuvre grÃ¢ce Ã  Cert-manager permet de minimiser les tÃ¢ches opÃ©rationnelles tout en amÃ©liorant le niveau de sÃ©curitÃ©.\nLa mise en oeuvre avec Let's Encrypt et Gateway API est vraiment super simple! D'autre part le niveau de sÃ©curitÃ© apportÃ© par Vault pour les commmunications internes est vraiment Ã  considÃ©rer. Cependant, il est vrai que cela nÃ©cessite la mise en oeuvre de plusieurs composants et une rigueur sans faille pour conserver un niveau de sÃ©curitÃ© optimal.\nPoints d\u0026#39;attention pour la prod Il est important de rappeler quelques recommandations et bonnes pratiques avant de considÃ©rer une mise en production. Afin que cet article reste lisible, certains points ne sont mÃªme pas Ã©tÃ© adressÃ©s mais il est primordial de les inclure dans votre stratÃ©gie:\nConservez le certificat racine hors ligne. En d'autres termes, il est impÃ©ratif de le stocker sur un support non connectÃ© pour le protÃ©ger de toute menace potentielle. La rÃ©vocation de la CA root ou intermÃ©diaire n'a pas Ã©tÃ© Ã©voquÃ©. Ainsi que la mise Ã  disposition d'une liste de rÃ©vocation (Certificate Revocation List). L'accÃ¨s Ã  l'API Vault doit Ãªtre rigoureusement restreint Ã  un rÃ©seau privÃ©. Vous devriez jeter un coup d'oeil Ã  mon article sur Tailscale. Vous noterez aussi que je ne parle pas du tout d'authentification mais Il est essentiel de configurer un fournisseur d'identitÃ© dÃ¨s le dÃ©but et d'activer l'authentification multi-facteurs (MFA) pour renforcer la sÃ©curitÃ©. Par ailleurs, il est conseillÃ© de rÃ©voquer le token racine de Vault une fois l'authentification et les autorisations adÃ©quates mises en place. Si nÃ©cessaire, le token peut Ãªtre rÃ©gÃ©nÃ©rÃ© suivant la procÃ©dure disponible ici. Par dÃ©faut le code proposÃ© dÃ©ploie des AMI (Images d'instances AWS) Ubuntu de Canonical. Il est conseillÃ© d'en utiliser une dont la sÃ©curitÃ© a Ã©tÃ© renforcÃ©e (Hardened AMI). J'ai construit la mienne en utilisant ce projet. Afin de pouvoir initialiser Vault une commande doit Ãªtre lancÃ©e sur l'instance ce qui justifie l'utilisation de SSM. Cependant il est conseillÃ© de le dÃ©sactiver lorsque la phase d'initialisation est terminÃ©e (enable_ssm: false dans les variables Opentofu) Envoyez les logs d'audit vers un SIEM afin de pouvoir dÃ©tecter des comportements suspects. Alerter avant que les certificats n'arrivent Ã  expiration. Vous pouvez, par exemple, utiliser cet exporter Prometheus opensourcÃ© par les potes d'Enix ğŸ˜‰. Il s'agit lÃ  d'une sÃ©curitÃ© supplÃ©mentaire sachant que l'architecture proposÃ©e rend le tout automatisÃ©. Accordez une attention particuliÃ¨re aux clÃ©s KMS: celle utilisÃ© pour dÃ©vÃ©rouiller Vault, mais aussi celle qui permet de crÃ©er des snapshots. Elles sont vitalespour la restauration de vos sauvegardes. \u0026quot;Une sauvegarde qui n'est pas vÃ©rifiÃ©e ne sert Ã  rien\u0026quot;: Il faut donc construire un workflow qui permettra de vÃ©rifier la consistence des donnÃ©es dans Vault. C'est peut Ãªtre le sujet d'un autre article, stay tuned! Organisez pÃ©riodiquement des exercices de reprise aprÃ¨s sinistre (PRA) pour garantir votre capacitÃ© Ã  reconstruire l'ensemble du systÃ¨me Ã  partir de zÃ©ro, en vous assurant de disposer de toute la documentation et des outils nÃ©cessaires. ğŸ”– References Github Issues\nCert-manager et Vault: chaine complÃ¨te de l'autoritÃ© de certification Blog posts\nPrivÃ© vs Public PKI: Construire un plan efficace (Author: Nick Naziridis) PKI Meilleures pratiques pour 2023 Build an Internal PKI with Vault (Author: StÃ©phane Este-Gracias) Documentation Hashicorp\nA propos du stockage Raft: Reference Architecture Deployment Guide AWS Production hardening PKI ","link":"https://blog.ogenki.io/fr/post/pki-gapi/","section":"post","tags":["security"],"title":"`TLS` avec Gateway API: Une gestion efficace et sÃ©curisÃ©e des certificats publiques et privÃ©s"},{"body":"","link":"https://blog.ogenki.io/fr/tags/security/","section":"tags","tags":null,"title":"Security"},{"body":" Update 2024-11-23 J'utilise dÃ©sormais KCL (Kusion Configuration Language) pour les compositions Crossplane.\nAvec l'Ã©mergence du Platform engineering, on assiste Ã  une Ã©volution vers la crÃ©ation de solutions dites \u0026quot;self-service\u0026quot; Ã  destination des dÃ©veloppeurs. Cette approche permet une standardisation des pratiques DevOps, une meilleure expÃ©rience pour les dÃ©veloppeurs, et une rÃ©duction de la charge cognitive liÃ©e Ã  la gestion des outils.\nCrossplane, un projet sous l'Ã©gide de la Cloud Native Computing Foundation (CNCF) vise Ã  devenir le framework incontournable pour crÃ©er des plateformes Cloud Natives. Dans mon premier article sur Crossplane, j'ai prÃ©sentÃ© cet outil et expliquÃ© comment il utilise les principes GitOPs pour l'infrastructure, permettant ainsi de crÃ©er un cluster GKE.\nLe projet, qui fÃªte maintenant ses 5 ans ğŸ‚ğŸ‰, a gagnÃ© en maturitÃ© et s'est enrichi de nouvelles fonctionnalitÃ©s au fil du temps.\nDans cet article, nous explorerons certaines fonctionnalitÃ©s clÃ©s de Crossplane, avec un intÃ©rÃªt particulier pour les compositions functions qui gÃ©nÃ¨rent un vif intÃ©rÃªt au sein de la communautÃ©. Allons-nous assister Ã  un tournant dÃ©cisif pour le projet ?\nğŸ¯ Notre objectif La documentation de Crossplane est trÃ¨s bien fournie, nous allons donc passer rapidement sur les concepts de base pour se concentrer sur un cas d'usage concret: DÃ©ployer Harbor sur un cluster EKS en suivant les recommandations en terme de haute disponibilitÃ©.\nHarbor Harbor, issue aussi de la CNCF, est une solution de gestion d'artefacts de conteneurs centrÃ©e sur la sÃ©curitÃ©. Son rÃ´le principal est de stocker, signer et analyser les vulnÃ©rabilitÃ©s des images de conteneurs. Harbor dispose d'un contrÃ´le d'accÃ¨s fin, d'une API ainsi que d'une interface web afin de permettre aux Ã©quipes de dev d'y accÃ©der et gÃ©rer leurs images simplement.\nLa disponibilitÃ© d'Harbor dÃ©pend principalement de ses composants avec des donnÃ©es persistantes (stateful). L'utilisateur est responsable de leur mise en Å“uvre, qui doit Ãªtre adaptÃ©e Ã  l'infrastructure cible. L'article prÃ©sente les options choisies pour un niveau de disponibilitÃ© optimal.\nRedis dÃ©ployÃ© avec le chart Helm de Bitnami en mode \u0026quot;master/slave\u0026quot; Les artefacts sont stockÃ©s dans un bucket AWS S3 Une instance RDS pour la base de donnÃ©es PostgreSQL Nous allons maintenant explorer comment Crossplane facilite le provisionnement d'une base de donnÃ©es (RDS), en offrant un niveau d'abstraction simple, exposant uniquement les options nÃ©cessaires. ğŸš€\nğŸ—ï¸ PrÃ©requis Avant de pouvoir construire nos compositions, nous devons prÃ©parer le terrain car certaines opÃ©rations prÃ©alables sont nÃ©cessaires. Ces Ã©tapes sont rÃ©alisÃ©es dans un ordre bien prÃ©cis:\nDÃ©ploiement du contrÃ´lleur Crossplane en utilisant le chart Helm. Installation des providers et de leur configuration. DÃ©ploiement de diverses configurations faisant usage des providers installÃ©s prÃ©alablement. Notamment les Compositions et les Composition Functions. DÃ©clarations de Claims pour consommer les Compositions. Ces Ã©tapes sont traduites en dÃ©pendances Flux, et peuvent Ãªtre consultÃ©es ici.\nLes sources Toutes les actions rÃ©alisÃ©es dans cet article proviennent de ce dÃ©pÃ´t git\nOn peut y trouver de nombreuses sources qui me permettent de construire mes articles de blog. N'hÃ©sitez pas Ã  me faire des retours, ouvrir des issues si nÃ©cessaire ... ğŸ™\nğŸ“¦ Les compositions Pour le dire simplement, une Composition dans Crossplane est un moyen d'agrÃ¨ger et de gÃ©rer automatiquement plusieurs ressources dont la configuration peut s'avÃ©rer parfois complexe.\nElle utilise l'API de Kubernetes pour dÃ©finir et orchestrer non seulement des Ã©lÃ©ments d'infrastructure tels que le stockage et le rÃ©seau, mais aussi de nombreux autres composants (se rÃ©fÃ©rer Ã  la liste des providers). Cette mÃ©thode offre aux dÃ©veloppeurs une interface simplifiÃ©e, reprÃ©sentant une couche d'abstraction qui masque les dÃ©tails techniques plus complexes de l'infrastructure sous-jacente.\nPour atteindre mon objectif, qui est de crÃ©er une base de donnÃ©es RDS lors du dÃ©ploiement de l'application Harbor, j'ai d'abord recherchÃ© s'il existait un exemple pertinent. Pour ce faire, j'ai utilisÃ© le marketplace d'Upbound, oÃ¹ l'on peut trouver de nombreuses Compositions pouvant servir de point de dÃ©part.\nEn me basant sur la composition configuration-rds, j'ai souhaitÃ© y ajouter les Ã©lÃ©ments suivants:\nğŸ”‘ Permettre aux pods d'accÃ©der Ã  l'instance. â–¶ï¸ CrÃ©ation d'un service de type ExternalName avec un nom prÃ©dictible qui peut Ãªtre utilisÃ© dans la configuration de Harbor ğŸ’¾ CrÃ©ation de bases de donnÃ©es et des rÃ´les qui en seront propriÃ©taires. â“ Comment cette Composition serait-elle alors utilisÃ©e si, par exemple, un dÃ©veloppeur souhaite disposer d'une base de donnÃ©es? Il suffit de dÃ©clarer une Claim qui reprÃ©sente le niveau d'abstraction exposÃ© aux utilisateurs.\ntooling/base/harbor/sqlinstance.yaml\n1apiVersion: cloud.ogenki.io/v1alpha1 2kind: SQLInstance 3metadata: 4 name: xplane-harbor 5 namespace: tooling 6spec: 7 parameters: 8 engine: postgres 9 engineVersion: \u0026#34;15\u0026#34; 10 size: small 11 storageGB: 20 12 databases: 13 - owner: harbor 14 name: registry 15 passwordSecretRef: 16 namespace: tooling 17 name: harbor-pg-masterpassword 18 key: password 19 compositionRef: 20 name: xsqlinstances.cloud.ogenki.io 21 writeConnectionSecretToRef: 22 name: xplane-harbor-rds ici nous constatons que cela se limite Ã  une simple ressource avec peu de paramÃ¨tres pour exprimer nos souhaits:\nUne instance PostgreSQL en version 15 sera crÃ©Ã©e La type d'instance de celle-ci est laissÃ© Ã  l'apprÃ©ciation de l'Ã©quipe plateforme (les mainteneurs de la composition). Dans la Claim ci-dessus nous souhaitons une \u0026quot;petite\u0026quot; instance, qui est traduit par la composition en db.t3.small. infrastructure/base/crossplane/configuration/sql-instance-composition.yaml\n1transforms: 2 - type: map 3 map: 4 large: db.t3.large 5 medium: db.t3.medium 6 small: db.t3.small Le mot de passe de l'utilisateur master est extrait d'un secret harbor-pg-masterpassword, gÃ©nÃ©rÃ© via un External Secret. Une fois l'instance crÃ©Ã©e, les dÃ©tails pour la connexion sont stockÃ©s dans un secret xplane-harbor-rds C'est lÃ  que nous pouvons pleinement apprÃ©cier la puissance des Compositions Crossplane! En effet, de nombreuses ressources sont gÃ©nÃ©rÃ©es de maniÃ¨re transparente, comme illustrÃ© par le schÃ©ma suivant :\nAu bout de quelques minutes, toutes les ressources sont crÃ©Ã©es. (â„¹ï¸ La CLI Crossplane permet dÃ©sormais de nombreuses opÃ©rations, notamment visualiser les ressources d'une Composition. Elle est dÃ©nommÃ©e crank pour la diffÃ©rencier du binaire crossplane qui est le binaire qui tourne sur Kubernetes. )\n1kubectl get xsqlinstances 2NAME SYNCED READY COMPOSITION AGE 3xplane-harbor-jmdhp True True xsqlinstances.cloud.ogenki.io 8m32s 4 5crank beta trace xsqlinstances.cloud.ogenki.io xplane-harbor-jmdhp 6NAME SYNCED READY STATUS 7XSQLInstance/xplane-harbor-jmdhp True True Available 8â”œâ”€ SecurityGroupIngressRule/xplane-harbor-jmdhp-n785k True True Available 9â”œâ”€ SecurityGroup/xplane-harbor-jmdhp-8jnhc True True Available 10â”œâ”€ Object/external-service-xplane-harbor True True Available 11â”œâ”€ Object/providersql-xplane-harbor True True Available 12â”œâ”€ Database/registry True True Available 13â”œâ”€ Role/harbor True True Available 14â”œâ”€ Instance/xplane-harbor-jmdhp-whv4g True True Available 15â””â”€ SubnetGroup/xplane-harbor-jmdhp-fjfth True True Available Et Harbor devient accessible grÃ¢ce Ã  Cilium et Gateway API (Vous pouvez jeter un oeil Ã  un prÃ©cÃ©dent post sur le sujet ğŸ˜‰)\nEnvironmentConfig Les EnvironmentConfigs permettent d'utiliser des variables spÃ©cifiques au cluster local. Ces Ã©lÃ©ments de configuration sont chargÃ©s en mÃ©moire et peuvent ensuite Ãªtre utilisÃ©s dans la composition.\nÃ‰tant donnÃ© que le cluster EKS est crÃ©Ã© avec Opentofu, nous stockons ses propriÃ©tÃ©s par le biais de variables Flux. (plus d'infos sur les variables de substitution ici)\ninfrastructure/base/crossplane/configuration/environmentconfig.yaml\n1apiVersion: apiextensions.crossplane.io/v1alpha1 2kind: EnvironmentConfig 3metadata: 4 name: eks-environment 5data: 6 clusterName: ${cluster_name} 7 oidcUrl: ${oidc_issuer_url} 8 oidcHost: ${oidc_issuer_host} 9 oidcArn: ${oidc_provider_arn} 10 accountId: ${aws_account_id} 11 region: ${region} 12 vpcId: ${vpc_id} 13 CIDRBlock: ${vpc_cidr_block} 14 privateSubnetIds: ${private_subnet_ids} Ces variables peuvent ensuite Ãªtre utilisÃ©es dans les Compositions via la directive FromEnvironmentFieldPath. Par exemple pour permettre aux pods d'accÃ©der Ã  notre instance RDS, nous autorisons le CIDR du VPC:\ninfrastructure/base/crossplane/configuration/irsa-composition.yaml\n1- name: SecurityGroupIngressRule 2 base: 3 apiVersion: ec2.aws.upbound.io/v1beta1 4 kind: SecurityGroupIngressRule 5 spec: 6 forProvider: 7 cidrIpv4: \u0026#34;\u0026#34; 8 patches: 9... 10 - fromFieldPath: CIDRBlock 11 toFieldPath: spec.forProvider.cidrIpv4 12 type: FromEnvironmentFieldPath âš ï¸ A l'heure oÃ¹ j'Ã©cris cet article, la fonctionnalitÃ© est toujours en alpha.\nğŸ› ï¸ Les fonctions de compositions Les fonctions de compositions (Composition Functions) reprÃ©sentent une Ã©volution significative pour le dÃ©veloppement de Compositions. En effet, la mÃ©thode traditionnelle de patching dans une composition prÃ©sentait certaines limitations, telles que l'incapacitÃ© Ã  utiliser des conditions, des boucles dans le code, ou Ã  exÃ©cuter des fonctions avancÃ©es (ex: calcul de sous-rÃ©seaux, vÃ©rification de l'Ã©tat des ressources externes...).\nLes Composition Functions permettent de lever ces limites et sont en rÃ©alitÃ© des programmes qui Ã©tendent les capacitÃ©s de templating des ressources au sein de Crossplane. Elles sont rÃ©digÃ©es dans n'importe quel langage de programmation, offrant ainsi une souplesse et une puissance presque infinies lors de la dÃ©finition des compositions. Cela permet dÃ©sormais des tÃ¢ches complexes telles que les transformations conditionnelles, les itÃ©rations, et les opÃ©rations dynamiques.\nCes fonctions sont exÃ©cutÃ©es de maniÃ¨re sÃ©quentielle (mode Pipeline), chaque fonction manipulant et transformant les ressources, puis transmettant le rÃ©sultat Ã  la fonction suivante, ouvrant ainsi la porte Ã  des combinaisons puissantes.\nMais revenons Ã  notre composition RDS ğŸ” ! Celle-ci utilise, en effet, cette nouvelle faÃ§on de dÃ©finir des Compositions et est composÃ©e de 3 Ã©tapes:\ninfrastructure/base/crossplane/configuration/sql-instance-composition.yaml\n1apiVersion: apiextensions.crossplane.io/v1 2kind: Composition 3metadata: 4 name: xsqlinstances.cloud.ogenki.io 5... 6spec: 7 mode: Pipeline 8... 9 pipeline: 10 - step: patch-and-transform 11 functionRef: 12 name: function-patch-and-transform 13... 14 - step: sql-go-templating 15 functionRef: 16 name: function-go-templating 17... 18 - step: ready 19 functionRef: 20 name: function-auto-ready La syntaxe de la premiÃ¨re Ã©tape patch-and-transform vous parait probablement familiÃ¨re ğŸ˜‰. Il s'agit en effet de la mÃ©thode de patching traditionnelle de Crossplane mais cette fois ci, elle est exÃ©cutÃ©e en tant que fonction dans le Pipeline. La seconde Ã©tape consiste Ã  faire appel Ã  la fonction function-go-templating dont nous allons parler un peu plus loin. Enfin la derniÃ¨re Ã©tape utilise la fonction function-auto-ready qui permet de vÃ©rifier que la ressource composite (XR) est prÃªte. C'est Ã  dire que l'ensemble des ressources qui la compose ont atteint l'Ã©tat Ready Migration Si vous avez dÃ©jÃ  des Compositions dans le format prÃ©cÃ©dent (Patch \u0026amp; Transforms), il existe un super outil qui permet de migrer vers le mode Pipeline: crossplane-migrator\nInstaller crossplane-migrator 1go install github.com/crossplane-contrib/crossplane-migrator@latest Puis lancer la commande suivante qui permettra d'obtenir le bon format dans compostion-pipeline.yaml 1crossplane-migrator new-pipeline-composition --function-name crossplane-contrib-function-patch-and-transform -i composition.yaml -o composition-pipeline.yaml â„¹ï¸ Cet outil devrait Ãªtre ajoutÃ© Ã  la CLI Crossplane en version 1.15\nğŸ¹ Du Go Template dans Crossplane Comme Ã©voquÃ© prÃ©cÃ©demment, la puissance des Composition functions rÃ©side principalement dans le fait que n'importe quel langage peut Ãªtre utilisÃ©. Il est notamment possible de gÃ©nÃ©rer des ressources Ã  partir de templates Go. Cela n'est pas si diffÃ©rent de l'Ã©criture de Charts Helm.\nIl suffit de faire appel Ã  la fonction et lui fournir en entrÃ©e le template permettant de gÃ©nÃ©rer des ressources Kubernetes. Dans la composition SQLInstance, les YAMLs sont gÃ©nÃ©rÃ©s directement en ligne mais il est aussi possible de charger des fichiers locaux (source: Filesystem)\n1 - step: sql-go-templating 2 functionRef: 3 name: function-go-templating 4 input: 5 apiVersion: gotemplating.fn.crossplane.io/v1beta1 6 kind: GoTemplate 7 source: Inline 8 inline: 9 template: | 10 ... Ensuite c'est Ã  vous de jouer! Par exemple il y a peu de diffÃ©rence pour gÃ©nÃ©rer une base de donnÃ©es MariaDB ou PostgreSQL et nous pouvons donc formuler des conditions comme la suivante:\n1{{- $apiVersion := \u0026#34;\u0026#34; }} 2{{- if eq $parameters.engine \u0026#34;postgres\u0026#34; }} 3 {{- $apiVersion = \u0026#34;postgresql.sql.crossplane.io/v1alpha1\u0026#34; }} 4{{- else }} 5 {{- $apiVersion = \u0026#34;mysql.sql.crossplane.io/v1alpha1\u0026#34; }} 6{{- end -}} Cela m'a aussi permit de dÃ©finir une liste de bases de donnÃ©es ainsi que le propriÃ©taire associÃ©\n1apiVersion: cloud.ogenki.io/v1alpha1 2kind: SQLInstance 3metadata: 4... 5spec: 6 parameters: 7... 8 databases: 9 - owner: owner1 10 name: db1 11 - owner: owner2 12 name: db2 Puis d'utiliser des boucles Golang pour les crÃ©er en utilisant le provider SQL.\n1{{- range $parameters.databases }} 2--- 3apiVersion: {{ $apiVersion }} 4kind: Database 5metadata: 6 name: {{ .name | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }} 7 annotations: 8 {{ setResourceNameAnnotation (print \u0026#34;db-\u0026#34; (replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; .name)) }} 9spec: 10... 11{{- end }} Il est mÃªme possible de dÃ©velopper une logique plus complexe dans des fonctions go template avec les directives habituelles define et include. Voici un extrait des exemples disponibles dans le repo de la fonction\n1{{- define \u0026#34;labels\u0026#34; -}} 2some-text: {{.val1}} 3other-text: {{.val2}} 4{{- end }} 5... 6labels: 7 {{- include \u0026#34;labels\u0026#34; $vals | nindent 4}} 8... Enfin nous pouvons tester la Composition et afficher le rendu du template avec la commande suivante:\n1crank beta render tooling/base/harbor/sqlinstance.yaml infrastructure/base/crossplane/configuration/sql-instance-composition.yaml infrastructure/base/crossplane/configuration/function-go-templating.yaml Comme on peut le voir, les possibilitÃ©s sont dÃ©cuplÃ©es grÃ¢ce Ã  la capacitÃ© de construire des ressources en utilisant un langage de programmation. Cependant, il est Ã©galement nÃ©cessaire de veiller Ã  ce que la composition reste lisible et maintenable sur le long terme. Nous assisterons probablement Ã  l'Ã©mergence de bonnes pratiques Ã  mesure que nous gagnons en expÃ©rience sur l'utilisation de ces fonctions.\nğŸ’­ DerniÃ¨res remarques Lorsqu'on parle d'Infrastructure As Code, Terraform est souvent le premier nom qui nous vient Ã  l'esprit. Cet outil, soutenu par une vaste communautÃ© et qui dispose d'un Ã©cosystÃ¨me bien Ã©tabli, reste un choix de premier ordre. Mais il est intÃ©ressant de se demander comment Terraform a Ã©voluÃ© face aux nouveaux paradigmes apportÃ©s par Kubernetes. Nous avons abordÃ© cette question dans notre article sur terraform controller. Depuis, vous avez sÃ»rement remarquÃ© le petit sÃ©isme causÃ© par la dÃ©cision de Hashicorp de passer sous licence BSL. Cette Ã©volution a suscitÃ© de nombreuses rÃ©actions et a peut-Ãªtre influencÃ© la stratÃ©gie et la roadmap d'autres solutions...\nIl est difficile de dire si c'est une rÃ©action directe, mais rÃ©cemment, Crossplane a mis Ã  jour sa charte pour Ã©largir son champ d'action Ã  l'ensemble de l'Ã©cosystÃ¨me (providers, functions), notamment en intÃ©grant le projet Upjet sous l'Ã©gide de la CNCF. L'objectif de cette dÃ©marche est de renforcer la gouvernance des projets associÃ©s et d'amÃ©liorer, en fin de compte, l'expÃ©rience des dÃ©veloppeurs.\nPersonnellement, j'utilise Crossplane depuis un moment pour des cas d'usage spÃ©cifiques. Je l'ai mÃªme mis en production dans une entreprise, utilisant une composition pour dÃ©finir des permissions spÃ©cifiques pour les pods sur EKS (IRSA). Nous avions Ã©galement restreint les types de ressources qu'un dÃ©veloppeur pouvait dÃ©clarer.\nâ“ Alors, que penser de cette nouvelle expÃ©rience avec Crossplane?\nIl faut le dire, les Composition Functions offrent de larges perspectives et on peut s'attendre Ã  voir de nombreuses fonctions apparaÃ®tre en 2024 ğŸš€\nToutefois, Ã  mon avis, il est crucial que les outils de dÃ©veloppement et d'exploitation se perfectionnent pour favoriser l'adoption du projet. Par exemple, une interface web ou un plugin k9s seraient utiles.\nPour un dÃ©butant souhaitant dÃ©velopper une composition ou une fonction, le premier pas peut sembler difficile. La validation d'une composition n'est pas simple, et les exemples Ã  suivre ne sont pas trÃ¨s nombreux. On espÃ¨re que le marketplace s'enrichira avec le temps.\nCela dit, ces prÃ©occupations ont Ã©tÃ© prises en compte par la communautÃ© de Crossplane, notamment par le SIG Dev XP dont il faut fÃ©liciter l'effort et qui mÃ¨ne actuellement un travail important. ğŸ‘\nJe vous encourage Ã  suivre de prÃ¨s l'Ã©volution du projet dans les prochains mois ğŸ‘€, et Ã  tenter l'expÃ©rience Crossplane pour vous faire votre propre idÃ©e. Pour ma part, je suis particuliÃ¨rement intÃ©ressÃ© par la fonction CUElang, un langage sur lequel je prÃ©vois de me pencher prochainement.\nğŸ”– References Crossplane blog: Improve Crossplane Compositions Authoring with go-templating-function Dev XP Roadmap VidÃ©o (Kubecon NA 2023): Crossplane Intro and Deep Dive - the Cloud Native Control Plane Framework VidÃ©o (DevOps Toolkit): Crossplane Composition Functions: Unleashing the Full Potential ","link":"https://blog.ogenki.io/fr/post/crossplane_composition_functions/","section":"post","tags":["infrastructure","devxp","gitops"],"title":"Aller plus loin avec `Crossplane`: Compositions et fonctions"},{"body":"","link":"https://blog.ogenki.io/fr/tags/gitops/","section":"tags","tags":null,"title":"Gitops"},{"body":"","link":"https://blog.ogenki.io/fr/tags/infrastructure/","section":"tags","tags":null,"title":"Infrastructure"},{"body":"","link":"https://blog.ogenki.io/fr/tags/network/","section":"tags","tags":null,"title":"Network"},{"body":"Lorsqu'on parle de sÃ©curisation de l'accÃ¨s aux ressources Cloud, l'une des rÃ¨gles d'or est d'Ã©viter les expositions directes Ã  Internet. La question qui se pose alors pour les Devs/Ops est : comment, par exemple, accÃ©der Ã  une base de donnÃ©es, un cluster Kubernetes ou un serveur via SSH sans compromettre la sÃ©curitÃ©? Les rÃ©seaux privÃ©s virtuels (VPN) offrent une rÃ©ponse en Ã©tablissant un lien sÃ©curisÃ© entre diffÃ©rents Ã©lÃ©ments d'un rÃ©seau, indÃ©pendamment de leur localisation gÃ©ographique. De nombreuses solutions existent, allant de modÃ¨les en SaaS aux solutions que l'on peut hÃ©berger soi-mÃªme, utilisant divers protocoles et Ã©tant soit open source, soit propriÃ©taires.\nParmi ces options, je souhaitais vous parler de Tailscale. Cette solution utilise le protocole WireGuard, rÃ©putÃ© pour sa simplicitÃ© et sa performance. Avec Tailscale, il est possible de connecter des appareils ou serveurs de maniÃ¨re sÃ©curisÃ©e, comme s'ils Ã©taient sur un mÃªme rÃ©seau local, bien qu'ils soient rÃ©partis Ã  travers le monde.\nğŸ¯ Nos objectifs Comprendre comment fonctionne Tailscale Mise en oeuvre d'une connexion sÃ©curisÃ©e avec AWS en quelques minutes Interragir avec l'API d'un cluster EKS via un rÃ©seau privÃ© AccÃ©der Ã  des services hÃ©bergÃ©s sur Kubernetes en utilisant le rÃ©seau privÃ© Pour le reste de cet article il faudra Ã©videmment crÃ©er un compte Tailscale. A noter que l'authentification est dÃ©lÃ©guÃ©e Ã  des fournisseurs d'identitÃ© tiers (ex: Okta, Onelogin, Google ...).\nLorsque le compte est crÃ©e, on a directement accÃ¨s Ã  la console de gestion ci-dessus. Elle permet notamment de lister les appareils connectÃ©s, de consulter les logs, de modifier la plupart des paramÃ¨tres...\nğŸ’¡ Sous le capot Terminologie Mesh VPN: Un mesh VPN est un type de rÃ©seau VPN oÃ¹ chaque nÅ“ud (c'est-Ã -dire chaque appareil ou machine) est connectÃ© Ã  tous les autres nÅ“uds du rÃ©seau, formant ainsi un maillage. Ã€ distinguer des configurations VPN traditionnelles qui sont conÃ§ues gÃ©nÃ©ralement \u0026quot;en Ã©toile\u0026quot;, oÃ¹ plusieurs clients se connectent Ã  un serveur central.\nZero trust: Signifie que chaque demande d'accÃ¨s Ã  un rÃ©seau est traitÃ©e comme si elle venait d'une source non fiable. Une application ou utilisateur doit prouver son identitÃ© et Ãªtre autorisÃ©e avant d'accÃ©der Ã  une ressource. On ne fait pas confiance simplement parce qu'une machine ou un utilisateur provient d'un rÃ©seau interne ou d'une certaine zone gÃ©ographique.\nTailnet: DÃ¨s la premiÃ¨re utilisation de Tailscale, un Tailnet est crÃ©e pour vous et correspond Ã  votre propre rÃ©seau privÃ©. Chaque appareil dans un tailnet reÃ§oit une IP Tailscale unique, permettant une communication directe entre eux.\nL'architecture de Tailscale est conÃ§ue de telle sorte que le Control plane et le Data plane sont clairement sÃ©parÃ©s:\nD'une part, il y a le serveur de coordination. Son rÃ´le est d'Ã©changer des mÃ©tadonnÃ©es et des clÃ©s publiques entre tous les participants d'un Tailnet (La clÃ© privÃ©e Ã©tant gardÃ©e en toute sÃ©curitÃ© son nÅ“ud d'origine).\nD'autre part, les nÅ“uds du Tailnet s'organisent en un rÃ©seau maillÃ© (Mesh). Au lieu de passer par le serveur de coordination pour Ã©changer des donnÃ©es, ces nÅ“uds communiquent directement les uns avec les autres en mode point Ã  point. Chaque nÅ“ud dispose d'une identitÃ© unique pour s'authentifier et rejoindre le Tailnet.\n\u0026#x1f4e5; Installation du client La majoritÃ© des plateformes sont supportÃ©es et les procÃ©dures d'installation sont listÃ©es ici. En ce qui me concerne je suis sur Archlinux:\n1sudo pacman -S tailscale Il est possible de dÃ©marrer le service automatiquement au dÃ©marrage de la machine.\n1sudo systemctl enable --now tailscaled Pour enregistrer son ordinateur perso, lancer la commande suivante:\n1sudo tailscale up --accept-routes 2 3To authenticate, visit: 4 5 https://login.tailscale.com/a/f50... â„¹ï¸ l'option --accept-routes est nÃ©cessaire sur Linux et permettra d'accepter les routes annoncÃ©es par les Subnet routers. On verra cela dans la suite de l'article\nVÃ©rifier que vous avez bien obtenu une IP du rÃ©seau Tailscale:\n1tailscale ip -4 2100.118.83.67 3 4tailscale status 5100.118.83.67 ogenki smainklh@ linux - â„¹ï¸ Pour les utilisateurs de Linux, vÃ©rifier que Tailscale fonctionne bien avec votre configuration DNS: Suivre cette documentation.\nLes sources Toutes les Ã©tapes rÃ©alisÃ©es dans cet article proviennent de ce dÃ©pÃ´t git\nIl va permettre de crÃ©er l'ensemble des composants qui ont pour objectif d'obtenir un cluster EKS de Lab et font suite Ã  un prÃ©cÃ©dent article sur Cilium et Gateway API.\nâ˜ï¸ AccÃ©der Ã  AWS en privÃ© Afin de pouvoir accÃ©der de maniÃ¨re sÃ©curisÃ©e Ã  l'ensemble des ressources disponibles sur AWS, il est possible de dÃ©ployer un Subnet router.\nUn Subnet router est une instance Tailscale qui permet d'accÃ©der Ã  des sous-rÃ©seaux qui ne sont pas directement liÃ©s Ã  Tailscale. Il fait office de pont entre le rÃ©seau privÃ© virtuel de Tailscale (Tailnet) et d'autres rÃ©seaux locaux.\nNous pouvons alors router des sous rÃ©seaux du Clouder Ã  travers le VPN de Tailscale.\nâš ï¸ Pour ce faire, sur AWS, il faudra bien entendu configurer les security groups correctement pour autoriser les Subnet routers.\nğŸš€ DÃ©ployer un Subnet router Entrons dans le vif du sujet et deployons un Subnet router sur un rÃ©seau AWS! Tout est fait en utilisant le code Terraform prÃ©sent dans le rÃ©pertoire opentofu/network. Nous allons analyser la configuration spÃ©cifique Ã  Tailscale qui est prÃ©sente dans le fichier tailscale.tf avant de procÃ©der au dÃ©ploiement.\nLe provider Terraform Il est possible de configurer certains paramÃ¨tres au travers de l'API Tailscale grÃ¢ce au provider Terraform. Pour cela il faut au prÃ©alable gÃ©nerer une clÃ© d'API ğŸ”‘ sur la console d'admin:\nIl faudra conserver cette clÃ© dans un endroit sÃ©curisÃ© car elle est utilisÃ©e pour dÃ©ployer le Subnet router\n1provider \u0026#34;tailscale\u0026#34; { 2 api_key = var.tailscale.api_key 3 tailnet = var.tailscale.tailnet 4} les ACL's\nLes ACL's permettent de dÃ©finir qui est autorisÃ© Ã  communiquer avec qui (utilisateur ou appareil). Ã€ la crÃ©ation d'un compte, celle-cis sont trÃ¨s permissives et il n'y a aucune restriction (tout le monde peut parler avec tout le monde).\n1resource \u0026#34;tailscale_acl\u0026#34; \u0026#34;this\u0026#34; { 2 acl = jsonencode({ 3 acls = [ 4 { 5 action = \u0026#34;accept\u0026#34; 6 src = [\u0026#34;*\u0026#34;] 7 dst = [\u0026#34;*:*\u0026#34;] 8 } 9 ] 10... 11} Note Pour mon environnement de Lab, j'ai conservÃ© cette configuration par dÃ©fault car je suis la seule personne Ã  y accÃ©der. De plus les seuls appareils connectÃ©s Ã  mon Tailnet sont mon laptop et le Subnet router. En revanche dans un cadre d'entreprise, il faudra bien y rÃ©flÃ©chir. Il est alors possible de dÃ©finir une politique basÃ©e sur des groupes d'utilisitateurs ou sur les tags des noeuds.\nConsulter cette doc pour plus d'info.\nLes noms de domaines (DNS)\nIl y a diffÃ©rentes faÃ§ons possibles de gÃ©rer les noms de domaines avec Tailscale:\nMagic DNS: Lorsqu'un appareil rejoint le Tailnet, il s'enregistre avec un nom et celui-ci peut-Ãªtre utilisÃ© directement pour communiquer avec l'appareil.\n1tailscale status 2100.118.83.67 ogenki smainklh@ linux - 3100.115.31.152 ip-10-0-43-98 smainklh@ linux active; relay \u0026#34;par\u0026#34;, tx 3044 rx 2588 4 5ping ip-10-0-43-98 6PING ip-10-0-43-98.tail9c382.ts.net (100.115.31.152) 56(84) bytes of data. 764 bytes from ip-10-0-43-98.tail9c382.ts.net (100.115.31.152): icmp_seq=1 ttl=64 time=11.4 ms AWS: Pour utiliser les noms de domaines internes Ã  AWS il est possible d'utiliser la deuxiÃ¨me IP du VPC qui correspond toujours au serveur DNS. Cela permet d'utiliser les Ã©ventuelles zones privÃ©es sur route53 ou de se connecter aux ressources en utilisant les noms de domaines.\nLa configuration la plus simple est donc de dÃ©clarer la liste des serveurs DNS Ã  utiliser et d'y ajouter celui de AWS. Ici un exemple avec le DNS publique de Cloudflare.\n1resource \u0026#34;tailscale_dns_nameservers\u0026#34; \u0026#34;this\u0026#34; { 2 nameservers = [ 3 \u0026#34;1.1.1.1\u0026#34;, 4 cidrhost(module.vpc.vpc_cidr_block, 2) 5 ] 6} La clÃ© d'authentification (\u0026quot;auth key\u0026quot;)\nPour qu'un appareil puisse rejoindre le Tailnet au dÃ©marrage il faut que Tailscale soit dÃ©marrÃ© en utilisant une clÃ© d'authentification. Celle-ci est gÃ©nÃ©rÃ©e comme suit\n1resource \u0026#34;tailscale_tailnet_key\u0026#34; \u0026#34;this\u0026#34; { 2 reusable = true 3 ephemeral = false 4 preauthorized = true 5} reusable: S'agissant d'un autoscaling group, il faut que cette mÃªme clÃ© puisse Ãªtre utilisÃ©e plusieurs fois. ephemeral: Pour cette dÃ©mo nous crÃ©ons une clÃ© qui n'expire pas. En production il serait prÃ©fÃ©rable d'activer l'expiration. preauthorized: Il faut que cette clÃ© soit dÃ©jÃ  valide et autorisÃ©e pour que l'instance rejoigne automatiquement le Tailscale. La clÃ© ainsi gÃ©nÃ©rÃ©e est utilisÃ©e pour lancer tailscale avec le paramÃ¨tre --auth-key\n1sudo tailscale up --authkey=\u0026lt;REDACTED\u0026gt; Annoncer les routes pour les rÃ©seaux AWS\nEnfin il faut annoncer le rÃ©seau que l'on souhaite faire passer par le Subnet router. Dans notre exemple, nous dÃ©cidons de router tout le rÃ©seau du VPC qui a pour CIDR 10.0.0.0/16.\nAfin que cela soit possible de faÃ§on automatique, il y a une rÃ¨gle autoApprovers Ã  ajouter. Cela permet d'indiquer que les routes annoncÃ©es par l'utilisateur smainklh@gmail.com sont autorisÃ©es sans que cela requiert une Ã©tape d'approbation.\n1 autoApprovers = { 2 routes = { 3 \u0026#34;10.0.0.0/16\u0026#34; = [\u0026#34;smainklh@gmail.com\u0026#34;] 4 } 5 } La commande lancÃ©e au dÃ©marrage de l'instance Subnet router est la suivante:\n1sudo tailscale up --authkey=\u0026lt;REDACTED\u0026gt; --advertise-routes=\u0026#34;10.0.0.0/16\u0026#34; Le module Terraform J'ai crÃ©Ã© un module trÃ¨s simple qui permet de dÃ©ployer un autoscaling group sur AWS et de configurer Tailscale. Au dÃ©marrage de l'instance, elle s'authentifiera en utilisant une auth_key et annoncera les rÃ©seaux indiquÃ©s. Dans l'exemple ci-dessous l'instance annonce le CIDR du VPC sur AWS.\n1module \u0026#34;tailscale_subnet_router\u0026#34; { 2 source = \u0026#34;Smana/tailscale-subnet-router/aws\u0026#34; 3 version = \u0026#34;1.0.4\u0026#34; 4 5 region = var.region 6 env = var.env 7 8 name = var.tailscale.subnet_router_name 9 auth_key = tailscale_tailnet_key.this.key 10 11 vpc_id = module.vpc.vpc_id 12 subnet_ids = module.vpc.private_subnets 13 advertise_routes = [module.vpc.vpc_cidr_block] 14... 15} Maintenant que nous avons analysÃ© les diffÃ©rents paramÃ¨tres, il est temps de dÃ©marrer notre Subnet router ğŸš€ !! Il faut au prÃ©alable crÃ©er un fichier variable.tfvars dans le rÃ©pertoire opentofu/network.\n1env = \u0026#34;dev\u0026#34; 2region = \u0026#34;eu-west-3\u0026#34; 3private_domain_name = \u0026#34;priv.cloud.ogenki.io\u0026#34; 4 5tailscale = { 6 subnet_router_name = \u0026#34;ogenki\u0026#34; 7 tailnet = \u0026#34;smainklh@gmail.com\u0026#34; 8 api_key = \u0026#34;tskey-api-...\u0026#34; 9} 10 11tags = { 12 project = \u0026#34;demo-cloud-native-ref\u0026#34; 13 owner = \u0026#34;Smana\u0026#34; 14} Puis lancer la commande suivante:\n1tofu plan --var-file variables.tfvars AprÃ¨s vÃ©rification du plan, appliquer les changements\n1tofu apply --var-file variables.tfvars Quand l'instance est dÃ©marrÃ©e, elle apparaitra dans la liste des appareils du Tailnet.\n1tailscale status 2100.118.83.67 ogenki smainklh@ linux - 3100.68.109.138 ip-10-0-26-99 smainklh@ linux active; relay \u0026#34;par\u0026#34;, tx 33868 rx 32292 Nous pouvons aussi vÃ©rifier que la route est bien annoncÃ©e comme suit:\n1tailscale status --json|jq \u0026#39;.Peer[] | select(.HostName == \u0026#34;ip-10-0-26-99\u0026#34;) .PrimaryRoutes\u0026#39; 2[ 3 \u0026#34;10.0.0.0/16\u0026#34; 4] âš ï¸ Pour des raisons de sÃ©curitÃ©, pensez Ã  supprimer le fichier variables.tfvars car il contient la clÃ© d'API.\nğŸ‘ Et voilÃ  ! Nous sommes maintenant en mesure d'accÃ©der au rÃ©seau sur AWS, Ã  condition d'avoir Ã©galement configurÃ© les rÃ¨gles de filtrage, comme les ACL et les security groups. Nous pouvons par exemple accÃ©der Ã  une base de donnÃ©es depuis le poste de travail\n1psql -h demo-tailscale.cymnaynfchjt.eu-west-3.rds.amazonaws.com -U postgres 2Password for user postgres: 3psql (15.4, server 15.3) 4SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, compression: off) 5Type \u0026#34;help\u0026#34; for help. 6 7postgres=\u0026gt; ğŸ’» Une autre faÃ§on de faire du SSH Traditionnellement, nous devons parfois nous connecter Ã  des serveurs en utilisant le protocole SSH. Pour ce faire, il faut gÃ©nÃ©rer une clÃ© privÃ©e et distribuer la clÃ© publique correspondante sur les serveurs distants.\nContrairement Ã  l'utilisation des clÃ©s SSH classiques, Ã©tant donnÃ© que Tailscale utilise Wireguard pour l'authentification et le chiffrement des connexions il n'est pas nÃ©cessaire de rÃ©-authentifier le client. De plus, Tailscale gÃ¨re Ã©galement la distribution des clÃ©s SSH d'hÃ´tes. Les rÃ¨gles ACL permettent de rÃ©voquer l'accÃ¨s des utilisateurs sans avoir Ã  supprimer les clÃ©s SSH. De plus, il est possible d'activer un mode de vÃ©rification qui renforce la sÃ©curitÃ© en exigeant une rÃ©-authentification pÃ©riodique. On peut donc affirmer que l'utilisation de Tailscale SSH simplifie l'authentification, la gestion des connexions SSH et amÃ©liore le niveau de sÃ©curitÃ©.\nLes autorisations pour utiliser SSH sont aussi gÃ©rÃ©es au niveau des ACL's\n1... 2 ssh = [ 3 { 4 action = \u0026#34;check\u0026#34; 5 src = [\u0026#34;autogroup:member\u0026#34;] 6 dst = [\u0026#34;autogroup:self\u0026#34;] 7 users = [\u0026#34;autogroup:nonroot\u0026#34;] 8 } 9 ] 10... La rÃ¨gle ci-dessus autorise tous les utilisateurs Ã  accÃ©der Ã  leurs propres appareils en utilisant SSH. Lorsqu'ils essaient de se connecter, ils doivent utiliser un compte utilisateur autre que root. Pour chaque tentative de connexion, une authentification supplÃ©mentaire est nÃ©cessaire (action=check). Cette authentification se fait en visitant un lien web spÃ©cifique\n1ssh ubuntu@ip-10-0-26-99 2... 3# Tailscale SSH requires an additional check. 4# To authenticate, visit: https://login.tailscale.com/a/f1f09a548cc6 5... 6ubuntu@ip-10-0-26-99:~$ Pour que cela soit possible il faut aussi dÃ©marrer Tailscale avec l'option --ssh\nLes logs d'accÃ¨s Ã  la machine peuvent Ãªtre consultÃ©s en utilisant journalctl\n1ubuntu@ip-10-0-26-99:~$ journalctl -aeu tailscaled|grep ssh 2Oct 15 15:51:34 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155130-00ede660b8: handling conn: 100.118.83.67:55098-\u0026gt;ubuntu@100.68.109.138:22 3Oct 15 15:51:56 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155156-b6d1dc28c0: handling conn: 100.118.83.67:44560-\u0026gt;ubuntu@100.68.109.138:22 4Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155156-b6d1dc28c0: starting session: sess-20231015T155252-5b2acc170e 5Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): handling new SSH connection from smainklh@gmail.com (100.118.83.67) to ssh-user \u0026#34;ubuntu\u0026#34; 6Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): access granted to smainklh@gmail.com as ssh-user \u0026#34;ubuntu\u0026#34; 7Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): starting pty command: [/usr/sbin/tailscaled be-child ssh --uid=1000 --gid=1000 --groups=1000,4,20,24,25,27,29,30,44,46,115,116 --local-user=ubuntu --remote-user=smainklh@gmail.com --remote-ip=100.118.83.67 --has-tty=true --tty-name=pts/0 --shell --login-cmd=/usr/bin/login --cmd=/bin/bash -- -l] â„¹ï¸ Avec Tailscale SSH il est possible de se connecter en SSH peu importe oÃ¹ est situÃ© l'appareil. En revanche dans un contexte 100% AWS, on prÃ©ferera probablement utiliser AWS SSM.\nLogs ğŸ’¾ En sÃ©curitÃ© il est primordial de pouvoir conserver les logs pour un usage ultÃ©rieur. Il existe diffÃ©rents types de logs:\nLogs d'audit: Ils sont essentiels pour savoir qui a fait quoi. Ils sont accessibles sur la console d'admin et peuvent aussi Ãªtre envoyÃ©s vers un SIEM.\nLogs sur les appareils: Ceux-cis peuvent Ãªtre consultÃ©s en utilisant les commandes appropriÃ©es Ã  l'appareil. (journalctl -u tailscaled sur Linux)\nLogs rÃ©seau: Utiles pour visualiser quels appareils sont connectÃ©s les uns aux autres.\nâ˜¸ Qu'en est-il de Kubernetes? Sur Kubernetes il existe plusieurs options pour accÃ©der Ã  un Service:\nProxy: Il s'agit d'un pod supplÃ©mentaire qui transfert les appels Ã  un Service existant. Sidecar: Permet de connecter le pod au Tailnet. Donc la connectivitÃ© se fait de bout en bout et il est mÃªme possible de communiquer dans les 2 sens. (du pod vers les noeuds du Tailnet). Operator: Permet d'exposer les services et l'API Kubernetes (ingress) ainsi que de permettre aux pods d'accÃ©der aux noeuds du Tailnet (egress). La configuration se fait en configurant les ressources existantes: Services et Ingresses Dans notre cas, nous disposons dÃ©jÃ  d'un Subnet router qui route tout le rÃ©seau du VPC. Il suffit donc que notre service soit exposÃ© sur une IP privÃ©e.\nL'API Kubernetes Pour accÃ©der Ã  l'API Kubernetes il est nÃ©cessaire d'autoriser le Subnet router. Cela se fait en dÃ©finissant la rÃ¨gle suivante pour le security group source.\n1module \u0026#34;eks\u0026#34; { 2... 3 cluster_security_group_additional_rules = { 4 ingress_source_security_group_id = { 5 description = \u0026#34;Ingress from the Tailscale security group to the API server\u0026#34; 6 protocol = \u0026#34;tcp\u0026#34; 7 from_port = 443 8 to_port = 443 9 type = \u0026#34;ingress\u0026#34; 10 source_security_group_id = data.aws_security_group.tailscale.id 11 } 12 } 13... 14} Nous allons vÃ©rifier que l'API est bien accessible sur une IP privÃ©e.\n1CLUSTER_URL=$(TERM=dumb kubectl cluster-info | grep \u0026#34;Kubernetes control plane\u0026#34; | awk \u0026#39;{print $NF}\u0026#39;) 2 3curl -s -o /dev/null -w \u0026#39;%{remote_ip}\\n\u0026#39; ${CLUSTER_URL} 410.228.244.167 5 6kubectl get ns 7NAME STATUS AGE 8cilium-secrets Active 5m46s 9crossplane-system Active 4m1s 10default Active 23m 11flux-system Active 5m29s 12infrastructure Active 4m1s 13... AccÃ©der aux services en privÃ© Un Service Kubernetes exposÃ© est une resource AWS comme une autre ğŸ˜‰. Il faut juste s'assurer que ce service utilise bien une IP privÃ©e. Dans mon exemple j'utilise Gateway API pour configurer la rÃ©partition de charge du Clouder et je vous invite Ã  lire mon prÃ©cÃ©dent article sur le sujet.\nIl suffirait donc de crÃ©er un NLB interne en s'assurant que le Service ait bien l'annotation service.beta.kubernetes.io/aws-load-balancer-scheme ayant pour valeur internal. Dans le cas de Gateway API, cela se fait via la clusterPolicy Kyverno.\n1 metadata: 2 annotations: 3 external-dns.alpha.kubernetes.io/hostname: gitops-${cluster_name}.priv.${domain_name},grafana-${cluster_name}.priv.${domain_name} 4 service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internal\u0026#34; 5 service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp 6 spec: 7 loadBalancerClass: service.k8s.aws/nlb Il y a cependant un prÃ©requis supplÃ©mentaire car nous ne pouvons pas utiliser Let's Encrypt pour les certificats internes. J'ai donc gÃ©nÃ©rÃ© une PKI interne qui gÃ©nÃ¨re des certificates auto-signÃ©s avec Cert-manager.\nIci je ne dÃ©taillerai pas le dÃ©ploiement du cluster EKS, ni la configuration de Flux. Lorsque le cluster est crÃ©Ã© et que toutes les ressources Kubernetes ont Ã©tÃ© rÃ©conciliÃ©, nous avons un service qui est exposÃ© via un LoadBalancer interne AWS.\n1NLB_DOMAIN=$(kubectl get svc -n infrastructure cilium-gateway-platform -o jsonpath={.status.loadBalancer.ingress[0].hostname}) 2dig +short ${NLB_DOMAIN} 310.0.33.5 410.0.26.228 510.0.9.183 Une entrÃ©e DNS est Ã©galement crÃ©Ã©e automatiquement pour les services exposÃ©s et nous pouvons donc accÃ©der en privÃ© grÃ¢ce Ã  Tailscale.\n1dig +short gitops-mycluster-0.priv.cloud.ogenki.io 210.0.9.183 310.0.26.228 410.0.33.5 ğŸ’­ DerniÃ¨res remarques Il y a quelques temps, dans le cadre professionnel, j'ai mis en place Cloudflare Zero Trust. Je dÃ©couvre ici que Tailscale prÃ©sente de nombreuses similitudes avec cette solution. La dÃ©cision entre les deux est loin d'Ãªtre triviale et dÃ©pend grandement du contexte. Pour ma part, j'ai Ã©tÃ© particuliÃ¨rement convaincu par la simplicitÃ© de mise en Å“uvre de Tailscale, rÃ©pondant parfaitement Ã  mon besoin d'accÃ©der au rÃ©seau du Clouder. Bien entendu il existe d'autres solutions comme Teleport, qui offre une approche diffÃ©rente pour accÃ©der Ã  des ressources internes.\nCela dit, focalisons-nous sur Tailscale.\nUne partie du code de Tailscale est open source, notamment le client qui est sous license BSD 3-Clause. La partie propriÃ©taire concerne Ã©ssentiellement la plateforme de coordination. Ã€ noter qu'il existe une alternative open source nommÃ©e Headscale. Celle-ci est une initiative distincte qui n'a aucun lien avec la sociÃ©tÃ© Tailscale.\nPour un usage personnel, Tailscale est vraiment gÃ©nÃ©reux, offrant un accÃ¨s gratuit pour jusqu'Ã  100 appareils et 3 utilisateurs. Ceci-dit Tailscale est une option sÃ©rieuse Ã  considÃ©rer en entreprise et il est important, selon moi, d'encourager ce type d'entreprises qui ont une politique open source claire et un produit de qualitÃ©.\n","link":"https://blog.ogenki.io/fr/post/tailscale/","section":"post","tags":["security","network"],"title":"SÃ©curiser le Cloud avec `Tailscale` : Mise en Å“uvre d'un VPN simplifiÃ©e"},{"body":"Lorsque l'on dÃ©ploie une application sur Kubernetes, l'Ã©tape suivante consiste gÃ©nÃ©ralement Ã  l'exposer aux utilisateurs. On utilise habituellement des \u0026quot;Ingress controllers\u0026quot;, comme Nginx, Haproxy, Traefik ou encore ceux des diffÃ©rents Clouders afin de diriger le trafic entrant vers l'application, gÃ©rer l'Ã©quilibrage de charge, la terminaison TLS et j'en passe.\nIl faut alors choisir parmi la plÃ©thore d'options disponibles ğŸ¤¯ la solution qui sera en charge de tous ces aspects et Cilium est, depuis relativement rÃ©cemment, l'une d'entre elles.\nCilium est une solution Open-Source de connectivitÃ© rÃ©seau et de sÃ©curitÃ© basÃ©e sur eBPF dont l'adoption est grandissante. Il s'agit probablement du plugin rÃ©seau qui fournit le plus de fonctionnalitÃ©s. Nous n'allons pas toutes les parcourir mais l'une d'entre elles consiste Ã  gÃ©rer le trafic entrant en utilisant le standard Gateway API (GAPI).\nğŸ¯ Notre objectif Comprendre ce qu'est exactement Gateway API et en quoi il s'agit d'une Ã©volution par rapport Ã  l'API Ingress. DÃ©monstrations de cas concrets Ã  la sauce GitOps. Les limitations actuelles et les Ã©volutions Ã  venir. Tip Toutes les Ã©tapes rÃ©alisÃ©es dans cet article proviennent de ce dÃ©pÃ´t git\nJe vous invite Ã  le parcourir car, il va bien au delÃ  du contexte de cet article:\nInstallation d'un cluster EKS avec Cilium configurÃ© en mode sans kube-proxy et un Daemonset dÃ©diÃ© Ã  Envoy Proposition de structure de configuration Flux avec une gestion des dÃ©pendances et une factorisation que je trouve efficaces. Crossplane et composition IRSA qui simplifie la gestion des permissions IAM pour les composants plateforme Gestion des noms de domaine ainsi que des certificats automatisÃ©s avec External-DNS et Let's Encrypt L'idÃ©e Ã©tant d'avoir l'ensemble configurÃ© au bout de quelques minutes, en une seule ligne de commande ğŸ¤©.\nâ˜¸ Introduction Ã  Gateway API Comme Ã©voquÃ© prÃ©cÃ©demment, il y a de nombreuses options qui font office d' Ingress controller et chacune a ses propres spÃ©cificitÃ©s et des fonctionnalitÃ©s particuliÃ¨res, rendant leur utilisation parfois complexe. Par ailleurs, l'API Ingress, utilisÃ©e historiquement dans Kubernetes possÃ¨de trÃ¨s peu d'options. Certaines solutions ont d'ailleurs crÃ©Ã© des CRDs (Ressources personalisÃ©es) quand d'autres font usage des annotations pour lever ces limites.\nC'est dans ce contexte que Gateway API fait son apparition. Il s'agit d'un standard qui permet de dÃ©finir des fonctionnalitÃ©s rÃ©seau avancÃ©es sans nÃ©cessiter d'extensions spÃ©cifiques au contrÃ´leur sous-jacent. De plus Ã©tant donnÃ© que tous les contrÃ´leurs utilisent la mÃªme API , il est possible de passer d'une solution Ã  une autre sans changer de configuration (les ressources qui gÃ¨rent le trafic entrant restent les mÃªmes).\nParmi les concepts que nous allons explorer la GAPI introduit un schÃ©ma de rÃ©partition des responsabilitÃ©s. Elle dÃ©finit des roles explicites avec des permissions bien distinctes. (Plus d'informations sur le modÃ¨le de sÃ©curitÃ© GAPI ici).\nEnfin il est important de noter que ce projet est dirigÃ© par le groupe de travail sig-network-kubernetes et un canal slack vous permettra de les solliciter si nÃ©cessaire.\nVoyons comment cela s'utilise concrÃ¨tement ğŸš€!\n\u0026#x2611;\u0026#xfe0f; PrÃ©requis Pour le reste de cet article nous considÃ©rons qu'un cluster EKS a Ã©tÃ© dÃ©ployÃ©. Si vous n'utilisez pas la mÃ©thode proposÃ©e dans le repo de dÃ©mo servant de socle Ã  cet article, il y a certains points Ã  valider pour que GAPI puisse Ãªtre utilisÃ©.\nâ„¹ï¸ La mÃ©thode d'installation decrite ici se base sur Helm, l'ensemble des values peuvent Ãªtre consultÃ©es ici.\nInstaller les CRDs (resources personnalisÃ©s) disponibles dans le repository Gateway API Note Si Cilium est configurÃ© avec le support GAPI (voir ci-dessous) et que les CRDs sont absentes, il ne dÃ©marrera pas.\nDans le repo de demo les CRDs GAPI sont installÃ©es une premiÃ¨re fois lors de la crÃ©ation du cluster afin que Cilium puisse dÃ©marrer puis elles sont ensuite gÃ©rÃ©es par Flux.\nRemplacer kube-proxy par les fonctionnalitÃ©s de transfert rÃ©seau apportÃ©es par Cilium et eBPF.\n1kubeProxyReplacement: true Activer le support de Gateway API 1gatewayAPI: 2 enabled: true VÃ©rifier L'installation Il faut pour cela installer le client en ligne de commande cilium. J'utilise personnellement asdf pour cela:\n1asdf plugin-add cilium-cli 2asdf install cilium-cli 0.15.7 3asdf global cilium 0.15.7 La commande suivante permet de s'assurer que tous les composants sont dÃ©marrÃ©s et opÃ©rationnels\n1cilium status --wait 2 /Â¯Â¯\\ 3/Â¯Â¯\\__/Â¯Â¯\\ Cilium: OK 4\\__/Â¯Â¯\\__/ Operator: OK 5/Â¯Â¯\\__/Â¯Â¯\\ Envoy DaemonSet: OK 6\\__/Â¯Â¯\\__/ Hubble Relay: disabled 7 \\__/ ClusterMesh: disabled 8 9Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 10DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 11DaemonSet cilium-envoy Desired: 2, Ready: 2/2, Available: 2/2 12Containers: cilium Running: 2 13 cilium-operator Running: 2 14 cilium-envoy Running: 2 15Cluster Pods: 33/33 managed by Cilium 16Helm chart version: 1.14.2 17Image versions cilium quay.io/cilium/cilium:v1.14.2@sha256:6263f3a3d5d63b267b538298dbeb5ae87da3efacf09a2c620446c873ba807d35: 2 18 cilium-operator quay.io/cilium/operator-aws:v1.14.2@sha256:8d514a9eaa06b7a704d1ccead8c7e663334975e6584a815efe2b8c15244493f1: 2 19 cilium-envoy quay.io/cilium/cilium-envoy:v1.25.9-e198a2824d309024cb91fb6a984445e73033291d@sha256:52541e1726041b050c5d475b3c527ca4b8da487a0bbb0309f72247e8127af0ec: 2 Enfin le support de GAPI peut Ãªtre vÃ©rifiÃ© comme suit\n1cilium config view | grep -w \u0026#34;enable-gateway-api\u0026#34; 2enable-gateway-api true 3enable-gateway-api-secrets-sync true Il est aussi possible de lancer des tests de connectivitÃ© pour s'assurer qu'il n'y a pas de problÃ¨mes avec la configuration rÃ©seau du cluster:\n1cilium connectivity test âš ï¸ Cette commande (connectivity test) provoque actuellement des erreurs lors de l'activation d'Envoy en tant que DaemonSet. (Issue Github).\nInfo as DaemonSet\nPar dÃ©faut l'agent cilium intÃ©gre Envoy et lui dÃ©legue les opÃ©rations rÃ©seau de niveau 7. Depuis la version v1.14, il est possible de dÃ©ployer Envoy sÃ©parÃ©ment ce qui apporte certains avantages:\nSi l'on modifie/redÃ©marre un composant (que ce soit Cilium ou Envoy), cela n'affecte pas l'autre. Mieux attribuer les ressources Ã  chacun des composants afin d'optimiser les perfs. Limite la surface d'attaque en cas de compromission d'un des pods. Les logs Envoy et de l'agent Cilium ne sont pas mÃ©langÃ©s Il est possible d'utiliser la commande suivante pour vÃ©rifier que cette fonctionnalitÃ© est bien active:\n1cilium status 2 /Â¯Â¯\\ 3 /Â¯Â¯\\__/Â¯Â¯\\ Cilium: OK 4 \\__/Â¯Â¯\\__/ Operator: OK 5 /Â¯Â¯\\__/Â¯Â¯\\ Envoy DaemonSet: OK 6 \\__/Â¯Â¯\\__/ Hubble Relay: disabled 7 \\__/ ClusterMesh: disabled Plus d'information.\nğŸšª La porte d'entrÃ©e: GatewayClass et Gateway Une fois les conditions nÃ©cessaires remplies, nous avons accÃ¨s Ã  plusieurs Ã©lÃ©ments. Nous pouvons notamment utiliser les ressources de la Gateway API grÃ¢ce aux CRDs. D'ailleurs, dÃ¨s l'installation de Cilium, une GatewayClass est directement disponible.\n1kubectl get gatewayclasses.gateway.networking.k8s.io 2NAME CONTROLLER ACCEPTED AGE 3cilium io.cilium/gateway-controller True 7m59s Sur un cluster il est possible de configurer plusieurs GatewayClass et donc d'avoir la possibilitÃ© de faire usage de diffÃ©rentes implÃ©mentations. Nous pouvons par exemple utiliser Linkerd en rÃ©fÃ©rencant la GatewayClass dans la configuration de la Gateway.\nLa Gateway est la ressource qui permet dÃ©clencher la crÃ©ation de composants de rÃ©partition de charge chez le Clouder.\nVoici un exemple simple: apps/base/echo/gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: echo-gateway 5 namespace: echo 6spec: 7 gatewayClassName: cilium 8 listeners: 9 - protocol: HTTP 10 port: 80 11 name: echo-1-echo-server 12 allowedRoutes: 13 namespaces: 14 from: Same Sur AWS (EKS), quand on configure une Gateway, Cilium crÃ©e un Service de type LoadBalancer. Ce service est alors interprÃ©tÃ© par un autre contrÃ´leur, l'(AWS Load Balancer Controler), qui produit un NLB.\n1kubectl get svc -n echo cilium-gateway-echo 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3cilium-gateway-echo LoadBalancer 172.20.19.82 k8s-echo-ciliumga-64708ec85c-fcb7661f1ae4e4a4.elb.eu-west-3.amazonaws.com 80:30395/TCP 2m58s Il est intÃ©ressant de noter que l'adresse en question est aussi associÃ©e Ã  la resource Gateway.\n1kubectl get gateway -n echo echo 2NAME CLASS ADDRESS PROGRAMMED AGE 3echo cilium k8s-echo-ciliumga-64708ec85c-fcb7661f1ae4e4a4.elb.eu-west-3.amazonaws.com True 16m \u0026#x21aa;\u0026#xfe0f; Les rÃ¨gles de routage: HTTPRoute Un routage simple Pour rÃ©sumer le schÃ©ma ci-dessus en quelques mots: Une ressource HTTPRoute permet de configurer le routage vers le service en rÃ©fÃ©rencant la gateway et en dÃ©finissant le les paramÃ¨tres de routage souhaitÃ©s.\nNote workaround\nÃ€ ce jour, il n'est pas possible de configurer les annotations des services gÃ©nÃ©rÃ©s par les Gateways (Issue Github). Une solution de contournement a Ã©tÃ© proposÃ© afin de modifier le service gÃ©nÃ©rÃ© par la Gateway dÃ¨s lors qu'il est crÃ©Ã©.\nKyverno est un outil qui permet de garantir la conformitÃ© des configurations par rapport aux bonnes pratiques et aux exigences de sÃ©curitÃ©. Nous utilisons ici uniquement sa capacitÃ© Ã  dÃ©crire une rÃ¨gle de mutation facilement.\nsecurity/mycluster-0/echo-gw-clusterpolicy.yaml\n1spec: 2 rules: 3 - name: mutate-svc-annotations 4 match: 5 any: 6 - resources: 7 kinds: 8 - Service 9 namespaces: 10 - echo 11 name: cilium-gateway-echo 12 mutate: 13 patchStrategicMerge: 14 metadata: 15 annotations: 16 external-dns.alpha.kubernetes.io/hostname: echo.${domain_name} 17 service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; 18 service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp 19 spec: 20 loadBalancerClass: service.k8s.aws/nlb Le service cilium-gateway-echo se verra donc ajouter les annotations du contrÃ´leur AWS ainsi qu'une annotation permettant de configurer une entrÃ©e DNS automatiquement.\napps/base/echo/httproute.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7 parentRefs: 8 - name: echo 9 namespace: echo 10 rules: 11 - matches: 12 - path: 13 type: PathPrefix 14 value: / 15 backendRefs: 16 - name: echo-1-echo-server 17 port: 80 L'exemple utilisÃ© ci-dessus est simpliste: toutes les requÃªtes sont transfÃ©rÃ©es au service echo-1-echo-server.\nparentRefs permet d'indiquer la Gateway Ã  utiliser puis les rÃ¨gles de routage sont dÃ©finies dans rules.\nLes rÃ¨gles de routages pourraient aussi Ãªtre basÃ©es sur le path.\n1... 2spec: 3 hostnames: 4 - foo.bar.com 5 rules: 6 - matches: 7 - path: 8 type: PathPrefix 9 value: /login Ou une entÃªte HTTP\n1... 2spec: 3 rules: 4 - matches: 5 headers: 6 - name: \u0026#34;version\u0026#34; 7 value: \u0026#34;2\u0026#34; 8... VÃ©rifions que le service est joignable:\n1curl -s http://echo.cloud.ogenki.io | jq -rc \u0026#39;.environment.HOSTNAME\u0026#39; 2echo-1-echo-server-fd88497d-w6sgn Comme vous pouvez le voir le service est exposÃ© en HTTP sans certificat. Essayons de corriger cela ğŸ˜‰\nExposer un service en utilisant un certificat TLS Il existe plusieurs mÃ©thodes pour configurer du TLS avec GAPI. Ici nous allons utiliser le cas le plus commun: protocole HTTPS et terminaison TLS sur la Gateway.\nSupposons que nous souhaitons configurer le nom de domaine echo.cloud.ogenki.io utilisÃ© prÃ©cÃ©demment. La configuration se fait principalement au niveau de la Gateway\napps/base/echo/tls-gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: echo 5 namespace: echo 6 annotations: 7 cert-manager.io/cluster-issuer: letsencrypt-prod 8spec: 9 gatewayClassName: cilium 10 listeners: 11 - name: http 12 hostname: \u0026#34;echo.${domain_name}\u0026#34; 13 port: 443 14 protocol: HTTPS 15 allowedRoutes: 16 namespaces: 17 from: Same 18 tls: 19 mode: Terminate 20 certificateRefs: 21 - name: echo-tls Le point essentiel ici est la rÃ©fÃ©rence Ã  un secret contenant le certificat echo-tls. Ce certificat peut Ãªtre crÃ©Ã© manuellement mais j'ai dÃ©cidÃ© pour cet article d'automatiser cela avec Let's Encrypt et cert-manager.\nInfo cert-manager\nAvec cert-manager il est trÃ¨s simple d'automatiser la crÃ©ation et la mise Ã  jour des certificats exposÃ©s par la Gateway. Pour cela, il faut permettre au contrÃ´lleur d'accÃ©der Ã  route53 afin de rÃ©soudre un challenge DNS01 (MÃ©canisme qui permet de s'assurer que les clients peuvent seulement demander des certificats pour des domaines qu'ils possÃ¨dent).\nUne ressource ClusterIssuer dÃ©crit la configuration nÃ©cessaire pour gÃ©nÃ©rer des certificats grÃ¢ce Ã  cert-manager.\nEnsuite il suffit d'ajouter une annotation cert-manager.io/cluster-issuer et indiquer le secret Kubernetes oÃ¹ sera stockÃ© le certificat.\nâ„¹ï¸ Dans le repo de demo les permissions sont attribuÃ©es en utilisant Crossplane qui se charge de configurer cela au niveau du Cloud AWS.\nPlus d'informations\nPour que le routage se fasse correctement il faut aussi bien entendu rÃ©fÃ©rencer la bonne Gateway mais aussi indiquer le nom de domaine dans la ressource HTTPRoute.\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7 parentRefs: 8 - name: echo 9 namespace: echo 10 hostnames: 11 - \u0026#34;echo.${domain_name}\u0026#34; 12... Il faut patienter quelques minutes le temps que le certificat soit crÃ©Ã©.\n1kubectl get cert -n echo 2NAME READY SECRET AGE 3echo-tls True echo-tls 43m Nous pouvons enfin vÃ©rifier que le certificat est bien issue de Let's Encrypt comme suit:\n1curl https://echo.cloud.ogenki.io -v 2\u0026gt;\u0026amp;1 | grep -A 6 \u0026#39;Server certificate\u0026#39; 2* Server certificate: 3* subject: CN=echo.cloud.ogenki.io 4* start date: Sep 15 14:43:00 2023 GMT 5* expire date: Dec 14 14:42:59 2023 GMT 6* subjectAltName: host \u0026#34;echo.cloud.ogenki.io\u0026#34; matched cert\u0026#39;s \u0026#34;echo.cloud.ogenki.io\u0026#34; 7* issuer: C=US; O=Let\u0026#39;s Encrypt; CN=R3 8* SSL certificate verify ok. Info GAPI permet aussi de configurer le TLS de bout en bout, jusqu'au conteneur. Cela se fait en configurant la Gateway en Passthrough et en utilisant une ressource TLSRoute. Il faut aussi que le certificat soit portÃ© par le pod qui fait terminaison TLS.\nUne Gateway partagÃ©e par plusieurs namespaces Avec GAPI il est possible de router le trafic Ã  travers les Namespaces. Cela est rendu possible grÃ¢ce Ã  des ressources distinctes pour chaque fonction: Une Gateway qui permet de configurer l'infrastructure et notamment de provisionner une adresse IP, et les *Routes. Ces routes peuvent rÃ©fÃ©rencer une Gateway situÃ©e dans un autre namespace. Il est ainsi possible pour diffÃ©rent(e)s Ã©quipes/projets de partager les mÃªmes Ã©lÃ©ments d'infrastructure.\nIl est cependant requis de spÃ©cifier quelle route est autorisÃ©e Ã  rÃ©fÃ©rencer la Gateway. Ici nous supposons que nous avons une Gateway dÃ©diÃ©e aux outils internes qui s'appelle platform. En utilisant le paramÃ¨tre allowedRoutes, nous spÃ©cifions explicitement quelles sont les namespaces autorisÃ©s Ã  rÃ©fÃ©rencer cette Gateway.\ninfrastructure/base/gapi/platform-gateway.yaml\n1... 2 allowedRoutes: 3 namespaces: 4 from: Selector 5 selector: 6 matchExpressions: 7 - key: kubernetes.io/metadata.name 8 operator: In 9 values: 10 - observability 11 - flux-system 12 tls: 13 mode: Terminate 14 certificateRefs: 15 - name: platform-tls Les HTTPRoutes situÃ©es dans les namespaces observability et flux-system rÃ©ferÃ©ncent cette mÃªme Gateway.\n1... 2spec: 3 parentRefs: 4 - name: platform 5 namespace: infrastructure Et utilisent le mÃªme rÃ©partiteur de charge du Clouder\n1NLB_DOMAIN=$(kubectl get svc -n infrastructure cilium-gateway-platform -o jsonpath={.status.loadBalancer.ingress[0].hostname}) 2 3dig +short ${NLB_DOMAIN} 413.36.89.108 5 6dig +short grafana-mycluster-0.cloud.ogenki.io 713.36.89.108 8 9dig +short gitops-mycluster-0.cloud.ogenki.io 1013.36.89.108 Note ğŸ”’ Ces outils internes ne devraient pas Ãªtre exposÃ©s sur Internet mais vous comprendrez qu'il s'agit lÃ  d'une dÃ©mo ğŸ™. On pourrait par exemple, utiliser et une Gateway interne (IP privÃ©e) en jouant sur les annotations et un moyen de connexion privÃ© (VPN, tunnels ...)\nTraffic splitting Il est souvent utile de tester une application sur une portion du trafic lorsqu'une nouvelle version est disponible (A/B testing ou Canary deployment). GAPI permet cela de faÃ§on trÃ¨s simple en utilisant des poids.\nVoici un exemple permettant de tester sur 5% du trafic vers le service echo-2-echo-server\napps/base/echo/httproute-split.yaml\n1... 2 hostnames: 3 - \u0026#34;split-echo.${domain_name}\u0026#34; 4 rules: 5 - matches: 6 - path: 7 type: PathPrefix 8 value: / 9 backendRefs: 10 - name: echo-1-echo-server 11 port: 80 12 weight: 95 13 - name: echo-2-echo-server 14 port: 80 15 weight: 5 VÃ©rifions que la rÃ©partition se fait bien comme attendu:\nscripts/check-split.sh\n1./scripts/check-split.sh https://split-echo.cloud.ogenki.io 2Number of requests for echo-1: 95 3Number of requests for echo-2: 5 Manipulation des entÃªtes HTTP (Headers) Il est aussi possible de jouer avec les entÃªtes HTTP (Headers): en ajouter, modifier ou supprimer. Ces modifications peuvent se faire sur les Headers de requÃªte ou de rÃ©ponse par le biais de filtres ajoutÃ©s Ã  la ressource HTTPRoute.\nNous allons par exemple ajouter un Header Ã  la requÃªte\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7... 8 rules: 9 - matches: 10 - path: 11 type: PathPrefix 12 value: /req-header-add 13 filters: 14 - type: RequestHeaderModifier 15 requestHeaderModifier: 16 add: 17 - name: foo 18 value: bar 19 backendRefs: 20 - name: echo-1-echo-server 21 port: 80 22... La commande suivante permet de vÃ©rifier que le header est bien prÃ©sent.\n1curl -s https://echo.cloud.ogenki.io/req-header-add -sk | jq \u0026#39;.request.headers\u0026#39; 2{ 3 \u0026#34;host\u0026#34;: \u0026#34;echo.cloud.ogenki.io\u0026#34;, 4 \u0026#34;user-agent\u0026#34;: \u0026#34;curl/8.2.1\u0026#34;, 5 \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34;, 6 \u0026#34;x-forwarded-for\u0026#34;: \u0026#34;81.220.234.254\u0026#34;, 7 \u0026#34;x-forwarded-proto\u0026#34;: \u0026#34;https\u0026#34;, 8 \u0026#34;x-envoy-external-address\u0026#34;: \u0026#34;81.220.234.254\u0026#34;, 9 \u0026#34;x-request-id\u0026#34;: \u0026#34;320ba4d2-3bd6-4c2f-8a97-74296a9f3f26\u0026#34;, 10 \u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34; 11} ğŸªª Les roles et permissions GAPI offre un modÃ¨le de partage des permissions claire entre l'infrastructure de routage du trafic (gÃ©rÃ©e par les administrateurs de cluster) et les applications (gÃ©rÃ©es par les dÃ©veloppeurs).\nLe fait de disposer de plusieurs ressources nous permet d'utiliser les ressources RBAC dans Kubernetes pour attribuer les droits de faÃ§on dÃ©clarative. J'ai ajoutÃ© quelques exemples qui n'ont aucun effet dans mon cluster de dÃ©mo mais qui peuvent vous permettre de vous faire une idÃ©e.\nLa configuration suivante permet aux membres du groupe developers de gÃ©rer les HTTPRoutes dans le namespace echo. En revanche ils ne possÃ©dent que des droits en lecture sur les Gateways.\n1--- 2apiVersion: rbac.authorization.k8s.io/v1 3kind: Role 4metadata: 5 namespace: echo 6 name: gapi-developer 7rules: 8 - apiGroups: [\u0026#34;gateway.networking.k8s.io\u0026#34;] 9 resources: [\u0026#34;httproutes\u0026#34;] 10 verbs: [\u0026#34;*\u0026#34;] 11 - apiGroups: [\u0026#34;gateway.networking.k8s.io\u0026#34;] 12 resources: [\u0026#34;gateways\u0026#34;] 13 verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] 14--- 15apiVersion: rbac.authorization.k8s.io/v1 16kind: RoleBinding 17metadata: 18 name: gapi-developer 19 namespace: echo 20subjects: 21 - kind: Group 22 name: \u0026#34;developers\u0026#34; 23 apiGroup: rbac.authorization.k8s.io 24roleRef: 25 kind: Role 26 name: gapi-developer 27 apiGroup: rbac.authorization.k8s.io ğŸ¤” Un pÃ©rimÃ¨tre pas Ã©vident Il ne faut pas confondre GAPI avec ce que l'on nomme couramment une API Gateway. Une section de la FAQ a d'ailleurs Ã©tÃ© crÃ©Ã© pour Ã©claircir ce point. Bien que GAPI offre des fonctionnalitÃ©s typiquement prÃ©sentes dans une API Gateway, il s'agit avant tout d'une implÃ©mentation spÃ©cifique pour Kubernetes. Cependant, ce choix de dÃ©nomination peut prÃªter Ã  confusion.\nIl est essentiel de mentionner que cet article se concentre uniquement sur le trafic entrant, appelÃ© north-south, traditionnellement gÃ©rÃ© par les Ingress Controllers. Ce trafic reprÃ©sente le pÃ©rimÃ¨tre initial de GAPI. Une initiative rÃ©cente nommÃ©e GAMMA vise Ã  Ã©galement gÃ©rer le routage east-west, ce qui permettra de standardiser certaines fonctionnalitÃ©s des solutions de Service Mesh Ã  l'avenir. (voir cet article pour plus d'informations).\nğŸ’­ DerniÃ¨res remarques Pour Ãªtre honnÃªte, j'ai entendu parlÃ© de Gateway API depuis un petit moment. J'ai lu quelques articles mais jusqu'ici je n'avais pas pris le temps d'approfondir le sujet. Je me disais \u0026quot;Pourquoi? J'arrive Ã  faire ce que je veux avec mon Ingress Controller ? et puis il faut apprendre Ã  utiliser de nouvelles ressources\u0026quot;.\nGAPI gagne en maturitÃ© et nous sommes proche d'une version GA. De nombreux projets l'ont dÃ©jÃ  adoptÃ©, Istio et Linkerd par exemple sont totalement compatibles avec la version 0.8.0 et cette faÃ§on de gÃ©rer le trafic au sein de Kubernetes deviendra rapidement la norme.\nToujours est-il que j'ai beaucoup aimÃ© la dÃ©claration des diffÃ©rentes configurations que je trouve trÃ¨s intuitive et explicite â¤ï¸. D'autre part le modÃ¨le de sÃ©curitÃ© permet de donner le pouvoir aux developpeurs sans sacrifier la sÃ©curitÃ©. Enfin la gestion de l'infrastructure se fait de faÃ§on transparente, nous pouvons rapidement passer d'une implÃ©mentation (contrÃ´leur sous-jacent) Ã  une autre sans toucher aux *Routes.\nAlors suis-je prÃªt Ã  changer mon Ingress Controller pour Cilium aujourd'hui? La rÃ©ponse courte est Non mais bientÃ´t!.\nTout d'abord j'aimerais mettre en Ã©vidence sur l'Ã©tendue des possiblitÃ©s offertes par Cilium: De nombreuses personnes se sentent noyÃ©es sous les nombreux outils qui gravitent autour de Kubernetes. Cilium permettrait de remplir les fonctionnalitÃ©s de nombre d'entre eux (metrics, tracing, service-mesh, sÃ©curitÃ© et ... Ingress Controller avec GAPI).\nCependant, bien que nous puissions faire du routage HTTP de base, il y Ã  certains points d'attention:\nLe support de TCP et UDP Le support de GRPC Devoir passer par une rÃ¨gle de mutation pour pouvoir configurer les composants cloud. (Issue Github) De nombreuses fonctionnalitÃ©s explorÃ©es sont toujours au stade expÃ©rimental. On peut citer les fonctions Ã©tendues qui supportÃ©s depuis quelques jours: J'ai par exemple tentÃ© de configurer une redirection HTTP\u0026gt;HTTPS simple mais je suis tombÃ© sur ce problÃ¨me. Je m'attends donc Ã  ce qu'il y ait des changements dans l'API trÃ¨s prochainement. Je n'ai pas abordÃ© toutes les fonctionnalitÃ©s de l'implÃ©mentation Cilium de GAPI (HonnÃªtement, cet article est dÃ©jÃ  bien fourni ğŸ˜œ). NÃ©anmoins, je suis vraiment convaincu de son potentiel. J'ai bon espoir qu'on pourra bientÃ´t envisager son utilisation en production. Si vous n'avez pas encore envisagÃ© cette transition, c'est le moment de s'y pencher ğŸ˜‰ ! Toutefois, compte tenu des aspects Ã©voquÃ©s prÃ©cÃ©demment, je conseillerais de patienter un peu.\nğŸ”– References https://gateway-api.sigs.k8s.io/ https://docs.cilium.io/en/latest/network/servicemesh/gateway-api/gateway-api/#gs-gateway-api https://isovalent.com/blog/post/cilium-gateway-api/ https://isovalent.com/blog/post/tutorial-getting-started-with-the-cilium-gateway-api/ Les labs d'Isovalent permettent de rapidement de tester GAPI et vous pourrez ajouter des badges Ã  votre collection ğŸ˜„ ","link":"https://blog.ogenki.io/fr/post/cilium-gateway-api/","section":"post","tags":["kubernetes","infrastructure","network"],"title":"`Gateway API`: Remplacer mon Ingress Controller avec `Cilium`?"},{"body":"","link":"https://blog.ogenki.io/fr/tags/kubernetes/","section":"tags","tags":null,"title":"Kubernetes"},{"body":" Update 2024-11-23 Ne plus utiliser Weave Gitops pour les ressources Flux. Il existe dÃ©sormais un plugin Headlamp.\nTerraform est probablement l'outil \u0026quot;Infrastructure As Code\u0026quot; le plus utilisÃ© pour construire, modifier et versionner les changements d'infrastructure Cloud. Il s'agit d'un projet Open Source dÃ©veloppÃ© par Hashicorp et qui utilise le langage HCL pour dÃ©clarer l'Ã©tat souhaitÃ© de ressources Cloud. L'Ã©tat des ressources crÃ©Ã©es est stockÃ© dans un fichier d'Ã©tat (terraform state).\nOn peut considÃ©rer que Terraform est un outil \u0026quot;semi-dÃ©claratif\u0026quot; car il n'y a pas de fonctionnalitÃ© de rÃ©conciliation automatique intÃ©grÃ©e. Il existe diffÃ©rentes approches pour rÃ©pondre Ã  cette problÃ©matique, mais en rÃ¨gle gÃ©nÃ©rale, une modification sera appliquÃ©e en utilisant terraform apply. Le code est bien dÃ©crit dans des fichiers de configuration HCL (dÃ©claratif) mais l'exÃ©cution est faite de maniÃ¨re impÃ©rative. De ce fait, il peut y avoir de la dÃ©rive entre l'Ã©tat dÃ©clarÃ© et le rÃ©el (par exemple, un collÃ¨gue qui serait passÃ© sur la console pour changer un paramÃ¨tre ğŸ˜‰).\nâ“â“ Alors, comment m'assurer que ce qui est commit dans mon repo git est vraiment appliquÃ©. Comment Ãªtre alertÃ© s'il y a un changement par rapport Ã  l'Ã©tat dÃ©sirÃ© et comment appliquer automatiquement ce qui est dans mon code (GitOps) ?\nC'est la promesse de tf-controller, un operateur Kubernetes Open Source de Weaveworks, Ã©troitement liÃ© Ã  Flux (un moteur GitOps de la mÃªme sociÃ©tÃ©). Flux est l'une des solutions que je plÃ©biscite, et je vous invite donc Ã  lire un prÃ©cÃ©dent article.\nInfo L'ensemble des Ã©tapes dÃ©crites ci-dessous sont faites avec ce repo Git\nğŸ¯ Notre objectif En suivant les Ã©tapes de cet article nous visons les objectifs suivant:\nDÃ©ployer un cluster Kubernetes qui servira de \u0026quot;Control plane\u0026quot;. Pour rÃ©sumer il hÃ©bergera le controlleur Terraform qui nous permettra de dÃ©clarer tous les Ã©lÃ©ments d'infrastructure souhaitÃ©s. Utiliser Flux comme moteur GitOps pour toutes les ressources Kubernetes. Concernant le controleur Terraform, nous allons voir:\nQuelle est le moyen de dÃ©finir des dÃ©pendances entre modules CrÃ©ation de plusieurs ressources AWS: Zone route53, Certificat ACM, rÃ©seau, cluster EKS. Les diffÃ©rentes options de reconciliation (automatique, nÃ©cessitant une confirmation) Comment sauvegarder et restaurer un fichier d'Ã©tat (tfstate) ğŸ› ï¸ Installer le controleur Terraform â˜¸ Le cluster \u0026quot;Control Plane\u0026quot; Afin de pouvoir utiliser le controleur Kubernetes tf-controller, il nous faut d'abord un cluster Kubernetes ğŸ˜†. Nous allons donc crÃ©er un cluster control plane en utilisant la ligne de commande terraform et les bonnes pratiques EKS.\nWarning Il est primordial que ce cluster soit rÃ©siliant, sÃ©curisÃ© et supervisÃ© car il sera responsable de la gestion de l'ensemble des ressources AWS crÃ©Ã©es par la suite.\nSans entrer dans le dÃ©tail, le cluster \u0026quot;control plane\u0026quot; a Ã©tÃ© crÃ©Ã© un utilisant ce code. CelÃ -dit, il est important de noter que toutes les opÃ©rations de dÃ©ploiement d'application se font en utilisant Flux.\nInfo En suivant les instructions du README, un cluster EKS sera crÃ©Ã© mais pas uniquement! Il faut en effet donner les permissions au controlleur Terraform pour appliquer les changements d'infrastructure. De plus, Flux doit Ãªtre installÃ© et configurÃ© afin d'appliquer la configuration dÃ©finie ici.\nAu final on se retrouve donc avec plusieurs Ã©lÃ©ments installÃ©s et configurÃ©s:\nles addons quasi indispensables que sont aws-loadbalancer-controller et external-dns les roles IRSA pour ces mÃªmes composants sont installÃ©s en utilisant tf-controller La stack de supervision Prometheus / Grafana. external-secrets pour pouvoir rÃ©cupÃ©rer des Ã©lÃ©ments sensibles depuis AWS secretsmanager. Afin de dÃ©montrer tout cela au bout de quelques minutes l'interface web pour Flux est accessible via l'URL gitops-\u0026lt;cluster_name\u0026gt;.\u0026lt;domain_name\u0026gt; VÃ©rifier toute de mÃªme que le cluster est accessible et que Flux fonctionne correctement\n1aws eks update-kubeconfig --name controlplane-0 --alias controlplane-0 2Updated context controlplane-0 in /home/smana/.kube/config 1flux check 2... 3âœ” all checks passed 4 5flux get kustomizations 6NAME REVISION SUSPENDED READY MESSAGE 7flux-config main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 8flux-system main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 9infrastructure main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 10security main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 11tf-controller main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 12... ğŸ“¦ Le chart Helm et Flux Maintenant que notre cluster \u0026quot;controlplane\u0026quot; est opÃ©rationnel, l'ajout le contrÃ´leur Terraform consiste Ã  utiliser le chart Helm.\nIl faut tout d'abord dÃ©clarer la source:\nsource.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: tf-controller 5spec: 6 interval: 30m 7 url: https://weaveworks.github.io/tf-controller Et dÃ©finir la HelmRelease:\nrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: tf-controller 5spec: 6 releaseName: tf-controller 7 chart: 8 spec: 9 chart: tf-controller 10 sourceRef: 11 kind: HelmRepository 12 name: tf-controller 13 namespace: flux-system 14 version: \u0026#34;0.12.0\u0026#34; 15 interval: 10m0s 16 install: 17 remediation: 18 retries: 3 19 values: 20 resources: 21 limits: 22 memory: 1Gi 23 requests: 24 cpu: 200m 25 memory: 500Mi 26 runner: 27 serviceAccount: 28 annotations: 29 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/tfcontroller_${cluster_name}\u0026#34; Lorsque ce changement est Ã©crit dans le repo Git, la HelmRelease sera dÃ©ployÃ©e et le contrÃ´lleur tf-controller dÃ©marera\n1kubectl get hr -n flux-system 2NAME AGE READY STATUS 3tf-controller 67m True Release reconciliation succeeded 4 5kubectl get po -n flux-system -l app.kubernetes.io/instance=tf-controller 6NAME READY STATUS RESTARTS AGE 7tf-controller-7ffdc69b54-c2brg 1/1 Running 0 2m6s Dans le repo de demo il y a dÃ©jÃ  un certain nombre de ressources AWS dÃ©clarÃ©es. Par consÃ©quent, au bout de quelques minutes, le cluster se charge de la crÃ©ation de celles-cis: Info Bien que la majoritÃ© des tÃ¢ches puisse Ãªtre rÃ©alisÃ©e de maniÃ¨re dÃ©clarative ou via les utilitaires de ligne de commande tels que kubectl et flux, un autre outil existe qui offre la possibilitÃ© d'interagir avec les ressources terraform : tfctl\nğŸš€ Appliquer un changement Parmis les bonnes pratiques avec Terraform, il y a l'usage de modules. Un module est un ensemble de ressources Terraform liÃ©es logigement afin d'obtenir une seule unitÃ© rÃ©utilisable. Cela permet d'abstraire la complexitÃ©, de prendre des entrÃ©es, effectuer des actions spÃ©cifiques et produire des sorties.\nIl est possible de crÃ©er ses propres modules et de les mettre Ã  disposition dans des Sources ou d'utiliser les nombreux modules partagÃ©s et maintenus par les communautÃ©s. Il suffit alors d'indiquer quelques variables afin de l'adapter au contexte.\nAvec tf-controller, la premiÃ¨re Ã©tape consiste donc Ã  indiquer la Source du module. Ici nous allons configurer le socle rÃ©seau sur AWS (vpc, subnets...) avec le module terraform-aws-vpc.\nsources/terraform-aws-vpc.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1 2kind: GitRepository 3metadata: 4 name: terraform-aws-vpc 5 namespace: flux-system 6spec: 7 interval: 30s 8 ref: 9 tag: v5.0.0 10 url: https://github.com/terraform-aws-modules/terraform-aws-vpc Nous pouvons ensuite crÃ©er la ressource Terraform qui en fait usage:\nvpc/dev.yaml\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6 interval: 8m 7 path: . 8 destroyResourcesOnDeletion: true # You wouldn\u0026#39;t do that on a prod env ;) 9 storeReadablePlan: human 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-vpc 13 namespace: flux-system 14 vars: 15 - name: name 16 value: vpc-dev 17 - name: cidr 18 value: \u0026#34;10.42.0.0/16\u0026#34; 19 - name: azs 20 value: 21 - \u0026#34;eu-west-3a\u0026#34; 22 - \u0026#34;eu-west-3b\u0026#34; 23 - \u0026#34;eu-west-3c\u0026#34; 24 - name: private_subnets 25 value: 26 - \u0026#34;10.42.0.0/19\u0026#34; 27 - \u0026#34;10.42.32.0/19\u0026#34; 28 - \u0026#34;10.42.64.0/19\u0026#34; 29 - name: public_subnets 30 value: 31 - \u0026#34;10.42.96.0/24\u0026#34; 32 - \u0026#34;10.42.97.0/24\u0026#34; 33 - \u0026#34;10.42.98.0/24\u0026#34; 34 - name: enable_nat_gateway 35 value: true 36 - name: single_nat_gateway 37 value: true 38 - name: private_subnet_tags 39 value: 40 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 41 \u0026#34;karpenter.sh/discovery\u0026#34;: dev 42 - name: public_subnet_tags 43 value: 44 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 45 writeOutputsToSecret: 46 name: vpc-dev Si l'on devait rÃ©sumer grossiÃ¨rement: le code terraform provenant de la source terraform-aws-vpc est utilisÃ© avec les variables vars.\nIl y a ensuite plusieurs paramÃ¨tres qui influent sur le fonctionnement de tf-controller. Les principaux paramÃ¨tres qui permettent de contrÃ´ler la faÃ§on dont sont appliquÃ©es les modifications sont .spec.approvePlan et .spec.autoApprove\nğŸš¨ DÃ©tection de la dÃ©rive DÃ©finir spec.approvePlan avec une valeur Ã  disable permet uniquement de notifier que l'Ã©tat actuel des ressources a dÃ©rivÃ© par rapport au code Terraform. Cela permet notamment de choisir le moment et la maniÃ¨re dont l'application des changements sera effectuÃ©e.\nNote De mon point de vue il manque une section sur les notifications: La dÃ©rive, les plans en attentes, les problÃ¨mese de rÃ©concilation. J'essaye d'identifier les mÃ©thodes possibles (de prÃ©fÃ©rence avec Prometheus) et de mettre Ã  jour cet article dÃ¨s que possible.\nğŸ”§ Application manuelle L'exemple donnÃ© prÃ©cÃ©demment (vpc-dev) ne contient pas le paramÃ¨tre .spec.approvePlan et hÃ©rite donc de la valeur par dÃ©faut qui est false. Par consÃ©quent, l'application concrÃ¨te des modifications (apply), n'est pas faite automatiquement.\nUn plan est exÃ©cutÃ© et sera en attente d'une validation:\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system vpc-dev Unknown Plan generated: set approvePlan: \u0026#34;plan-v5.0.0-26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. true 2 minutes Je conseille d'ailleurs de configurer le paramÃ¨tre storeReadablePlan Ã  human. Cela permet de visualiser simplement les modifications en attente en utilisant tfctl:\n1tfctl show plan vpc-dev 2 3Terraform used the selected providers to generate the following execution 4plan. ressource actions are indicated with the following symbols: 5 + create 6 7Terraform will perform the following actions: 8 9 # aws_default_network_acl.this[0] will be created 10 + ressource \u0026#34;aws_default_network_acl\u0026#34; \u0026#34;this\u0026#34; { 11 + arn = (known after apply) 12 + default_network_acl_id = (known after apply) 13 + id = (known after apply) 14 + owner_id = (known after apply) 15 + tags = { 16 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 17 } 18 + tags_all = { 19 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 20 } 21 + vpc_id = (known after apply) 22 23 + egress { 24 + action = \u0026#34;allow\u0026#34; 25 + from_port = 0 26 + ipv6_cidr_block = \u0026#34;::/0\u0026#34; 27 + protocol = \u0026#34;-1\u0026#34; 28 + rule_no = 101 29 + to_port = 0 30 } 31 + egress { 32... 33Plan generated: set approvePlan: \u0026#34;plan-v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. 34To set the field, you can also run: 35 36 tfctl approve vpc-dev -f filename.yaml AprÃ¨s revue des modifications ci-dessus, il suffit donc d'ajouter l'identifiant du plan Ã  valider et de pousser le changement sur git comme suit:\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6... 7 approvePlan: plan-v5.0.0-26c38a66f1 8... En quelques instants un runner sera lancÃ© qui se chargera d'appliquer les changements:\n1kubectl logs -f -n flux-system vpc-dev-tf-runner 22023/07/01 15:33:36 Starting the runner... version sha 3... 4aws_vpc.this[0]: Creating... 5aws_vpc.this[0]: Still creating... [10s elapsed] 6... 7aws_route_table_association.private[1]: Creation complete after 0s [id=rtbassoc-01b7347a7e9960a13] 8aws_nat_gateway.this[0]: Still creating... [10s elapsed] La rÃ©conciliation Ã©ffectuÃ©e, la ressource passe Ã  l'Ã©tat READY: True\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m ğŸ¤– Application automatique Nous pouvons aussi activer la rÃ©conciliation automatique. Pour ce faire il faut dÃ©clarer le paramÃ¨tre .spec.autoApprove Ã  true.\nToutes les ressources IRSA sont configurÃ©es de la sorte:\nexternal-secrets.yaml\n1piVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: irsa-external-secrets 5spec: 6 approvePlan: auto 7 destroyResourcesOnDeletion: true 8 interval: 8m 9 path: ./modules/iam-role-for-service-accounts-eks 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-iam 13 namespace: flux-system 14 vars: 15 - name: role_name 16 value: ${cluster_name}-external-secrets 17 - name: attach_external_secrets_policy 18 value: true 19 - name: oidc_providers 20 value: 21 main: 22 provider_arn: ${oidc_provider_arn} 23 namespace_service_accounts: [\u0026#34;security:external-secrets\u0026#34;] Donc si je fais le moindre changement sur la console AWS par exemple, celui-ci sera rapidement Ã©crasÃ© par celui gÃ©rÃ© par tf-controller.\nInfo La politique de suppression d'une ressource Terraform est dÃ©finie par le paramÃ¨tre destroyResourcesOnDeletion. Par dÃ©faut elles sont conservÃ©es et il faut donc que ce paramÃ¨tre ait pour valeur true afin de dÃ©truire les Ã©lÃ©ments crÃ©es lorsque l'objet Kubernetes est supprimÃ©.\nIci nous voulons la possibilitÃ© de supprimer les rÃ´les IRSA. Ils sont en effet Ã©troitement liÃ©s aux clusters.\nğŸ”„ EntrÃ©es et sorties: dÃ©pendances entre modules Lorsque qu'on utilise Terraform, on a souvent besoin de passer des donnÃ©es d'un module Ã  l'autre. GÃ©nÃ©ralement ce sont les outputs du module qui exportent ces informations. Il faut donc un moyen de les importer dans un autre module.\nReprenons encore l'exemple donnÃ© ci-dessus (vpc-dev). Nous notons en bas du YAML la directive suivante:\n1... 2 writeOutputsToSecret: 3 name: vpc-dev Lorsque cette ressource est appliquÃ©e nous aurons un message qui confirme que les outputs sont disponibles (\u0026quot;Outputs written\u0026quot;):\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m En effet ce module exporte de nombreuses informations (126):\n1kubectl get secrets -n flux-system vpc-dev 2NAME TYPE DATA AGE 3vpc-dev Opaque 126 15s 4 5kubectl get secret -n flux-system vpc-dev --template=\u0026#39;{{.data.vpc_id}}\u0026#39; | base64 -d 6vpc-0c06a6d153b8cc4db Certains de ces Ã©lÃ©ments d'informations sont ensuite utilisÃ©s pour crÃ©er un cluster EKS de dev:\nvpc/dev.yaml\n1... 2 varsFrom: 3 - kind: Secret 4 name: vpc-dev 5 varsKeys: 6 - vpc_id 7 - private_subnets 8... ğŸ’¾ Sauvegarder et restaurer un tfstate Dans mon cas je ne souhaite pas recrÃ©er la zone et le certificat Ã  chaque destruction du controlplane. Voici un exemple des Ã©tapes Ã  mener pour que je puisse restaurer l'Ã©tat de ces ressources lorsque j'utilise cette demo.\nNote Il s'agit lÃ  d'une procÃ©dure manuelle afin de dÃ©montrer le comportement de tf-controller par rapport aux fichiers d'Ã©tat. Par dÃ©faut ces tfstates sont stockÃ©s dans des secrets mais on prÃ©ferera configurer un backend GCS ou S3\nLa crÃ©ation initiale de l'environnement de dÃ©mo m'a permis de sauvegarder les fichiers d'Ã©tat (tfstate) de cette faÃ§on.\n1WORKSPACE=\u0026#34;default\u0026#34; 2STACK=\u0026#34;route53-cloud-hostedzone\u0026#34; 3BACKUPDIR=\u0026#34;${HOME}/tf-controller-backup\u0026#34; 4 5mkdir -p ${BACKUPDIR} 6 7kubectl get secrets -n flux-system tfstate-${WORKSPACE}-${STACK} -o jsonpath=\u0026#39;{.data.tfstate}\u0026#39; | \\ 8base64 -d | gzip -d \u0026gt; ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate Lorsque le cluster est crÃ©Ã© Ã  nouveau, tf-controller essaye de crÃ©er la zone car le fichier d'Ã©tat est vide.\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system route53-cloud-hostedzone Unknown Plan generated: set approvePlan: \u0026#34;plan-main@sha1:345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. true 16 minutes 5 6tfctl show plan route53-cloud-hostedzone 7 8Terraform used the selected providers to generate the following execution 9plan. resource actions are indicated with the following symbols: 10 + create 11 12Terraform will perform the following actions: 13 14 # aws_route53_zone.this will be created 15 + resource \u0026#34;aws_route53_zone\u0026#34; \u0026#34;this\u0026#34; { 16 + arn = (known after apply) 17 + comment = \u0026#34;Experimentations for blog.ogenki.io\u0026#34; 18 + force_destroy = false 19 + id = (known after apply) 20 + name = \u0026#34;cloud.ogenki.io\u0026#34; 21 + name_servers = (known after apply) 22 + primary_name_server = (known after apply) 23 + tags = { 24 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 25 } 26 + tags_all = { 27 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 28 } 29 + zone_id = (known after apply) 30 } 31 32Plan: 1 to add, 0 to change, 0 to destroy. 33 34Changes to Outputs: 35 + domain_name = \u0026#34;cloud.ogenki.io\u0026#34; 36 + nameservers = (known after apply) 37 + zone_arn = (known after apply) 38 + zone_id = (known after apply) 39 40Plan generated: set approvePlan: \u0026#34;plan-main@345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. 41To set the field, you can also run: 42 43 tfctl approve route53-cloud-hostedzone -f filename.yaml La procÃ©dure de restauration consiste donc Ã  crÃ©er le secret Ã  nouveau:\n1gzip ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate 2 3cat \u0026lt;\u0026lt;EOF | kubectl apply -f - 4apiVersion: v1 5kind: Secret 6metadata: 7 name: tfstate-${WORKSPACE}-${STACK} 8 namespace: flux-system 9 annotations: 10 encoding: gzip 11type: Opaque 12data: 13 tfstate: $(cat ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate.gz | base64 -w 0) 14EOF Il faudra aussi relancer un plan de faÃ§on explicite pour mettre Ã  jour l'Ã©tat de la ressource en question\n1tfctl replan route53-cloud-hostedzone 2ï˜« Replan requested for flux-system/route53-cloud-hostedzone 3Error: timed out waiting for the condition Nous pouvons alors vÃ©rifier que le fichier d'Ã©tat a bien Ã©tÃ© mis Ã  jour\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3flux-system route53-cloud-hostedzone True Outputs written: main@sha1:d0934f979d832feb870a8741ec01a927e9ee6644 false 19 minutes ğŸ” Focus sur certaines fonctionnalitÃ©s de Flux Oui j'ai un peu menti sur l'agenda ğŸ˜. Il me semblait nÃ©cessaire de mettre en lumiÃ¨re 2 fonctionnalitÃ©s que je n'avais pas exploitÃ© jusque lÃ  et qui sont fort utiles!\nSubstition de variables Lorsque Flux est initiliasÃ© un certain nombre de Kustomization spÃ©cifique Ã  ce cluster sont crÃ©Ã©s. Il est possible d'y indiquer des variables de substitution qui pourront Ãªtre utilisÃ©es dans l'ensemble des ressources dÃ©ployÃ©es par cette Kustomization. Cela permet d'Ã©viter un maximum la dÃ©duplication de code.\nJ'ai dÃ©couvert l'efficacitÃ© de cette fonctionnalitÃ© trÃ¨s rÃ©cemment. Je vais dÃ©crire ici la faÃ§on dont je l'utilise:\nLe code terraform qui crÃ©e un cluster EKS, gÃ©nÃ¨re aussi une ConfigMap qui contient les variables propres au cluster. On y retrouvera, bien sÃ»r, le nom du cluster, mais aussi tous les paramÃ¨tres qui varient entre les clusters et qui sont utilisÃ©s dans les manifests Kubernetes.\nflux.tf\n1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;flux_clusters_vars\u0026#34; { 2 metadata { 3 name = \u0026#34;eks-${var.cluster_name}-vars\u0026#34; 4 namespace = \u0026#34;flux-system\u0026#34; 5 } 6 7 data = { 8 cluster_name = var.cluster_name 9 oidc_provider_arn = module.eks.oidc_provider_arn 10 aws_account_id = data.aws_caller_identity.this.account_id 11 region = var.region 12 environment = var.env 13 vpc_id = module.vpc.vpc_id 14 } 15 depends_on = [flux_bootstrap_git.this] 16} Comme spÃ©cifiÃ© prÃ©cedemment, les variables de substition sont dÃ©finies dans les Kustomization. Prenons un exemple concret. Ci-dessous on dÃ©finie la Kustomization qui dÃ©ploie toutes les ressources qui sont consommÃ©es par tf-controller On dÃ©clare ici la ConfigMap eks-controlplane-0-vars qui avait Ã©tÃ© gÃ©nÃ©rÃ© Ã  la crÃ©ation du cluster EKS.\ninfrastructure.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1 2kind: Kustomization 3metadata: 4 name: tf-custom-resources 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 path: ./infrastructure/controlplane-0/opentofu/custom-resources 10 postBuild: 11 substitute: 12 domain_name: \u0026#34;cloud.ogenki.io\u0026#34; 13 substituteFrom: 14 - kind: ConfigMap 15 name: eks-controlplane-0-vars 16 - kind: Secret 17 name: eks-controlplane-0-vars 18 optional: true 19 sourceRef: 20 kind: GitRepository 21 name: flux-system 22 dependsOn: 23 - name: tf-controller Enfin voici un exemple de ressource Kubernetes qui en fait usage. Cet unique manifest peut Ãªtre utilisÃ© par tous les clusters!.\nexternal-dns/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: external-dns 5spec: 6... 7 values: 8 global: 9 imageRegistry: public.ecr.aws 10 fullnameOverride: external-dns 11 aws: 12 region: ${region} 13 zoneType: \u0026#34;public\u0026#34; 14 batchChangeSize: 1000 15 domainFilters: [\u0026#34;${domain_name}\u0026#34;] 16 logFormat: json 17 txtOwnerId: \u0026#34;${cluster_name}\u0026#34; 18 serviceAccount: 19 annotations: 20 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/${cluster_name}-external-dns\u0026#34; Cela Ã©limine totalement les overlays qui consistaient Ã  ajouter les paramÃ¨tres spÃ©cifiques au cluster.\nWeb UI (Weave GitOps) Dans mon prÃ©cÃ©dent article sur Flux, je mentionnais le fait que l'un des inconvÃ©nients (si l'on compare avec son principale concurrent: ArgoCD) est le manque d'une interface Web. Bien que je sois un adepte de la ligne de commande, c'est parfois bien utile d'avoir une vue synthÃ©tique et de pouvoir effectuer certaines opÃ©ration en quelques clicks \u0026#x1f5b1;\u0026#xfe0f;\nC'est dÃ©sormais possible avec Weave Gitops! Bien entendu ce n'est pas comparable avec l'UI d'ArgoCD, mais l'essentiel est lÃ : Mettre en pause la rÃ©concilation, visualiser les manifests, les dÃ©pendances, les Ã©vÃ©nements...\nIl existe aussi le plugin VSCode comme alternative.\nğŸ’­ Remarques Et voilÃ , nous arrivons au bout de notre exploration de cet autre outil de gestion d'infrastructure sur Kubernetes. MalgrÃ© quelques petits soucis rencontrÃ©s en cours de route, que j'ai partagÃ© sur le repo Git du projet, l'expÃ©rience m'a beaucoup plu. tf-controller offre une rÃ©ponse concrÃ¨te Ã  une question frÃ©quente : comment gÃ©rer notre infra comme on gÃ¨re notre code ?\nJ'aime beaucoup l'approche GitOps appliquÃ©e Ã  l'infrastructure, j'avais d'ailleurs Ã©crit un article sur Crossplane. tf-controller aborde la problÃ©matique sous un angle diffÃ©rent: utiliser du Terraform directement. Cela signifie qu'on peut utiliser nos connaissances actuelles et notre code existant. Pas besoin d'apprendre une nouvelle faÃ§on de dÃ©clarer nos ressources. C'est un critÃ¨re Ã  prendre en compte car migrer vers un nouvel outil lorsque l'on a un existant reprÃ©sente un Ã©ffort non nÃ©gligeable. Cependant j'ajouterais aussi que tf-controller s'adresse aux utilisateurs de Flux uniquement et, de ce fait, restreint le publique cible.\nAujourd'hui, j'utilise une combinaison de Terraform, Terragrunt et RunAtlantis. tf-controller pourrait devenir une alternative viable: Nous avons en effet Ã©voquÃ© l'intÃ©rÃªt de Kustomize associÃ© aux substitions de variables pour la factorisation de code. Dans la roadmap du projet il y a aussi l'objectif d'afficher les plans dans les pull-requests. Autre problÃ©matique frÃ©quente: la nÃ©cessitÃ© de passer des Ã©lÃ©ments sensibles aux modules. En utilisant une ressource Terraform, on peut injecter des variables depuis des secrets Kubernetes. Ce qui permet d'utiliser certains outils, tels que external-secrets, sealed-secrets ...\nJe vous encourage donc Ã  essayer tf-controller vous-mÃªme, et peut-Ãªtre mÃªme d'y apporter votre contribution ğŸ™‚.\nNote La dÃ©mo que j'ai faite ici utilise pas mal de ressources, dont certaines assez cruciales (comme le rÃ©seau). Donc, gardez en tÃªte que c'est juste pour la dÃ©mo ! Je suggÃ¨re une approche progressive si vous envisagez de le mettre en ouvre: commencez par utiliser la dÃ©tection de dÃ©rives, puis crÃ©ez des ressources simples. J'ai aussi pris quelques raccourcis en terme de sÃ©curitÃ© Ã  Ã©viter absolument, notamment le fait de donner les droits admin au contrÃ´leur. ","link":"https://blog.ogenki.io/fr/post/terraform-controller/","section":"post","tags":["infrastructure"],"title":"Appliquer les principes de GitOps Ã  l'infrastructure: Introduction Ã  `tf-controller`"},{"body":"Kubernetes est dÃ©sormais la plate-forme privilÃ©giÃ©e pour orchestrer les applications \u0026quot;sans Ã©tat\u0026quot; aussi appelÃ© \u0026quot;stateless\u0026quot;. Les conteneurs qui ne stockent pas de donnÃ©es peuvent Ãªtre dÃ©truits et recrÃ©Ã©s ailleurs sans impact. En revanche, la gestion d'applications \u0026quot;stateful\u0026quot; dans un environnement dynamique tel que Kubernetes peut Ãªtre un vÃ©ritable dÃ©fi. MalgrÃ© le fait qu'il existe un nombre croissant de solutions de base de donnÃ©es \u0026quot;Cloud Native\u0026quot; (comme CockroachDB, TiDB, K8ssandra, Strimzi ...) et il y a de nombreux Ã©lÃ©ments Ã  considÃ©rer lors de leur Ã©valuation:\nQuelle est la maturitÃ© de l'opÃ©rateur? (Dynamisme et contributeurs, gouvernance du projet) Quels sont les resources personalisÃ©es disponibles (\u0026quot;custom resources\u0026quot;), quelles opÃ©rations permettent t-elles de rÃ©aliser? Quels sont les type de stockage disponibles: HDD / SSD, stockage local / distant? Que se passe-t-il lorsque quelque chose se passe mal: Quelle est le niveau de rÃ©silience de la solution? Sauvegarde et restauration: est-il facile d'effectuer et de planifier des sauvegardes? Quelles options de rÃ©plication et de mise Ã  l'Ã©chelle sont disponibles? Qu'en est-il des limites de connexion et de concurrence, les pools de connexion? A propos de la supervision, quelles sont les mÃ©triques exposÃ©es et comment les exploiter? J'Ã©tais Ã  la recherche d'une solution permettant de gÃ©rer un serveur PostgreSQL. La base de donnÃ©es qui y serait hÃ©bergÃ©e est nÃ©cessaire pour un logiciel de rÃ©servation de billets nommÃ© Alf.io. Nous sommes en effet en train d'organiser les Kubernetes Community Days France vous Ãªtes tous conviÃ©s! ğŸ‘.\nJe cherchais spÃ©cifiquement une solution indÃ©pendante d'un clouder (cloud agnostic) et l'un des principaux critÃ¨res Ã©tait la simplicitÃ© d'utilisation. Je connaissais dÃ©jÃ  plusieurs opÃ©rateurs Kubernetes, et j'ai fini par Ã©valuer une solution relativement rÃ©cente: CloudNativePG.\nCloudNativepg est l'opÃ©rateur de Kubernetes qui couvre le cycle de vie complet d'un cluster de base de donnÃ©es PostgreSQL hautement disponible avec une architecture de rÃ©plication native en streaming.\nCe projet Ã©tÃ© crÃ©Ã© par l'entreprise EnterpriseDB et a Ã©tÃ© soumis Ã  la CNCF afin de rejoindre les projets Sandbox.\nğŸ¯ Notre objectif Je vais donner ici une introduction aux principales fonctionnalitÃ©s de CloudNativePG.\nL'objectif est de:\nCrÃ©er une base de donnÃ©es PostgreSQL sur un cluster GKE, Ajouter une instance secondaire (rÃ©plication) ExÃ©cuter quelques tests de rÃ©silience. Nous verrons Ã©galement comment tout cela se comporte en terme de performances et quels sont les outils de supervision disponibles. Enfin, nous allons jeter un Å“il aux mÃ©thodes de sauvegarde/restauration.\nInfo Dans cet article, nous allons tout crÃ©er et tout mettre Ã  jour manuellement. Mais dans un environnement de production, il est conseillÃ© d'utiliser un moteur GitOps, par exemple Flux (sujet couvert dans un article prÃ©cÃ©dent).\nSi vous souhaitez voir un exemple complet, vous pouvez consulter le dÃ©pÃ´t git KCD France infrastructure.\nToutes les resources de cet article sont dans ce dÃ©pÃ´t.\n\u0026#x2611;\u0026#xfe0f; PrÃ©requis \u0026#x1f4e5; Outils gcloud SDK: Nous allons dÃ©ployer sur Google Cloud (en particulier sur GKE) et, pour ce faire, nous devrons crÃ©er quelques ressources dans notre projet GCP. Nous aurons donc besoin du SDK et de la CLI Google Cloud. Il est donc nÃ©cessaire de l'installer en suivant cette documentation.\nkubectl plugin: Pour faciliter la gestion des clusters, il existe un plugin kubectl qui donne des informations synthÃ©tiques sur l'instance PostgreSQL et permet aussi d'effectuer certaines opÃ©rations. Ce plugin peut Ãªtre installÃ© en utilisant krew:\n1kubectl krew install cnpg â˜ï¸ CrÃ©er les resources Google Cloud Avant de crÃ©er notre instance PostgreSQL, nous devons configurer certaines choses:\nNous avons besoin d'un cluster Kubernetes. (Cet article suppose que vous avez dÃ©jÃ  pris soin de provisionner un cluster GKE) Nous allons crÃ©er un bucket (Google Cloud Storage) pour stocker les sauvegardes et Fichiers WAL. Nous configurerons les permissions pour nos pods afin qu'ils puissent Ã©crire dans ce bucket. CrÃ©er le bucket Ã  l'aide de CLI gcloud\n1gcloud storage buckets create --location=eu --default-storage-class=coldline gs://cnpg-ogenki 2Creating gs://cnpg-ogenki/... 3 4gcloud storage buckets describe gs://cnpg-ogenki 5[...] 6name: cnpg-ogenki 7owner: 8 entity: project-owners-xxxx0008 9projectNumber: \u0026#39;xxx00008\u0026#39; 10rpo: DEFAULT 11selfLink: https://www.googleapis.com/storage/v1/b/cnpg-ogenki 12storageClass: STANDARD 13timeCreated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; 14updated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; Nous allons maintenant configurer les permissions afin que les pods (PostgreSQL Server) puissent permettant Ã©crire/lire Ã  partir du bucket grÃ¢ce Ã  Workload Identity.\nNote Workload Identity doit Ãªtre activÃ© au niveau du cluster GKE. Afin de vÃ©rifier que le cluster est bien configurÃ©, vous pouvez lancer la commande suivante:\n1gcloud container clusters describe \u0026lt;cluster_name\u0026gt; --format json --zone \u0026lt;zone\u0026gt; | jq .workloadIdentityConfig 2{ 3 \u0026#34;workloadPool\u0026#34;: \u0026#34;{{ gcp_project }}.svc.id.goog\u0026#34; 4} CrÃ©er un compte de service Google Cloud\n1gcloud iam service-accounts create cloudnative-pg --project={{ gcp_project }} 2Created service account [cloudnative-pg]. Attribuer au compte de service la permission storage.admin\n1gcloud projects add-iam-policy-binding {{ gcp_project }} \\ 2--member \u0026#34;serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com\u0026#34; \\ 3--role \u0026#34;roles/storage.admin\u0026#34; 4[...] 5- members: 6 - serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 7 role: roles/storage.admin 8etag: BwXrGA_VRd4= 9version: 1 Autoriser le compte de service (Attention il s'agit lÃ  du compte de service au niveau Kubernetes) afin d'usurper le compte de service IAM. \u0026#x2139;\u0026#xfe0f; Assurez-vous d'utiliser le format appropriÃ© serviceAccount:{{ gcp_project }}.svc.id.goog[{{ kubernetes_namespace }}/{{ kubernetes_serviceaccount }}]\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 7 role: roles/iam.workloadIdentityUser 8etag: BwXrGBjt5kQ= 9version: 1 Nous sommes prÃªts Ã  crÃ©er les ressources Kubernetes \u0026#x1f4aa;\n\u0026#x1f511; CrÃ©er les secrets pour les utilisateurs PostgreSQL Nous devons crÃ©er les paramÃ¨tres d'authentification des utilisateurs qui seront crÃ©Ã©s pendant la phase de \u0026quot;bootstrap\u0026quot; (nous y reviendrons par la suite): le superutilisateur et le propriÃ©taire de base de donnÃ©es nouvellement crÃ©Ã©.\n1kubectl create secret generic cnpg-mydb-superuser --from-literal=username=postgres --from-literal=password=foobar --namespace demo 2secret/cnpg-mydb-superuser created 1kubectl create secret generic cnpg-mydb-user --from-literal=username=smana --from-literal=password=barbaz --namespace demo 2secret/cnpg-mydb-user created \u0026#x1f6e0;\u0026#xfe0f; DÃ©ployer l'opÃ©rateur CloudNativePG avec Helm Ici nous utiliserons le chart Helm pour dÃ©ployer CloudNativePG:\n1helm repo add cnpg https://cloudnative-pg.github.io/charts 2 3helm upgrade --install cnpg --namespace cnpg-system \\ 4--create-namespace charts/cloudnative-pg 5 6kubectl get po -n cnpg-system 7NAME READY STATUS RESTARTS AGE 8cnpg-74488f5849-8lhjr 1/1 Running 0 6h17m Cela installe aussi quelques resources personnalisÃ©es (Custom Resources Definitions)\n1kubectl get crds | grep cnpg.io 2backups.postgresql.cnpg.io 2022-10-08T16:15:14Z 3clusters.postgresql.cnpg.io 2022-10-08T16:15:14Z 4poolers.postgresql.cnpg.io 2022-10-08T16:15:14Z 5scheduledbackups.postgresql.cnpg.io 2022-10-08T16:15:14Z Pour une liste complÃ¨te des paramÃ¨tres possibles, veuillez vous rÃ©fÃ©rer Ã  la doc de l'API.\n\u0026#x1f680; CrÃ©er un serveur PostgreSQL Nous pouvons dÃ©sormais crÃ©er notre premiÃ¨re instance en utilisant une resource personnalisÃ©e Cluster. La dÃ©finition suivante est assez simple: Nous souhaitons dÃ©marrer un serveur PostgreSQL, crÃ©er automatiquement une base de donnÃ©es nommÃ©e mydb et configurer les informations d'authentification en utilisant les secrets crÃ©Ã©s prÃ©cÃ©demment.\n1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki 5 namespace: demo 6spec: 7 description: \u0026#34;PostgreSQL Demo Ogenki\u0026#34; 8 imageName: ghcr.io/cloudnative-pg/postgresql:14.5 9 instances: 1 10 11 bootstrap: 12 initdb: 13 database: mydb 14 owner: smana 15 secret: 16 name: cnpg-mydb-user 17 18 serviceAccountTemplate: 19 metadata: 20 annotations: 21 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 22 23 superuserSecret: 24 name: cnpg-mydb-superuser 25 26 storage: 27 storageClass: standard 28 size: 10Gi 29 30 backup: 31 barmanObjectStore: 32 destinationPath: \u0026#34;gs://cnpg-ogenki\u0026#34; 33 googleCredentials: 34 gkeEnvironment: true 35 retentionPolicy: \u0026#34;30d\u0026#34; 36 37 resources: 38 requests: 39 memory: \u0026#34;1Gi\u0026#34; 40 cpu: \u0026#34;500m\u0026#34; 41 limits: 42 memory: \u0026#34;1Gi\u0026#34; CrÃ©er le namespace oÃ¹ notre instance postgresql sera dÃ©ployÃ©e\n1kubectl create ns demo 2namespace/demo created Adapdez le fichier YAML ci-dessus vos besoins et appliquez comme suit:\n1kubectl apply -f cluster.yaml 2cluster.postgresql.cnpg.io/ogenki created Vous remarquerez que le cluster sera en phase Initializing. Nous allons utiliser le plugin CNPG pour la premiÃ¨re fois afin de vÃ©rifier son Ã©tat. Cet outil deviendra par la suite notre meilleur ami pour afficher une vue synthÃ©tique de l'Ã©tat du cluster.\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Primary server is initializing 4Name: ogenki 5Namespace: demo 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: (switching to ogenki-1) 8Status: Setting up primary Creating primary instance ogenki-1 9Instances: 1 10Ready instances: 0 11 12Certificates Status 13Certificate Name Expiration Date Days Left Until Expiration 14---------------- --------------- -------------------------- 15ogenki-ca 2023-01-13 20:02:40 +0000 UTC 90.00 16ogenki-replication 2023-01-13 20:02:40 +0000 UTC 90.00 17ogenki-server 2023-01-13 20:02:40 +0000 UTC 90.00 18 19Continuous Backup status 20First Point of Recoverability: Not Available 21No Primary instance found 22Streaming Replication status 23Not configured 24 25Instances status 26Name Database Size Current LSN Replication role Status QoS Manager Version Node 27---- ------------- ----------- ---------------- ------ --- --------------- ---- immÃ©diatement aprÃ¨s la dÃ©claration de notre nouveau Cluster, une action de bootstrap est lancÃ©e. Dans notre exemple, nous crÃ©ons une toute nouvelle base de donnÃ©es nommÃ©e mydb avec un propriÃ©taire smana dont les informations d'authentification viennent du secret crÃ©Ã© prÃ©cÃ©demment.\n1[...] 2 bootstrap: 3 initdb: 4 database: mydb 5 owner: smana 6 secret: 7 name: cnpg-mydb-user 8[...] 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 0/1 Running 0 55s 4ogenki-1-initdb-q75cz 0/1 Completed 0 2m32s AprÃ¨s quelques secondes, le cluster change de statut et devient Ready (configurÃ© et prÃªt Ã  l'usage) \u0026#x1f44f;\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7154833472216277012 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 1 10Ready instances: 1 11 12[...] 13 14Instances status 15Name Database Size Current LSN Replication role Status QoS Manager Version Node 16---- ------------- ----------- ---------------- ------ --- --------------- ---- 17ogenki-1 33 MB 0/17079F8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xczh Info Il existe de nombreuses faÃ§ons de bootstrap un cluster. Par exemple, la restauration d'une sauvegarde dans une toute nouvelle instance ou en exÃ©cutant du code SQL ... Plus d'infos ici.\nğŸ©¹ Instance de secours et rÃ©silience Info Dans les architectures postgresql traditionnelles, nous trouvons gÃ©nÃ©ralement un composant supplÃ©mentaire pour gÃ©rer la haute disponibilitÃ© (ex: Patroni). Un particularitÃ© de l'opÃ©rateur CloudNativePG est qu'il bÃ©nÃ©ficie des fonctionnalitÃ©s de base de Kubernetes et s'appuie sur un composant nommÃ© Postgres instance manager.\nAjoutez une instance de secours (\u0026quot;standby\u0026quot;) en dÃ©finissant le nombre de rÃ©pliques sur 2.\n1kubectl edit cluster -n demo ogenki 2cluster.postgresql.cnpg.io/ogenki edited 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3[...] 4spec: 5 instances: 2 6[...] L'opÃ©rateur remarque immÃ©diatement le changement, ajoute une instance de secours et dÃ©marre le processus de rÃ©plication.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Creating a new replica Creating replica ogenki-2-join 9Instances: 2 10Ready instances: 1 11Current Write LSN: 0/1707A30 (Timeline: 1 - WAL File: 000000010000000000000001) 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 0 3m16s 4ogenki-2-join-xxrwx 0/1 Pending 0 82s AprÃ¨s un certain temps (qui dÃ©pend de la quantitÃ© de donnÃ©es Ã  rÃ©pliquer), l'instance de secours devient opÃ©rationnelle et nous pouvons voir les statistiques de rÃ©plication.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 2 10Ready instances: 2 11Current Write LSN: 0/3000060 (Timeline: 1 - WAL File: 000000010000000000000003) 12 13[...] 14 15Streaming Replication status 16Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 17---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 18ogenki-2 0/3000060 0/3000060 0/3000060 0/3000060 00:00:00 00:00:00 00:00:00 streaming async 0 19 20Instances status 21Name Database Size Current LSN Replication role Status QoS Manager Version Node 22---- ------------- ----------- ---------------- ------ --- --------------- ---- 23ogenki-1 33 MB 0/3000060 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 24ogenki-2 33 MB 0/3000060 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc Nous allons dÃ©sormais Ã©ffectuer ce que l'on appelle un \u0026quot;Switchover\u0026quot;: Nous allons promouvoir l'instance de secours en instance primaire.\nLe plugin cnpg permet de le faire de faÃ§on impÃ©rative, en utilisant la ligne de commande suivante:\n1kubectl cnpg promote ogenki ogenki-2 -n demo 2Node ogenki-2 in cluster ogenki will be promoted Dans mon cas, le basculement Ã©tait vraiment rapide. Nous pouvons vÃ©rifier que l'instance ogenki-2 est devenu primaire et que la rÃ©plication est effectuÃ©e dans l'autre sens.\n1kubectl cnpg status -n demo ogenki 2[...] 3Status: Switchover in progress Switching over to ogenki-2 4Instances: 2 5Ready instances: 1 6[...] 7Streaming Replication status 8Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 9---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 10ogenki-1 0/4004CA0 0/4004CA0 0/4004CA0 0/4004CA0 00:00:00 00:00:00 00:00:00 streaming async 0 11 12Instances status 13Name Database Size Current LSN Replication role Status QoS Manager Version Node 14---- ------------- ----------- ---------------- ------ --- --------------- ---- 15ogenki-2 33 MB 0/4004CA0 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc 16ogenki-1 33 MB 0/4004CA0 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 Maintenant, provoquons un Failover en supprimant le pod principal\n1kubectl delete po -n demo --grace-period 0 --force ogenki-2 2Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. 3pod \u0026#34;ogenki-2\u0026#34; force deleted 1Cluster Summary 2Name: ogenki 3Namespace: demo 4System ID: 7155095145869606932 5PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 6Primary instance: ogenki-1 7Status: Failing over Failing over from ogenki-2 to ogenki-1 8Instances: 2 9Ready instances: 1 10Current Write LSN: 0/4005D98 (Timeline: 3 - WAL File: 000000030000000000000004) 11 12[...] 13Instances status 14Name Database Size Current LSN Replication role Status QoS Manager Version Node 15---- ------------- ----------- ---------------- ------ --- --------------- ---- 16ogenki-1 33 MB 0/40078D8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 17ogenki-2 - - - pod not available Burstable - gke-kcdfrance-main-np-0e87115b-xszc Quelques secondes plus tard le cluster devient opÃ©rationnel Ã  nouveau.\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 13m 2 2 Cluster in healthy state ogenki-1 Jusqu'ici tout va bien, nous avons pu faire quelques tests de la haute disponibilitÃ© et c'Ã©tait assez probant ğŸ˜.\nğŸ‘ï¸ Supervision Nous allons utiliser la Stack Prometheus. Nous ne couvrirons pas son installation dans cet article. Si vous voulez voir comment l'installer avec Flux, vous pouvez jeter un oeil Ã  cet exemple.\nPour rÃ©cupÃ©rer les mÃ©triques de notre instance, nous devons crÃ©er un PodMonitor.\n1apiVersion: monitoring.coreos.com/v1 2kind: PodMonitor 3metadata: 4 labels: 5 prometheus-instance: main 6 name: cnpg-ogenki 7 namespace: demo 8spec: 9 namespaceSelector: 10 matchNames: 11 - demo 12 podMetricsEndpoints: 13 - port: metrics 14 selector: 15 matchLabels: 16 postgresql: ogenki Nous pouvons ensuite ajouter le tableau de bord Grafana disponible ici.\nEnfin, vous souhaiterez peut-Ãªtre configurer des alertes et vous pouvez crÃ©er un PrometheusRule en utilisant ces rÃ¨gles.\n\u0026#x1f525; Performances and benchmark Info Mise Ã  jour: Il est dÃ©sormais possible de faire un test de performance avec le plugin cnpg\nAfin de connaitre les limites de votre serveur, vous devriez faire un test de performances et de conserver une base de rÃ©fÃ©rence pour de futures amÃ©liorations.\nNote Au sujet des performances, il existe de nombreux domaines d'amÃ©lioration sur lesquels nous pouvons travailler.Cela dÃ©pend principalement de l'objectif que nous voulons atteindre. En effet, nous ne voulons pas perdre du temps et de l'argent pour les performances dont nous n'aurons probablement jamais besoin.\nVoici les principaux Ã©lÃ©ments Ã  analyser:\nTuning de la configuration PostgreSQL Resources systÃ¨mes (cpu et mÃ©moire) Types de Disque : IOPS, stockage locale (local-volume-provisioner), Disques dÃ©diÃ©es pour les WAL et les donnÃ©es PG_DATA \u0026quot;Pooling\u0026quot; de connexions PGBouncer. CloudNativePG fourni une resource personnalisÃ©e Pooler qui permet de configurer cela facilement. Optimisation de la base de donnÃ©es, analyser les plans d'exÃ©cution grÃ¢ce Ã  explain, utiliser l'extension pg_stat_statement ... Tout d'abord, nous ajouterons des \u0026quot;labels\u0026quot; aux nÅ“uds afin d'exÃ©cuter la commande pgbench sur diffÃ©rentes machines de celles hÃ©bergeant la base de donnÃ©es.\n1PG_NODE=$(kubectl get po -n demo -l postgresql=ogenki,role=primary -o jsonpath={.items[0].spec.nodeName}) 2kubectl label node ${PG_NODE} workload=postgresql 3node/gke-kcdfrance-main-np-0e87115b-vlzm labeled 4 5 6# Choose any other node different than the ${PG_NODE} 7kubectl label node gke-kcdfrance-main-np-0e87115b-p5d7 workload=pgbench 8node/gke-kcdfrance-main-np-0e87115b-p5d7 labeled Et nous dÃ©ploierons le chart Helm comme suit\n1git clone git@github.com:EnterpriseDB/cnp-bench.git 2cd cnp-bench 3 4cat \u0026gt; pgbench-benchmark/myvalues.yaml \u0026lt;\u0026lt;EOF 5cnp: 6 existingCluster: true 7 existingHost: ogenki-rw 8 existingCredentials: cnpg-mydb-superuser 9 existingDatabase: mydb 10 11pgbench: 12 # Node where to run pgbench 13 nodeSelector: 14 workload: pgbench 15 initialize: true 16 scaleFactor: 1 17 time: 600 18 clients: 10 19 jobs: 1 20 skipVacuum: false 21 reportLatencies: false 22EOF 23 24helm upgrade --install -n demo pgbench -f pgbench-benchmark/myvalues.yaml pgbench-benchmark/ Info Il existe diffÃ©rents services selon que vous souhaitez lire et Ã©crire ou de la lecture seule.\n1kubectl get ep -n demo 2NAME ENDPOINTS AGE 3ogenki-any 10.64.1.136:5432,10.64.1.3:5432 15d 4ogenki-r 10.64.1.136:5432,10.64.1.3:5432 15d 5ogenki-ro 10.64.1.136:5432 15d 6ogenki-rw 10.64.1.3:5432 15d 1kubectl logs -n demo job/pgbench-pgbench-benchmark -f 2Defaulted container \u0026#34;pgbench\u0026#34; out of: pgbench, wait-for-cnp (init), pgbench-init (init) 3pgbench (14.1, server 14.5 (Debian 14.5-2.pgdg110+2)) 4starting vacuum...end. 5transaction type: \u0026lt;builtin: TPC-B (sort of)\u0026gt; 6scaling factor: 1 7query mode: simple 8number of clients: 10 9number of threads: 1 10duration: 600 s 11number of transactions actually processed: 545187 12latency average = 11.004 ms 13initial connection time = 111.585 ms 14tps = 908.782896 (without initial connection time) ğŸ’½ Sauvegarde and restaurations Note Le fait de pouvoir stocker des sauvegarde et fichiers WAL dans le bucket GCP est possible car nous avons attribuÃ© les autorisations en utilisant une annotation prÃ©sente dans le ServiceAccount utilisÃ© par le cluster\n1serviceAccountTemplate: 2 metadata: 3 annotations: 4 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com Nous pouvons d'abord dÃ©clencher une sauvegarde on demand Ã  l'aide de la ressource personnalisÃ©e Backup\n1apiVersion: postgresql.cnpg.io/v1 2kind: Backup 3metadata: 4 name: ogenki-now 5 namespace: demo 6spec: 7 cluster: 8 name: ogenki 1kubectl apply -f backup.yaml 2backup.postgresql.cnpg.io/ogenki-now created 3 4kubectl get backup -n demo 5NAME AGE CLUSTER PHASE ERROR 6ogenki-now 36s ogenki completed Si vous jetez un Å“il au contenu du bucket GCS, vous verrez un nouveau rÃ©pertoire qui stocke les sauvegardes de base (\u0026quot;base backups\u0026quot;).\n1gcloud storage ls gs://cnpg-ogenki/ogenki/base 2gs://cnpg-ogenki/ogenki/base/20221023T130327/ Mais la plupart du temps, nous prÃ©fererons configurer une sauvegarde planifiÃ©e (\u0026quot;scheduled\u0026quot;). Ci-dessous un exemple pour une sauvegarde quotidienne:\n1apiVersion: postgresql.cnpg.io/v1 2kind: ScheduledBackup 3metadata: 4 name: ogenki-daily 5 namespace: demo 6spec: 7 backupOwnerReference: self 8 cluster: 9 name: ogenki 10 schedule: 0 0 0 * * * Les restaurations ne peuvent Ãªtre effectuÃ©es que sur de nouvelles instances. Ici, nous utiliserons la sauvegarde que nous avions crÃ©Ã©e prÃ©cÃ©demment afin d'initialiser une toute nouvelle instance.\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore] 7 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 8 role: roles/iam.workloadIdentityUser 9etag: BwXrs755FPA= 10version: 1 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki-restore 5 namespace: demo 6spec: 7 instances: 1 8 9 serviceAccountTemplate: 10 metadata: 11 annotations: 12 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 13 14 storage: 15 storageClass: standard 16 size: 10Gi 17 18 resources: 19 requests: 20 memory: \u0026#34;1Gi\u0026#34; 21 cpu: \u0026#34;500m\u0026#34; 22 limits: 23 memory: \u0026#34;1Gi\u0026#34; 24 25 superuserSecret: 26 name: cnpg-mydb-superuser 27 28 bootstrap: 29 recovery: 30 backup: 31 name: ogenki-now Nous notons qu'un pod se charge immÃ©diatement de la restauration complÃ¨te (\u0026quot;full recovery\u0026quot;).\n1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 1 (18h ago) 18h 4ogenki-2 1/1 Running 0 18h 5ogenki-restore-1 0/1 Init:0/1 0 0s 6ogenki-restore-1-full-recovery-5p4ct 0/1 Completed 0 51s Le nouveau cluster devient alors opÃ©rationnel (\u0026quot;Ready\u0026quot;).\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 18h 2 2 Cluster in healthy state ogenki-1 4ogenki-restore 80s 1 1 Cluster in healthy state ogenki-restore-1 \u0026#x1f9f9; Nettoyage Suppression du cluster\n1kubectl delete cluster -n demo ogenki ogenki-restore 2cluster.postgresql.cnpg.io \u0026#34;ogenki\u0026#34; deleted 3cluster.postgresql.cnpg.io \u0026#34;ogenki-restore\u0026#34; deleted Supprimer le service IAM\n1gcloud iam service-accounts delete cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 2You are about to delete service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 3 4Do you want to continue (Y/n)? y 5 6deleted service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com] ğŸ’­ Conclusion Je viens tout juste de dÃ©couvrir CloudNativePG et je n'ai fait qu'en percevoir la surface, mais une chose est sÃ»re: la gestion d'une instance PostgreSQL est vraiment facilitÃ©e. Cependant, le choix d'une solution de base de donnÃ©es est une dÃ©cision complexe. Il faut prendre en compte le cas d'usage, les contraintes de l'entreprise, la criticitÃ© de l'application et les compÃ©tences des Ã©quipes opÃ©rationnelles. Il existe de nombreuses options: bases de donnÃ©es gÃ©rÃ©es par le fournisseur Cloud, installation traditionnelle sur serveur baremetal, solutions distribuÃ©es ...\nNous pouvons Ã©galement envisager d'utiliser Crossplane et une Composition pour fournir un niveau d'abstraction supplÃ©mentaire afin de dÃ©clarer des bases de donnÃ©es des fournisseurs Cloud, mais cela nÃ©cessite plus de configuration.\nCloudNativePG sort du lot par sa simplicitÃ©: Super facile Ã  exÃ©cuter et Ã  comprendre. De plus, la documentation est excellente (l'une des meilleures que j'aie jamais vues!), Surtout pour un si jeune projet open source (cela aidera peut Ãªtre pour Ãªtre acceptÃ© en tant que projet \u0026quot;Sandbox\u0026quot; CNCF ğŸ¤).\nSi vous voulez en savoir plus, il y avait une prÃ©sentation Ã  ce sujet Ã  KubeCon NA 2022.\n","link":"https://blog.ogenki.io/fr/post/cnpg/","section":"post","tags":["data"],"title":"`CloudNativePG`: et PostgreSQL devient facile sur Kubernetes"},{"body":"","link":"https://blog.ogenki.io/fr/tags/data/","section":"tags","tags":null,"title":"Data"},{"body":"In a previous article, we've seen how to use Crossplane so that we can manage cloud resources the same way as our applications. \u0026#x2764;\u0026#xfe0f; Declarative approach! There were several steps and command lines in order to get everything working and reach our target to provision a dev Kubernetes cluster.\nHere we'll achieve exactly the same thing but we'll do that in the GitOps way. According to the OpenGitOps working group there are 4 GitOps principles:\nThe desired state of our system must be expressed declaratively. This state must be stored in a versioning system. Changes are pulled and applied automatically in the target platform whenever the desired state changes. If, for any reason, the current state is modified, it will be automatically reconciled with the desired state. There are several GitOps engine options. The most famous ones are ArgoCD and Flux. We won't compare them here. I chose Flux because I like its composable architecture with different controllers, each one handling a core Flux feature (GitOps toolkit).\nLearn more about GitOps toolkit components here.\nğŸ¯ Our target Here we want to declare our desired infrastructure components only by adding git changes. By the end of this article you'll get a GKE cluster provisioned using a local Crossplane instance. We'll discover Flux basics and how to use it in order to build a complete GitOps CD workflow.\n\u0026#x2611;\u0026#xfe0f; Requirements \u0026#x1f4e5; Install required tools First of all we need to install a few tools using asdf\nCreate a local file .tool-versions\n1cd ~/sources/devflux/ 2 3cat \u0026gt; .tool-versions \u0026lt;\u0026lt;EOF 4flux2 0.31.3 5kubectl 1.24.3 6kubeseal 0.18.1 7kustomize 4.5.5 8EOF 1for PLUGIN in $(cat .tool-versions | awk \u0026#39;{print $1}\u0026#39;); do asdf plugin-add $PLUGIN; done 2 3asdf install 4Downloading ... 100.0% 5Copying Binary 6... Check that all the required tools are actually installed.\n1asdf current 2flux2 0.31.3 /home/smana/sources/devflux/.tool-versions 3kubectl 1.24.3 /home/smana/sources/devflux/.tool-versions 4kubeseal 0.18.1 /home/smana/sources/devflux/.tool-versions 5kustomize 4.5.5 /home/smana/sources/devflux/.tool-versions \u0026#x1f511; Create a Github personal access token In this article the git repository is hosted in Github. In order to be able to use the flux bootstrap a personnal access token is required.\nPlease follow this procedure.\nWarning Store the Github token in a safe place for later use\n\u0026#x1f9d1;\u0026zwj;\u0026#x1f4bb; Clone the devflux repository All the files used for the upcoming steps can be retrieved from this repository. You should clone it, that will be easier to copy them into your own repository.\n1git clone https://github.com/Smana/devflux.git \u0026#x1f680; Bootstrap flux in the Crossplane cluster As we will often be using the flux CLI you may want to configure the bash|zsh completion\n1source \u0026lt;(flux completion bash) Warning Here we consider that you already have a local k3d instance. If not you may want to either go through the whole previous article or just run the local cluster creation.\nEnsure that you're working in the right context\n1kubectl config current-context 2k3d-crossplane Run the bootstrap command that will basically deploy all Flux's components in the namespace flux-system. Here I'll create a repository named devflux using my personal Github account.\n1export GITHUB_USER=\u0026lt;YOUR_ACCOUNT\u0026gt; 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/k3d-crossplane 6â–º cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git\u0026#34; 7... 8âœ” configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/k3d-crossplane\u0026#34; for \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux\u0026#34; 9... 10âœ” all components are healthy Check that all the pods are running properly and that the kustomization flux-system has been successfully reconciled.\n1kubectl get po -n flux-system 2NAME READY STATUS RESTARTS AGE 3helm-controller-5985c795f8-gs2pc 1/1 Running 0 86s 4notification-controller-6b7d7485fc-lzlpg 1/1 Running 0 86s 5kustomize-controller-6d4669f847-9x844 1/1 Running 0 86s 6source-controller-5fb4888d8f-wgcqv 1/1 Running 0 86s 7 8flux get kustomizations 9NAME REVISION SUSPENDED READY MESSAGE 10flux-system main/33ebef1 False True Applied revision: main/33ebef1 Running the bootstap command actually creates a github repository if it doesn't exist yet. Clone it now for our upcoming changes. You'll notice that the first commit has been made by Flux.\n1git clone https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git 2Cloning into \u0026#39;devflux\u0026#39;... 3 4cd devflux 5 6git log -1 7commit 2beb6aafea67f3386b50cbc706fb34575844040d (HEAD -\u0026gt; main, origin/main, origin/HEAD) 8Author: Flux \u0026lt;\u0026gt; 9Date: Thu Jul 14 17:13:27 2022 +0200 10 11 Add Flux sync manifests 12 13ls clusters/k3d-crossplane/flux-system/ 14gotk-components.yaml gotk-sync.yaml kustomization.yaml \u0026#x1f4c2; Flux repository structure There are several options for organizing your resources in the Flux configuration repository. Here is a proposition for the sake of this article.\n1tree -d -L 2 2. 3â”œâ”€â”€ apps 4â”‚Â â”œâ”€â”€ base 5â”‚Â â””â”€â”€ dev-cluster 6â”œâ”€â”€ clusters 7â”‚Â â”œâ”€â”€ dev-cluster 8â”‚Â â””â”€â”€ k3d-crossplane 9â”œâ”€â”€ infrastructure 10â”‚Â â”œâ”€â”€ base 11â”‚Â â”œâ”€â”€ dev-cluster 12â”‚Â â””â”€â”€ k3d-crossplane 13â”œâ”€â”€ observability 14â”‚Â â”œâ”€â”€ base 15â”‚Â â”œâ”€â”€ dev-cluster 16â”‚Â â””â”€â”€ k3d-crossplane 17â””â”€â”€ security 18 â”œâ”€â”€ base 19 â”œâ”€â”€ dev-cluster 20 â””â”€â”€ k3d-crossplane Directory Description Example /apps our applications Here we'll deploy a demo application \u0026quot;online-boutique\u0026quot; /infrastructure base infrastructure/network components Crossplane as it will be used to provision cloud resources but we can also find CSI/CNI/EBS drivers... /observability All metrics/apm/logging tools Prometheus of course, Opentelemetry ... /security Any component that enhance our security level SealedSecrets (see below) Info For the upcoming steps please refer to the demo repository here\nLet's use this structure and begin to deploy applications \u0026#x1f680;.\n\u0026#x1f510; SealedSecrets There are plenty of alternatives when it comes to secrets management in Kubernetes. In order to securely store secrets in a git repository the GitOps way we'll make use of SealedSecrets. It uses a custom resource definition named SealedSecrets in order to encrypt the Kubernetes secret at the client side then the controller is in charge of decrypting and generating the expected secret in the cluster.\n\u0026#x1f6e0;\u0026#xfe0f; Deploy the controller using Helm The first thing to do is to declare the kustomization that handles all the security tools.\nclusters/k3d-crossplane/security.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: security 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 sourceRef: 10 kind: GitRepository 11 name: flux-system 12 path: ./security/k3d-crossplane 13 healthChecks: 14 - apiVersion: helm.toolkit.fluxcd.io/v1beta1 15 kind: HelmRelease 16 name: sealed-secrets 17 namespace: kube-system Info A Kustomization is a custom resource that comes with Flux. It basically points to a set of Kubernetes resources managed with kustomize The above security kustomization points to a local directory where the kustomize resources are.\n1... 2spec: 3 path: ./security/k3d-crossplane 4... Note This is worth noting that there are two types on kustomizations. That can be confusing when you start playing with Flux.\nOne managed by flux's kustomize controller. Its API is kustomization.kustomize.toolkit.fluxcd.io The other kustomization.kustomize.config.k8s.io is for the kustomize overlay The kustomization.yaml file is always used for the kustomize overlay. Flux itself doesn't need this overlay in all cases, but if you want to use features of a Kustomize overlay you will occasionally need to create it in order to access them. It provides instructions to the Kustomize CLI.\nWe will deploy SealedSecrets using the Helm chart. So we need to declare the source of this chart. Using the kustomize overlay system, we'll first create the base files that will be inherited at the cluster level.\nsecurity/base/sealed-secrets/source.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: sealed-secrets 5 namespace: flux-system 6spec: 7 interval: 30m 8 url: https://bitnami-labs.github.io/sealed-secrets Then we'll define the HelmRelease which references the above source. Put the values you want to apply to the Helm chart under spec.values\nsecurity/base/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 releaseName: sealed-secrets 8 chart: 9 spec: 10 chart: sealed-secrets 11 sourceRef: 12 kind: HelmRepository 13 name: sealed-secrets 14 namespace: flux-system 15 version: \u0026#34;2.4.0\u0026#34; 16 interval: 10m0s 17 install: 18 remediation: 19 retries: 3 20 values: 21 fullnameOverride: sealed-secrets-controller 22 resources: 23 requests: 24 cpu: 80m 25 memory: 100Mi If you're starting your repository from scratch you'll need to generate the kustomization.yaml file (kustomize overlay).\n1kustomize create --autodetect security/base/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4- helmrelease.yaml 5- source.yaml Now we declare the sealed-secret kustomization at the cluster level. Just for the example we'll overwrite a value at the cluster level using kustomize's overlay system.\nsecurity/k3d-crossplane/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 values: 8 resources: 9 requests: 10 cpu: 100m security/k3d-crossplane/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3bases: 4 - ../../base 5patches: 6 - helmrelease.yaml Pushing our changes is the only thing to do in order to get sealed-secrets deployed in the target cluster.\n1git commit -m \u0026#34;security: deploy sealed-secrets in k3d-crossplane\u0026#34; 2[security/sealed-secrets 283648e] security: deploy sealed-secrets in k3d-crossplane 3 6 files changed, 66 insertions(+) 4 create mode 100644 clusters/k3d-crossplane/security.yaml 5 create mode 100644 security/base/sealed-secrets/helmrelease.yaml 6 create mode 100644 security/base/sealed-secrets/kustomization.yaml 7 create mode 100644 security/base/sealed-secrets/source.yaml 8 create mode 100644 security/k3d-crossplane/sealed-secrets/helmrelease.yaml 9 create mode 100644 security/k3d-crossplane/sealed-secrets/kustomization.yaml After a few seconds (1 minutes by default) a new kustomization will appear.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3flux-system main/d36a33c False True Applied revision: main/d36a33c 4security main/d36a33c False True Applied revision: main/d36a33c And all the resources that we declared in the flux repository should be available and READY.\n1flux get sources helm 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee False True stored artifact for revision \u0026#39;4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee\u0026#39; 1flux get helmrelease -n kube-system 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 2.2.0 False True Release reconciliation succeeded ğŸ§ª A first test SealedSecret Let's use the CLI kubeseal to test it out. We'll create a SealedSecret that will be decrypted by the sealed-secrets controller in the cluster and create the expected secret foobar\n1kubectl create secret generic foobar -n default --dry-run=client -o yaml --from-literal=foo=bar \\ 2| kubeseal --namespace default --format yaml | kubectl apply -f - 3sealedsecret.bitnami.com/foobar created 4 5kubectl get secret -n default foobar 6NAME TYPE DATA AGE 7foobar Opaque 1 3m13s 8 9kubectl delete sealedsecrets.bitnami.com foobar 10sealedsecret.bitnami.com \u0026#34;foobar\u0026#34; deleted \u0026#x2601;\u0026#xfe0f; Deploy and configure Crossplane \u0026#x1f511; Create the Google service account secret The first thing we need to do in order to get Crossplane working is to create the GCP serviceaccount. The steps have been covered here in the previous article. We'll create a SealedSecret gcp-creds that contains the serviceaccount file crossplane.json.\ninfrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml\n1kubectl create secret generic gcp-creds --context k3d-crossplane -n crossplane-system --from-file=creds=./crossplane.json --dry-run=client -o yaml \\ 2| kubeseal --format yaml --namespace crossplane-system - \u0026gt; infrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml ğŸ”„ Crossplane dependencies Now we will deploy Crossplane with Flux. I won't put the manifests here you'll find all of them in this repository. However it's important to understand that, in order to deploy and configure Crossplane properly we need to do that in a specific order. Indeed several CRD's (custom resource definitions) are required:\nFirst of all we'll install the crossplane controller. Then we'll configure the provider because the custom resource is now available thanks to the crossplane controller installation. Finally a provider installation deploys several CRDs that can be used to configure the provider itself and cloud resources. The dependencies between kustomizations can be controlled using the parameters dependsOn. Looking at the file clusters/k3d-crossplane/infrastructure.yaml, we can see for example that the kustomization infrastructure-custom-resources depends on the kustomization crossplane_provider which itself depends on crossplane-configuration....\n1--- 2apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 3kind: Kustomization 4metadata: 5 name: crossplane-provider 6spec: 7... 8 dependsOn: 9 - name: crossplane-core 10--- 11apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 12kind: Kustomization 13metadata: 14 name: crossplane-configuration 15spec: 16... 17 dependsOn: 18 - name: crossplane-provider 19--- 20apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 21kind: Kustomization 22metadata: 23 name: infrastructure-custom-resources 24spec: 25... 26 dependsOn: 27 - name: crossplane-configuration Commit and push the changes for the kustomisations to appear. Note that they'll be reconciled in the defined order.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3infrastructure-custom-resources False False dependency \u0026#39;flux-system/crossplane-configuration\u0026#39; is not ready 4crossplane-configuration False False dependency \u0026#39;flux-system/crossplane-provider\u0026#39; is not ready 5security main/666f85a False True Applied revision: main/666f85a 6flux-system main/666f85a False True Applied revision: main/666f85a 7crossplane-core main/666f85a False True Applied revision: main/666f85a 8crossplane-provider main/666f85a False True Applied revision: main/666f85a Then all Crossplane components will be deployed, we can have a look to the HelmRelease status for instance.\n1kubectl describe helmrelease -n crossplane-system crossplane 2... 3Status: 4 Conditions: 5 Last Transition Time: 2022-07-15T19:12:04Z 6 Message: Release reconciliation succeeded 7 Reason: ReconciliationSucceeded 8 Status: True 9 Type: Ready 10 Last Transition Time: 2022-07-15T19:12:04Z 11 Message: Helm upgrade succeeded 12 Reason: UpgradeSucceeded 13 Status: True 14 Type: Released 15 Helm Chart: crossplane-system/crossplane-system-crossplane 16 Last Applied Revision: 1.9.0 17 Last Attempted Revision: 1.9.0 18 Last Attempted Values Checksum: 056dc1c6029b3a644adc7d6a69a93620afd25b65 19 Last Release Revision: 2 20 Observed Generation: 1 21Events: 22 Type Reason Age From Message 23 ---- ------ ---- ---- ------- 24 Normal info 20m helm-controller HelmChart \u0026#39;crossplane-system/crossplane-system-crossplane\u0026#39; is not ready 25 Normal info 20m helm-controller Helm upgrade has started 26 Normal info 19m helm-controller Helm upgrade succeeded And our GKE cluster should also be created because we defined a bunch of crossplane custom resources in infrastructure/k3d-crossplane/custom-resources/crossplane\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RUNNING 34.x.x.190 europe-west9-a 22m \u0026#x1f680; Bootstrap flux in the dev cluster Our local Crossplane cluster is now ready and it created our dev cluster and we also want it to be managed with Flux. So let's configure Flux for this dev cluster using the same bootstrap command.\nAuthenticate to the newly created cluster. The following command will automatically change your current context.\n1gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project \u0026lt;your_project\u0026gt; 2Fetching cluster endpoint and auth data. 3kubeconfig entry generated for dev-cluster. 4 5kubectl config current-context 6gke_\u0026lt;your_project\u0026gt;_europe-west9-a_dev-cluster Run the bootstrap command for the dev-cluster.\n1export GITHUB_USER=Smana 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/dev-cluster 6â–º cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/Smana/devflux.git\u0026#34; 7... 8âœ” configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/dev-cluster\u0026#34; for \u0026#34;https://github.com/Smana/devflux\u0026#34; 9... 10âœ” all components are healthy Note It's worth noting that each Kubernetes cluster generates its own sealing keys. That means that if you recreate the dev-cluster, you must regenerate all the sealedsecrets. In our example we declared a secret in order to set the Grafana credentials. Here's the command you need to run in order to create a new version of the sealedsecret and don't forget to use the proper context \u0026#x1f609;.\n1kubectl create secret generic kube-prometheus-stack-grafana \\ 2--from-literal=admin-user=admin --from-literal=admin-password=\u0026lt;yourpassword\u0026gt; --namespace observability --dry-run=client -o yaml \\ 3| kubeseal --namespace observability --format yaml \u0026gt; observability/dev-cluster/kube-prometheus-stack/sealedsecrets.yaml After a few seconds we'll get the following kustomizations deployed.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3apps main/1380eaa False True Applied revision: main/1380eaa 4flux-system main/1380eaa False True Applied revision: main/1380eaa 5observability main/1380eaa False True Applied revision: main/1380eaa 6security main/1380eaa False True Applied revision: main/1380eaa Here we configured the prometheus stack and deployed a demo microservices stack named \u0026quot;online-boutique\u0026quot; This demo application exposes the frontend through a service of type LoadBalancer.\n1kubectl get svc -n demo frontend-external 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3frontend-external LoadBalancer 10.140.174.201 34.155.121.2 80:31943/TCP 7m44s Use the EXTERNAL_IP\n\u0026#x1f575;\u0026#xfe0f; Troubleshooting The cheatsheet in Flux's documentation contains many ways for troubleshooting when something goes wrong. Here I'll just give a sample of my favorite command lines.\nObjects that aren't ready\n1flux get all -A --status-selector ready=false Checking the logs of a given kustomization\n1flux logs --kind kustomization --name infrastructure-custom-resources 22022-07-15T19:38:52.996Z info Kustomization/infrastructure-custom-resources.flux-system - server-side apply completed 32022-07-15T19:38:53.016Z info Kustomization/infrastructure-custom-resources.flux-system - Reconciliation finished in 66.12266ms, next run in 4m0s 42022-07-15T19:11:34.697Z info Kustomization/infrastructure-custom-resources.flux-system - Discarding event, no alerts found for the involved object Show how a given pod is managed by Flux.\n1flux trace -n crossplane-system pod/crossplane-5dc8d888d7-g95qx 2 3Object: Pod/crossplane-5dc8d888d7-g95qx 4Namespace: crossplane-system 5Status: Managed by Flux 6--- 7HelmRelease: crossplane 8Namespace: crossplane-system 9Revision: 1.9.0 10Status: Last reconciled at 2022-07-15 21:12:04 +0200 CEST 11Message: Release reconciliation succeeded 12--- 13HelmChart: crossplane-system-crossplane 14Namespace: crossplane-system 15Chart: crossplane 16Version: 1.9.0 17Revision: 1.9.0 18Status: Last reconciled at 2022-07-15 21:11:36 +0200 CEST 19Message: pulled \u0026#39;crossplane\u0026#39; chart with version \u0026#39;1.9.0\u0026#39; 20--- 21HelmRepository: crossplane 22Namespace: crossplane-system 23URL: https://charts.crossplane.io/stable 24Revision: 362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d 25Status: Last reconciled at 2022-07-15 21:11:35 +0200 CEST 26Message: stored artifact for revision \u0026#39;362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d\u0026#39; If you want to check what would be the changes before pushing your commit. In thi given example I just increased the cpu requests for the sealed-secrets controller.\n1flux diff kustomization security --path security/k3d-crossplane 2âœ“ Kustomization diffing... 3â–º HelmRelease/kube-system/sealed-secrets drifted 4 5metadata.generation 6 Â± value change 7 - 6 8 + 7 9 10spec.values.resources.requests.cpu 11 Â± value change 12 - 100m 13 + 120m 14 15âš ï¸ identified at least one change, exiting with non-zero exit code \u0026#x1f9f9; Cleanup Don't forget to delete the Cloud resources if you don't want to have a bad suprise \u0026#x1f4b5;! Just comment the file infrastructure/k3d-crossplane/custom-resources/crossplane/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 # - cluster.yaml 5 - network.yaml \u0026#x1f44f; Achievements With our current setup everything is configured using the GitOps approach:\nWe can manage infrastructure resources using Crossplane. Our secrets are securely stored in our git repository. We have a dev-cluster that we can enable or disable just but commenting a yaml file. Our demo application can be deployed from scratch in seconds. ğŸ’­ final thoughts Flux is probably the tool I'm using the most on a daily basis. It's really amazing!\nWhen you get familiar with its concepts and the command line it becomes really easy to use and troubleshoot. You can use either Helm when a chart is available or Kustomize.\nHowever we faced a few issues:\nIt's not straightforward to find an efficient structure depending on the company needs. Especially when you have several Kubernetes controllers that depend on other CRDs. The Helm controller doesn't maintain a state of the Kubernetes resources deployed by the Helm chart. That means that if you delete a resource which has been deployed through a Helm chart, it won't be reconciled (It will change soon. Being discussed here) Flux doesn't provide itself a web UI and switching between CLIs (kubectl, flux ...) can be annoying from a developer perspective. (I'm going to test weave-gitops ) I've been using Flux in production for more than a year and we configured it with the image automation so that the only thing a developer has to do is to merge a pull request and the new version of the application is automatically deployed in the target cluster.\nI should probably give another try to ArgoCD in order to be able to compare these precisely ğŸ¤”.\n","link":"https://blog.ogenki.io/fr/post/devflux/","section":"post","tags":["gitops","devxp"],"title":"100% `GitOps` using Flux"},{"body":"Qui suis-je? Je suis un ingÃ©nieur systÃ¨me / SRE senior. Je porte un intÃ©rÃªt particulier aux conteneurs Linux et les technologies dÃ®tes \u0026quot;Cloud Native\u0026quot;. Au fil de ma carriÃ¨re, j'ai eu l'occasion de collaborer avec une variÃ©tÃ© d'entreprises, des startups dynamiques aux grandes structures, oÃ¹ j'ai contribuÃ© Ã  optimiser la fiabilitÃ© et la disponibilitÃ© des plateformes, tout en amÃ©liorant l'expÃ©rience des dÃ©veloppeurs. Mon parcours inclut l'accompagnement de plusieurs sociÃ©tÃ©s dans leur migration vers des solutions Cloud. J'ai Ã©galement pu diriger des Ã©quipes SRE/DevOps composÃ©es de profils variÃ©s, incluant dÃ©veloppeurs et ingÃ©nieurs SRE, avec pour objectif un engagement collectif vers des objectifs communs.\nEn parallÃ¨le de mon parcours professionnel, je m'investis dans l'organisation du meetup Cloud Native Computing Ã  Paris ainsi que les Kubernetes Community Days France , reflÃ©tant mon engagement continu dans l'Ã©cosystÃ¨me du Cloud.\nAutres centre d'intÃ©ret : Lecture de romans de science-fiction, Kickboxing, Surf, Longboard et Rollers..\nLes images sympas (miniatures) de chaque article ont Ã©tÃ© gÃ©nÃ©rÃ©es avec DALL-E\n","link":"https://blog.ogenki.io/fr/about/","section":"","tags":null,"title":"About"},{"body":"La cible de cette documentation est de pouvoir crÃ©er et gÃ©rer un cluster GKE en utilisant Crossplane.\nCrossplane exploite les principes de base de Kubernetes afin de fournir des ressources cloud et bien plus encore: une ** approche dÃ©clarative ** avec ** Drift Detections ** et ** rÃ©conciliations ** Utilisation de boucles de contrÃ´le: Exploseding_head:.En d'autres termes, nous dÃ©clarons les ressources cloud que nous voulons et Crossplane garantit que l'Ã©tat cible correspond Ã  celui appliquÃ© via l'API Kubernetes.\nVoici les Ã©tapes que nous suivrons afin d'obtenir un cluster Kubernetes pour le dÃ©veloppement et les cas d'utilisation des expÃ©rimentations.\n\u0026#x1f433; Create the local k3d cluster for Crossplane's control plane k3d est un cluster Kubernetes lÃ©ger qui exploite K3S qui s'exÃ©cute dans notre ordinateur portable local. Il existe plusieurs modÃ¨les de dÃ©ploiement pour Crossplane, nous pourrions par exemple dÃ©ployer le plan de contrÃ´le sur un cluster de gestion sur Kubernetes ou un plan de contrÃ´le par cluster Kubernetes. Ici, j'ai choisi une mÃ©thode simple qui est bien pour un cas d'utilisation personnelle: une instance Kubernetes locale ** dans laquelle je vais dÃ©ployer Crossplane.\nLet's install k3d using asdf.\n1asdf plugin-add k3d 2 3asdf install k3d $(asdf latest k3d) 4* Downloading k3d release 5.4.1... 5k3d 5.4.1 installation was successful! Create a single node Kubernetes cluster.\n1k3d cluster create crossplane 2... 3INFO[0043] You can now use it like this: 4kubectl cluster-info 5 6k3d cluster list 7crossplane 1/1 0/0 true Check that the cluster is reachable using the kubectl CLI.\n1kubectl cluster-info 2Kubernetes control plane is running at https://0.0.0.0:40643 3CoreDNS is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4Metrics-server is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy We only need a single node for our Crossplane use case.\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k3d-crossplane-server-0 Ready control-plane,master 26h v1.22.7+k3s1 \u0026#x2601;\u0026#xfe0f; Generate the Google Cloud service account Warning Store the downloaded crossplane.json credentials file in a safe place.\nCreate a service account\n1GCP_PROJECT=\u0026lt;your_project\u0026gt; 2gcloud iam service-accounts create crossplane --display-name \u0026#34;Crossplane\u0026#34; --project=${GCP_PROJECT} 3Created service account [crossplane]. Assign the proper permissions to the service account.\nCompute Network Admin Kubernetes Engine Admin Service Account User 1SA_EMAIL=$(gcloud iam service-accounts list --filter=\u0026#34;email ~ ^crossplane\u0026#34; --format=\u0026#39;value(email)\u0026#39;) 2 3gcloud projects add-iam-policy-binding \u0026#34;${GCP_PROJECT}\u0026#34; --member=serviceAccount:\u0026#34;${SA_EMAIL}\u0026#34; \\ 4--role=roles/container.admin --role=roles/compute.networkAdmin --role=roles/iam.serviceAccountUser 5Updated IAM policy for project [\u0026lt;project\u0026gt;]. 6bindings: 7- members: 8 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 9 role: roles/compute.networkAdmin 10- members: 11 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 12... 13version: 1 Download the service account key (json format)\n1gcloud iam service-accounts keys create crossplane.json --iam-account ${SA_EMAIL} 2created key [ea2eb9ce2939127xxxxxxxxxx] of type [json] as [crossplane.json] for [crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com] \u0026#x1f6a7; Deploy and configure Crossplane Now that we have a credentials file for Google Cloud, we can deploy the Crossplane operator and configure the provider-gcp provider.\nInfo Most of the following steps are issued from the official documentation\nWe'll first use Helm in order to install the operator\n1helm repo add crossplane-master https://charts.crossplane.io/master/ 2\u0026#34;crossplane-master\u0026#34; has been added to your repositories 3 4helm repo update 5...Successfully got an update from the \u0026#34;crossplane-master\u0026#34; chart repository 6 7helm install crossplane --namespace crossplane-system --create-namespace \\ 8--version 1.18.1 crossplane-stable/crossplane 9 10NAME: crossplane 11LAST DEPLOYED: Mon Jun 6 22:00:02 2022 12NAMESPACE: crossplane-system 13STATUS: deployed 14REVISION: 1 15TEST SUITE: None 16NOTES: 17Release: crossplane 18... Check that the operator is running properly.\n1kubectl get po -n crossplane-system 2NAME READY STATUS RESTARTS AGE 3crossplane-rbac-manager-54d96cd559-222hc 1/1 Running 0 3m37s 4crossplane-688c575476-lgklq 1/1 Running 0 3m37s Info All the files used for the upcoming steps are stored within this blog repository. So you should clone and change the current directory:\n1git clone https://github.com/Smana/smana.github.io.git 2 3cd smana.github.io/content/resources/crossplane_k3d Now we'll configure Crossplane so that it will be able to create and manage GCP resources. This is done by configuring the provider provider-gcp as follows.\nprovider.yaml\n1apiVersion: pkg.crossplane.io/v1 2kind: Provider 3metadata: 4 name: crossplane-provider-gcp 5spec: 6 package: crossplane/provider-gcp:v0.21.0 1kubectl apply -f provider.yaml 2provider.pkg.crossplane.io/crossplane-provider-gcp created 3 4kubectl get providers 5NAME INSTALLED HEALTHY PACKAGE AGE 6crossplane-provider-gcp True True crossplane/provider-gcp:v0.21.0 10s Create the Kubernetes secret that holds the GCP credentials file created above\n1kubectl create secret generic gcp-creds -n crossplane-system --from-file=creds=./crossplane.json 2secret/gcp-creds created Then we need to create a resource named ProviderConfig and reference the newly created secret.\nprovider-config.yaml\n1apiVersion: gcp.crossplane.io/v1beta1 2kind: ProviderConfig 3metadata: 4 name: default 5spec: 6 projectID: ${GCP_PROJECT} 7 credentials: 8 source: Secret 9 secretRef: 10 namespace: crossplane-system 11 name: gcp-creds 12 key: creds 1kubectl apply -f provider-config.yaml 2providerconfig.gcp.crossplane.io/default created Info If the serviceaccount has the proper permissions we can create resources in GCP. In order to learn about all the available resources and parameters we can have a look to the provider's API reference.\nThe first resource we'll create is the network that will host our Kubernetes cluster.\nnetwork.yaml\n1apiVersion: compute.gcp.crossplane.io/v1beta1 2kind: Network 3metadata: 4 name: dev-network 5 labels: 6 service: vpc 7 creation: crossplane 8spec: 9 forProvider: 10 autoCreateSubnetworks: false 11 description: \u0026#34;Network used for experimentations and POCs\u0026#34; 12 routingConfig: 13 routingMode: REGIONAL 1kubectl get network 2NAME READY SYNCED 3dev-network True True You can even get more details by describing this resource. For instance if something fails you would see the message returned by the Cloud provider in the events.\n1kubectl describe network dev-network | grep -A 20 \u0026#39;^Status:\u0026#39; 2Status: 3 At Provider: 4 Creation Timestamp: 2022-06-28T09:45:30.703-07:00 5 Id: 3005424280727359173 6 Self Link: https://www.googleapis.com/compute/v1/projects/${GCP_PROJECT}/global/networks/dev-network 7 Conditions: 8 Last Transition Time: 2022-06-28T16:45:31Z 9 Reason: Available 10 Status: True 11 Type: Ready 12 Last Transition Time: 2022-06-30T16:36:59Z 13 Reason: ReconcileSuccess 14 Status: True 15 Type: Synced \u0026#x1f680; Create a GKE cluster Everything is ready so that we can create our GKE cluster. Applying the file cluster.yaml will create a cluster and attach a node group to it.\ncluster.yaml\n1--- 2apiVersion: container.gcp.crossplane.io/v1beta2 3kind: Cluster 4metadata: 5 name: dev-cluster 6spec: 7 forProvider: 8 description: \u0026#34;Kubernetes cluster for experimentations and POCs\u0026#34; 9 initialClusterVersion: \u0026#34;1.24\u0026#34; 10 releaseChannel: 11 channel: \u0026#34;RAPID\u0026#34; 12 location: europe-west9-a 13 addonsConfig: 14 gcePersistentDiskCsiDriverConfig: 15 enabled: true 16 networkPolicyConfig: 17 disabled: false 18 networkRef: 19 name: dev-network 20 ipAllocationPolicy: 21 createSubnetwork: true 22 useIpAliases: true 23 defaultMaxPodsConstraint: 24 maxPodsPerNode: 110 25 networkPolicy: 26 enabled: false 27 writeConnectionSecretToRef: 28 namespace: default 29 name: gke-conn 30--- 31apiVersion: container.gcp.crossplane.io/v1beta1 32kind: NodePool 33metadata: 34 name: main-np 35spec: 36 forProvider: 37 initialNodeCount: 1 38 autoscaling: 39 autoprovisioned: false 40 enabled: true 41 maxNodeCount: 4 42 minNodeCount: 1 43 clusterRef: 44 name: dev-cluster 45 config: 46 machineType: n2-standard-2 47 diskSizeGb: 120 48 diskType: pd-standard 49 imageType: cos_containerd 50 preemptible: true 51 labels: 52 environment: dev 53 managed-by: crossplane 54 oauthScopes: 55 - \u0026#34;https://www.googleapis.com/auth/devstorage.read_only\u0026#34; 56 - \u0026#34;https://www.googleapis.com/auth/logging.write\u0026#34; 57 - \u0026#34;https://www.googleapis.com/auth/monitoring\u0026#34; 58 - \u0026#34;https://www.googleapis.com/auth/servicecontrol\u0026#34; 59 - \u0026#34;https://www.googleapis.com/auth/service.management.readonly\u0026#34; 60 - \u0026#34;https://www.googleapis.com/auth/trace.append\u0026#34; 61 metadata: 62 disable-legacy-endpoints: \u0026#34;true\u0026#34; 63 shieldedInstanceConfig: 64 enableIntegrityMonitoring: true 65 enableSecureBoot: true 66 management: 67 autoRepair: true 68 autoUpgrade: true 69 maxPodsConstraint: 70 maxPodsPerNode: 60 71 locations: 72 - \u0026#34;europe-west9-a\u0026#34; 1kubectl apply -f cluster.yaml 2cluster.container.gcp.crossplane.io/dev-cluster created 3nodepool.container.gcp.crossplane.io/main-np created Note that it takes around 10 minutes for the Kubernetes API and the nodes to be available. The STATE will transition from PROVISIONING to RUNNING and when a change is being applied the cluster status is RECONCILING\n1watch \u0026#39;kubectl get cluster,nodepool\u0026#39; 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3cluster.container.gcp.crossplane.io/dev-cluster False True PROVISIONING 34.155.122.6 europe-west9-a 3m15s 4 5NAME READY SYNCED STATE CLUSTER-REF AGE 6nodepool.container.gcp.crossplane.io/main-np False False dev-cluster 3m15s When the column READY switches to True you can download the cluster's credentials.\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RECONCILING 34.42.42.42 europe-west9-a 6m23s 4 5gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project ${GCP_PROJECT} 6Fetching cluster endpoint and auth data. 7kubeconfig entry generated for dev-cluster. For better readability you may want to rename the context id for the newly created cluster\n1kubectl config rename-context gke_${GCP_PROJECT}_europe-west9-a_dev-cluster dev-cluster 2Context \u0026#34;gke_${GCP_PROJECT}_europe-west9-a_dev-cluster\u0026#34; renamed to \u0026#34;dev-cluster\u0026#34;. 3 4kubectl config get-contexts 5CURRENT NAME CLUSTER AUTHINFO NAMESPACE 6* dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster 7 k3d-crossplane k3d-crossplane admin@k3d-crossplane Check that you can call our brand new GKE API\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3gke-dev-cluster-main-np-d0d978f9-5fc0 Ready \u0026lt;none\u0026gt; 10m v1.24.1-gke.1400 That's great \u0026#x1f389; we know have a GKE cluster up and running.\nğŸ’­ final thoughts I've been using Crossplane for a few months now in a production environment.\nEven if I'm conviced about the declarative approach using the Kubernetes API, we decided to move with caution with it. It clearly doesn't have Terraform's community and maturity. We're still declaring our resources using the deletionPolicy: Orphan so that even if something goes wrong on the controller side the resource won't be deleted.\nFurthermore we limited to a specific list of usual AWS resources requested by our developers. Nevertheless our target has always been to empower developers and we had really positive feedback from them. That's the best indicator for us. As the project matures, we'll move more and more resources from Terraform to Crossplane.\nIMHO the key success of Crossplane depends on the providers maintenance and evolution. The Cloud providers interest and involvement is really important.\nIn our next article we'll see how to use a GitOps engine to run all the above steps.\n","link":"https://blog.ogenki.io/fr/post/crossplane_k3d/","section":"post","tags":["kubernetes","infrastructure"],"title":"Mon cluster Kubernetes (GKE) avec `Crossplane`"},{"body":"Afin d'installer des binaires et de pouvoir passer d'une version Ã  une autre, j'aime utiliser asdf.\n\u0026#x1f4e5; Installation L'installation recommandÃ©e consiste Ã  utiliser Git comme suit\n1git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.10.0 Il y a quelques Ã©tapes supplÃ©mentaires qui dÃ©pendent de votre shell. Voici celles que j'utilise pour bash:\n1. $HOME/.asdf/asdf.sh Vous voudrez probablement configurer la completion du shell comme suit\n1. $HOME/.asdf/completions/asdf.bash \u0026#x1f680; Prenons un exemple Listons tous les plugins disponibles pour trouver k3d\n1asdf plugin-list-all | grep k3d 2k3d https://github.com/spencergilbert/asdf-k3d.git Installons k3d\n1asdf plugin-add k3d VÃ©rifier les versions disponibles\n1asdf list-all k3d| tail -n 3 25.4.0-dev.3 35.4.0 45.4.1 Nous installerons la derniÃ¨re version\n1asdf install k3d latest 2* Downloading k3d release 5.4.1... 3k3d 5.4.1 installation was successful! Enfin, nous pouvons passer d'une version Ã  une autre. Nous pouvons dÃ©finir une version \u0026quot;globale\u0026quot; qui serait utilisÃ©e sur tous les rÃ©pertoires.\n1asdf global k3d 5.4.1 ou utilisez une version locale en fonction du rÃ©pertoire actuel\n1cd /tmp 2asdf local k3d 5.4.1 3 4asdf current k3d 5k3d 5.4.1 /tmp/.tool-versions \u0026#x1f9f9; Faire le mÃ©nage DÃ©sinstaller une version donnÃ©e\n1asdf uninstall k3d 5.4.1 Retirer un plugin\n1asdf plugin remove k3d ","link":"https://blog.ogenki.io/fr/post/asdf/asdf/","section":"post","tags":["tooling","local"],"title":"GÃ©rer les versions d'outils avec `asdf`"},{"body":"","link":"https://blog.ogenki.io/fr/tags/local/","section":"tags","tags":null,"title":"Local"},{"body":"","link":"https://blog.ogenki.io/fr/tags/tooling/","section":"tags","tags":null,"title":"Tooling"},{"body":"","link":"https://blog.ogenki.io/fr/categories/","section":"categories","tags":null,"title":"Categories"}]