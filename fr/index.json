[{"body":"","link":"https://blog.ogenki.io/fr/","section":"","tags":null,"title":""},{"body":"","link":"https://blog.ogenki.io/fr/tags/network/","section":"tags","tags":null,"title":"network"},{"body":"","link":"https://blog.ogenki.io/fr/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"Lorsqu'on parle de s√©curisation de l'acc√®s aux ressources Cloud, l'une des r√®gles d'or est d'√©viter les expositions directes √† Internet. La question qui se pose alors pour les Devs/Ops est : comment, par exemple, acc√©der √† une base de donn√©es, un cluster Kubernetes ou un serveur via SSH sans compromettre la s√©curit√©? Les r√©seaux priv√©s virtuels (VPN) offrent une r√©ponse en √©tablissant un lien s√©curis√© entre diff√©rents √©l√©ments d'un r√©seau, ind√©pendamment de leur localisation g√©ographique. De nombreuses solutions existent, allant de mod√®les en SaaS aux solutions que l'on peut h√©berger soi-m√™me, utilisant divers protocoles et √©tant soit open source, soit propri√©taires.\nParmi ces options, je souhaitais vous parler de Tailscale. Cette solution utilise le protocole WireGuard, r√©put√© pour sa simplicit√© et sa performance. Avec Tailscale, il est possible de connecter des appareils ou serveurs de mani√®re s√©curis√©e, comme s'ils √©taient sur un m√™me r√©seau local, bien qu'ils soient r√©partis √† travers le monde.\nüéØ Nos objectifs Comprendre comment fonctionne Tailscale Mise en oeuvre d'une connexion s√©curis√©e avec AWS en quelques minutes Interragir avec l'API d'un cluster EKS via un r√©seau priv√© Acc√©der √† des services h√©berg√©s sur Kubernetes en utilisant le r√©seau priv√© Pour le reste de cet article il faudra √©videmment cr√©er un compte Tailscale. A noter que l'authentification est d√©l√©gu√©e √† des fournisseurs d'identit√© tiers (ex: Okta, Onelogin, Google ...).\nLorsque le compte est cr√©e, on a directement acc√®s √† la console de gestion ci-dessus. Elle permet notamment de lister les appareils connect√©s, de consulter les logs, de modifier la plupart des param√®tres...\nüí° Sous le capot Terminologie Mesh VPN: Un mesh VPN est un type de r√©seau VPN o√π chaque n≈ìud (c'est-√†-dire chaque appareil ou machine) est connect√© √† tous les autres n≈ìuds du r√©seau, formant ainsi un maillage. √Ä distinguer des configurations VPN traditionnelles qui sont con√ßues g√©n√©ralement \u0026quot;en √©toile\u0026quot;, o√π plusieurs clients se connectent √† un serveur central.\nZero trust: Signifie que chaque demande d'acc√®s √† un r√©seau est trait√©e comme si elle venait d'une source non fiable. Une application ou utilisateur doit prouver son identit√© et √™tre autoris√©e avant d'acc√©der √† une ressource. On ne fait pas confiance simplement parce qu'une machine ou un utilisateur provient d'un r√©seau interne ou d'une certaine zone g√©ographique.\nTailnet: D√®s la premi√®re utilisation de Tailscale, un Tailnet est cr√©e pour vous et correspond √† votre propre r√©seau priv√©. Chaque appareil dans un tailnet re√ßoit une IP Tailscale unique, permettant une communication directe entre eux.\nL'architecture de Tailscale est con√ßue de telle sorte que le Control plane et le Data plane sont clairement s√©par√©s:\nD'une part, il y a le serveur de coordination. Son r√¥le est d'√©changer des m√©tadonn√©es et des cl√©s publiques entre tous les participants d'un Tailnet (La cl√© priv√©e √©tant gard√©e en toute s√©curit√© son n≈ìud d'origine).\nD'autre part, les n≈ìuds du Tailnet s'organisent en un r√©seau maill√© (Mesh). Au lieu de passer par le serveur de coordination pour √©changer des donn√©es, ces n≈ìuds communiquent directement les uns avec les autres en mode point √† point. Chaque n≈ìud dispose d'une identit√© unique pour s'authentifier et rejoindre le Tailnet.\n\u0026#x1f4e5; Installation du client La majorit√© des plateformes sont support√©es et les proc√©dures d'installation sont list√©es ici. En ce qui me concerne je suis sur Archlinux:\n1sudo pacman -S tailscale Il est possible de d√©marrer le service automatiquement au d√©marrage de la machine.\n1sudo systemctl enable --now tailscaled Pour enregistrer son ordinateur perso, lancer la commande suivante:\n1sudo tailscale up --accept-routes 2 3To authenticate, visit: 4 5 https://login.tailscale.com/a/f50... ‚ÑπÔ∏è l'option --accept-routes est n√©cessaire sur Linux et permettra d'accepter les routes annonc√©es par les Subnet routers. On verra cela dans la suite de l'article\nV√©rifier que vous avez bien obtenu une IP du r√©seau Tailscale:\n1tailscale ip -4 2100.118.83.67 3 4tailscale status 5100.118.83.67 ogenki smainklh@ linux - ‚ÑπÔ∏è Pour les utilisateurs de Linux, v√©rifier que Tailscale fonctionne bien avec votre configuration DNS: Suivre cette documentation.\nLes sources Toutes les √©tapes r√©alis√©es dans cet article proviennent de ce d√©p√¥t git\nIl va permettre de cr√©er l'ensemble des composants qui ont pour objectif d'obtenir un cluster EKS de Lab et font suite √† un pr√©c√©dent article sur Cilium et Gateway API.\n‚òÅÔ∏è Acc√©der √† AWS en priv√© Afin de pouvoir acc√©der de mani√®re s√©curis√©e √† l'ensemble des ressources disponibles sur AWS, il est possible de d√©ployer un Subnet router.\nUn Subnet router est une instance Tailscale qui permet d'acc√©der √† des sous-r√©seaux qui ne sont pas directement li√©s √† Tailscale. Il fait office de pont entre le r√©seau priv√© virtuel de Tailscale (Tailnet) et d'autres r√©seaux locaux.\nNous pouvons alors router des sous r√©seaux du Clouder √† travers le VPN de Tailscale.\n‚ö†Ô∏è Pour ce faire, sur AWS, il faudra bien entendu configurer les security groups correctement pour autoriser les Subnet routers.\nüöÄ D√©ployer un Subnet router Entrons dans le vif du sujet et deployons un Subnet router sur un r√©seau AWS! Tout est fait en utilisant le code Terraform pr√©sent dans le r√©pertoire terraform/network. Nous allons analyser la configuration sp√©cifique √† Tailscale qui est pr√©sente dans le fichier tailscale.tf avant de proc√©der au d√©ploiement.\nLe provider Terraform Il est possible de configurer certains param√®tres au travers de l'API Tailscale gr√¢ce au provider Terraform. Pour cela il faut au pr√©alable g√©nerer une cl√© d'API üîë sur la console d'admin:\nIl faudra conserver cette cl√© dans un endroit s√©curis√© car elle est utilis√©e pour d√©ployer le Subnet router\n1provider \u0026#34;tailscale\u0026#34; { 2 api_key = var.tailscale.api_key 3 tailnet = var.tailscale.tailnet 4} les ACL's\nLes ACL's permettent de d√©finir qui est autoris√© √† communiquer avec qui (utilisateur ou appareil). √Ä la cr√©ation d'un compte, celle-cis sont tr√®s permissives et il n'y a aucune restriction (tout le monde peut parler avec tout le monde).\n1resource \u0026#34;tailscale_acl\u0026#34; \u0026#34;this\u0026#34; { 2 acl = jsonencode({ 3 acls = [ 4 { 5 action = \u0026#34;accept\u0026#34; 6 src = [\u0026#34;*\u0026#34;] 7 dst = [\u0026#34;*:*\u0026#34;] 8 } 9 ] 10... 11} Note Pour mon environnement de Lab, j'ai conserv√© cette configuration par d√©fault car je suis la seule personne √† y acc√©der. De plus les seuls appareils connect√©s √† mon Tailnet sont mon laptop et le Subnet router. En revanche dans un cadre d'entreprise, il faudra bien y r√©fl√©chir. Il est alors possible de d√©finir une politique bas√©e sur des groupes d'utilisitateurs ou sur les tags des noeuds.\nConsulter cette doc pour plus d'info.\nLes noms de domaines (DNS)\nIl y a diff√©rentes fa√ßons possibles de g√©rer les noms de domaines avec Tailscale:\nMagic DNS: Lorsqu'un appareil rejoint le Tailnet, il s'enregistre avec un nom et celui-ci peut-√™tre utilis√© directement pour communiquer avec l'appareil.\n1tailscale status 2100.118.83.67 ogenki smainklh@ linux - 3100.115.31.152 ip-10-0-43-98 smainklh@ linux active; relay \u0026#34;par\u0026#34;, tx 3044 rx 2588 4 5ping ip-10-0-43-98 6PING ip-10-0-43-98.tail9c382.ts.net (100.115.31.152) 56(84) bytes of data. 764 bytes from ip-10-0-43-98.tail9c382.ts.net (100.115.31.152): icmp_seq=1 ttl=64 time=11.4 ms AWS: Pour utiliser les noms de domaines internes √† AWS il est possible d'utiliser la deuxi√®me IP du VPC qui correspond toujours au serveur DNS. Cela permet d'utiliser les √©ventuelles zones priv√©es sur route53 ou de se connecter aux ressources en utilisant les noms de domaines.\nLa configuration la plus simple est donc de d√©clarer la liste des serveurs DNS √† utiliser et d'y ajouter celui de AWS. Ici un exemple avec le DNS publique de Cloudflare.\n1resource \u0026#34;tailscale_dns_nameservers\u0026#34; \u0026#34;this\u0026#34; { 2 nameservers = [ 3 \u0026#34;1.1.1.1\u0026#34;, 4 cidrhost(module.vpc.vpc_cidr_block, 2) 5 ] 6} La cl√© d'authentification (\u0026quot;auth key\u0026quot;)\nPour qu'un appareil puisse rejoindre le Tailnet au d√©marrage il faut que Tailscale soit d√©marr√© en utilisant une cl√© d'authentification. Celle-ci est g√©n√©r√©e comme suit\n1resource \u0026#34;tailscale_tailnet_key\u0026#34; \u0026#34;this\u0026#34; { 2 reusable = true 3 ephemeral = false 4 preauthorized = true 5} reusable: S'agissant d'un autoscaling group, il faut que cette m√™me cl√© puisse √™tre utilis√©e plusieurs fois. ephemeral: Pour cette d√©mo nous cr√©ons une cl√© qui n'expire pas. En production il serait pr√©f√©rable d'activer l'expiration. preauthorized: Il faut que cette cl√© soit d√©j√† valide et autoris√©e pour que l'instance rejoigne automatiquement le Tailscale. La cl√© ainsi g√©n√©r√©e est utilis√©e pour lancer tailscale avec le param√®tre --auth-key\n1sudo tailscale up --authkey=\u0026lt;REDACTED\u0026gt; Annoncer les routes pour les r√©seaux AWS\nEnfin il faut annoncer le r√©seau que l'on souhaite faire passer par le Subnet router. Dans notre exemple, nous d√©cidons de router tout le r√©seau du VPC qui a pour CIDR 10.0.0.0/16.\nAfin que cela soit possible de fa√ßon automatique, il y a une r√®gle autoApprovers √† ajouter. Cela permet d'indiquer que les routes annonc√©es par l'utilisateur smainklh@gmail.com sont autoris√©es sans que cela requiert une √©tape d'approbation.\n1 autoApprovers = { 2 routes = { 3 \u0026#34;10.0.0.0/16\u0026#34; = [\u0026#34;smainklh@gmail.com\u0026#34;] 4 } 5 } La commande lanc√©e au d√©marrage de l'instance Subnet router est la suivante:\n1sudo tailscale up --authkey=\u0026lt;REDACTED\u0026gt; --advertise-routes=\u0026#34;10.0.0.0/16\u0026#34; Le module Terraform J'ai cr√©√© un module tr√®s simple qui permet de d√©ployer un autoscaling group sur AWS et de configurer Tailscale. Au d√©marrage de l'instance, elle s'authentifiera en utilisant une auth_key et annoncera les r√©seaux indiqu√©s. Dans l'exemple ci-dessous l'instance annonce le CIDR du VPC sur AWS.\n1module \u0026#34;tailscale_subnet_router\u0026#34; { 2 source = \u0026#34;Smana/tailscale-subnet-router/aws\u0026#34; 3 version = \u0026#34;1.0.4\u0026#34; 4 5 region = var.region 6 env = var.env 7 8 name = var.tailscale.subnet_router_name 9 auth_key = tailscale_tailnet_key.this.key 10 11 vpc_id = module.vpc.vpc_id 12 subnet_ids = module.vpc.private_subnets 13 advertise_routes = [module.vpc.vpc_cidr_block] 14... 15} Maintenant que nous avons analys√© les diff√©rents param√®tres, il est temps de d√©marrer notre Subnet router üöÄ !! Il faut au pr√©alable cr√©er un fichier variable.tfvars dans le r√©pertoire terraform/network.\n1env = \u0026#34;dev\u0026#34; 2region = \u0026#34;eu-west-3\u0026#34; 3private_domain_name = \u0026#34;priv.cloud.ogenki.io\u0026#34; 4 5tailscale = { 6 subnet_router_name = \u0026#34;ogenki\u0026#34; 7 tailnet = \u0026#34;smainklh@gmail.com\u0026#34; 8 api_key = \u0026#34;tskey-api-...\u0026#34; 9} 10 11tags = { 12 project = \u0026#34;demo-cloud-native-ref\u0026#34; 13 owner = \u0026#34;Smana\u0026#34; 14} Puis lancer la commande suivante:\n1tofu plan --var-file variables.tfvars Apr√®s v√©rification du plan, appliquer les changements\n1tofu apply --var-file variables.tfvars Quand l'instance est d√©marr√©e, elle apparaitra dans la liste des appareils du Tailnet.\n1tailscale status 2100.118.83.67 ogenki smainklh@ linux - 3100.68.109.138 ip-10-0-26-99 smainklh@ linux active; relay \u0026#34;par\u0026#34;, tx 33868 rx 32292 Nous pouvons aussi v√©rifier que la route est bien annonc√©e comme suit:\n1tailscale status --json|jq \u0026#39;.Peer[] | select(.HostName == \u0026#34;ip-10-0-26-99\u0026#34;) .PrimaryRoutes\u0026#39; 2[ 3 \u0026#34;10.0.0.0/16\u0026#34; 4] ‚ö†Ô∏è Pour des raisons de s√©curit√©, pensez √† supprimer le fichier variables.tfvars car il contient la cl√© d'API.\nüëè Et voil√† ! Nous sommes maintenant en mesure d'acc√©der au r√©seau sur AWS, √† condition d'avoir √©galement configur√© les r√®gles de filtrage, comme les ACL et les security groups. Nous pouvons par exemple acc√©der √† une base de donn√©es depuis le poste de travail\n1psql -h demo-tailscale.cymnaynfchjt.eu-west-3.rds.amazonaws.com -U postgres 2Password for user postgres: 3psql (15.4, server 15.3) 4SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, compression: off) 5Type \u0026#34;help\u0026#34; for help. 6 7postgres=\u0026gt; üíª Une autre fa√ßon de faire du SSH Traditionnellement, nous devons parfois nous connecter √† des serveurs en utilisant le protocole SSH. Pour ce faire, il faut g√©n√©rer une cl√© priv√©e et distribuer la cl√© publique correspondante sur les serveurs distants.\nContrairement √† l'utilisation des cl√©s SSH classiques, √©tant donn√© que Tailscale utilise Wireguard pour l'authentification et le chiffrement des connexions il n'est pas n√©cessaire de r√©-authentifier le client. De plus, Tailscale g√®re √©galement la distribution des cl√©s SSH d'h√¥tes. Les r√®gles ACL permettent de r√©voquer l'acc√®s des utilisateurs sans avoir √† supprimer les cl√©s SSH. De plus, il est possible d'activer un mode de v√©rification qui renforce la s√©curit√© en exigeant une r√©-authentification p√©riodique. On peut donc affirmer que l'utilisation de Tailscale SSH simplifie l'authentification, la gestion des connexions SSH et am√©liore le niveau de s√©curit√©.\nLes autorisations pour utiliser SSH sont aussi g√©r√©es au niveau des ACL's\n1... 2 ssh = [ 3 { 4 action = \u0026#34;check\u0026#34; 5 src = [\u0026#34;autogroup:member\u0026#34;] 6 dst = [\u0026#34;autogroup:self\u0026#34;] 7 users = [\u0026#34;autogroup:nonroot\u0026#34;] 8 } 9 ] 10... La r√®gle ci-dessus autorise tous les utilisateurs √† acc√©der √† leurs propres appareils en utilisant SSH. Lorsqu'ils essaient de se connecter, ils doivent utiliser un compte utilisateur autre que root. Pour chaque tentative de connexion, une authentification suppl√©mentaire est n√©cessaire (action=check). Cette authentification se fait en visitant un lien web sp√©cifique\n1ssh ubuntu@ip-10-0-26-99 2... 3# Tailscale SSH requires an additional check. 4# To authenticate, visit: https://login.tailscale.com/a/f1f09a548cc6 5... 6ubuntu@ip-10-0-26-99:~$ Pour que cela soit possible il faut aussi d√©marrer Tailscale avec l'option --ssh\nLes logs d'acc√®s √† la machine peuvent √™tre consult√©s en utilisant journalctl\n1ubuntu@ip-10-0-26-99:~$ journalctl -aeu tailscaled|grep ssh 2Oct 15 15:51:34 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155130-00ede660b8: handling conn: 100.118.83.67:55098-\u0026gt;ubuntu@100.68.109.138:22 3Oct 15 15:51:56 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155156-b6d1dc28c0: handling conn: 100.118.83.67:44560-\u0026gt;ubuntu@100.68.109.138:22 4Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-conn-20231015T155156-b6d1dc28c0: starting session: sess-20231015T155252-5b2acc170e 5Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): handling new SSH connection from smainklh@gmail.com (100.118.83.67) to ssh-user \u0026#34;ubuntu\u0026#34; 6Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): access granted to smainklh@gmail.com as ssh-user \u0026#34;ubuntu\u0026#34; 7Oct 15 15:52:52 ip-10-0-26-99 tailscaled[1768]: ssh-session(sess-20231015T155252-5b2acc170e): starting pty command: [/usr/sbin/tailscaled be-child ssh --uid=1000 --gid=1000 --groups=1000,4,20,24,25,27,29,30,44,46,115,116 --local-user=ubuntu --remote-user=smainklh@gmail.com --remote-ip=100.118.83.67 --has-tty=true --tty-name=pts/0 --shell --login-cmd=/usr/bin/login --cmd=/bin/bash -- -l] ‚ÑπÔ∏è Avec Tailscale SSH il est possible de se connecter en SSH peu importe o√π est situ√© l'appareil. En revanche dans un contexte 100% AWS, on pr√©ferera probablement utiliser AWS SSM.\nLogs üíæ En s√©curit√© il est primordial de pouvoir conserver les logs pour un usage ult√©rieur. Il existe diff√©rents types de logs:\nLogs d'audit: Ils sont essentiels pour savoir qui a fait quoi. Ils sont accessibles sur la console d'admin et peuvent aussi √™tre envoy√©s vers un SIEM.\nLogs sur les appareils: Ceux-cis peuvent √™tre consult√©s en utilisant les commandes appropri√©es √† l'appareil. (journalctl -u tailscaled sur Linux)\nLogs r√©seau: Utiles pour visualiser quels appareils sont connect√©s les uns aux autres.\n‚ò∏ Qu'en est-il de Kubernetes? Sur Kubernetes il existe plusieurs options pour acc√©der √† un Service:\nProxy: Il s'agit d'un pod suppl√©mentaire qui transfert les appels √† un Service existant. Sidecar: Permet de connecter le pod au Tailnet. Donc la connectivit√© se fait de bout en bout et il est m√™me possible de communiquer dans les 2 sens. (du pod vers les noeuds du Tailnet). Operator: Permet d'exposer les services et l'API Kubernetes (ingress) ainsi que de permettre aux pods d'acc√©der aux noeuds du Tailnet (egress). La configuration se fait en configurant les ressources existantes: Services et Ingresses Dans notre cas, nous disposons d√©j√† d'un Subnet router qui route tout le r√©seau du VPC. Il suffit donc que notre service soit expos√© sur une IP priv√©e.\nL'API Kubernetes Pour acc√©der √† l'API Kubernetes il est n√©cessaire d'autoriser le Subnet router. Cela se fait en d√©finissant la r√®gle suivante pour le security group source.\n1module \u0026#34;eks\u0026#34; { 2... 3 cluster_security_group_additional_rules = { 4 ingress_source_security_group_id = { 5 description = \u0026#34;Ingress from the Tailscale security group to the API server\u0026#34; 6 protocol = \u0026#34;tcp\u0026#34; 7 from_port = 443 8 to_port = 443 9 type = \u0026#34;ingress\u0026#34; 10 source_security_group_id = data.aws_security_group.tailscale.id 11 } 12 } 13... 14} Nous allons v√©rifier que l'API est bien accessible sur une IP priv√©e.\n1CLUSTER_URL=$(TERM=dumb kubectl cluster-info | grep \u0026#34;Kubernetes control plane\u0026#34; | awk \u0026#39;{print $NF}\u0026#39;) 2 3curl -s -o /dev/null -w \u0026#39;%{remote_ip}\\n\u0026#39; ${CLUSTER_URL} 410.228.244.167 5 6kubectl get ns 7NAME STATUS AGE 8cilium-secrets Active 5m46s 9crossplane-system Active 4m1s 10default Active 23m 11flux-system Active 5m29s 12infrastructure Active 4m1s 13... Acc√©der aux services en priv√© Un Service Kubernetes expos√© est une resource AWS comme une autre üòâ. Il faut juste s'assurer que ce service utilise bien une IP priv√©e. Dans mon exemple j'utilise Gateway API pour configurer la r√©partition de charge du Clouder et je vous invite √† lire mon pr√©c√©dent article sur le sujet.\nIl suffirait donc de cr√©er un NLB interne en s'assurant que le Service ait bien l'annotation service.beta.kubernetes.io/aws-load-balancer-scheme ayant pour valeur internal. Dans le cas de Gateway API, cela se fait via la clusterPolicy Kyverno.\n1 metadata: 2 annotations: 3 external-dns.alpha.kubernetes.io/hostname: gitops-${cluster_name}.priv.${domain_name},grafana-${cluster_name}.priv.${domain_name} 4 service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internal\u0026#34; 5 service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp 6 spec: 7 loadBalancerClass: service.k8s.aws/nlb Il y a cependant un pr√©requis suppl√©mentaire car nous ne pouvons pas utiliser Let's Encrypt pour les certificats internes. J'ai donc g√©n√©r√© une PKI interne qui g√©n√®re des certificates auto-sign√©s avec Cert-manager.\nIci je ne d√©taillerai pas le d√©ploiement du cluster EKS, ni la configuration de Flux. Lorsque le cluster est cr√©√© et que toutes les ressources Kubernetes ont √©t√© r√©concili√©, nous avons un service qui est expos√© via un LoadBalancer interne AWS.\n1NLB_DOMAIN=$(kubectl get svc -n infrastructure cilium-gateway-platform -o jsonpath={.status.loadBalancer.ingress[0].hostname}) 2dig +short ${NLB_DOMAIN} 310.0.33.5 410.0.26.228 510.0.9.183 Une entr√©e DNS est √©galement cr√©√©e automatiquement pour les services expos√©s et nous pouvons donc acc√©der en priv√© gr√¢ce √† Tailscale.\n1dig +short gitops-mycluster-0.priv.cloud.ogenki.io 210.0.9.183 310.0.26.228 410.0.33.5 üí≠ Derni√®res remarques Il y a quelques temps, dans le cadre professionnel, j'ai mis en place Cloudflare Zero Trust. Je d√©couvre ici que Tailscale pr√©sente de nombreuses similitudes avec cette solution. La d√©cision entre les deux est loin d'√™tre triviale et d√©pend grandement du contexte. Pour ma part, j'ai √©t√© particuli√®rement convaincu par la simplicit√© de mise en ≈ìuvre de Tailscale, r√©pondant parfaitement √† mon besoin d'acc√©der au r√©seau du Clouder. Bien entendu il existe d'autres solutions comme Teleport, qui offre une approche diff√©rente pour acc√©der √† des ressources internes.\nCela dit, focalisons-nous sur Tailscale.\nUne partie du code de Tailscale est open source, notamment le client qui est sous license BSD 3-Clause. La partie propri√©taire concerne √©ssentiellement la plateforme de coordination. √Ä noter qu'il existe une alternative open source nomm√©e Headscale. Celle-ci est une initiative distincte qui n'a aucun lien avec la soci√©t√© Tailscale.\nPour un usage personnel, Tailscale est vraiment g√©n√©reux, offrant un acc√®s gratuit pour jusqu'√† 100 appareils et 3 utilisateurs. Ceci-dit Tailscale est une option s√©rieuse √† consid√©rer en entreprise et il est important, selon moi, d'encourager ce type d'entreprises qui ont une politique open source claire et un produit de qualit√©.\n","link":"https://blog.ogenki.io/fr/post/tailscale/","section":"post","tags":["security","network"],"title":"S√©curiser le Cloud avec `Tailscale` : Mise en ≈ìuvre d'un VPN simplifi√©e"},{"body":"","link":"https://blog.ogenki.io/fr/tags/security/","section":"tags","tags":null,"title":"security"},{"body":"","link":"https://blog.ogenki.io/fr/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"Lorsque l'on d√©ploie une application sur Kubernetes, l'√©tape suivante consiste g√©n√©ralement √† l'exposer aux utilisateurs. On utilise habituellement des \u0026quot;Ingress controllers\u0026quot;, comme Nginx, Haproxy, Traefik ou encore ceux des diff√©rents Clouders afin de diriger le trafic entrant vers l'application, g√©rer l'√©quilibrage de charge, la terminaison TLS et j'en passe.\nIl faut alors choisir parmi la pl√©thore d'options disponibles ü§Ø la solution qui sera en charge de tous ces aspects et Cilium est, depuis relativement r√©cemment, l'une d'entre elles.\nCilium est une solution Open-Source de connectivit√© r√©seau et de s√©curit√© bas√©e sur eBPF dont l'adoption est grandissante. Il s'agit probablement du plugin r√©seau qui fournit le plus de fonctionnalit√©s. Nous n'allons pas toutes les parcourir mais l'une d'entre elles consiste √† g√©rer le trafic entrant en utilisant le standard Gateway API (GAPI).\nüéØ Notre objectif Comprendre ce qu'est exactement Gateway API et en quoi il s'agit d'une √©volution par rapport √† l'API Ingress. D√©monstrations de cas concrets √† la sauce GitOps. Les limitations actuelles et les √©volutions √† venir. Tip Toutes les √©tapes r√©alis√©es dans cet article proviennent de ce d√©p√¥t git\nJe vous invite √† le parcourir car, il va bien au del√† du contexte de cet article:\nInstallation d'un cluster EKS avec Cilium configur√© en mode sans kube-proxy et un Daemonset d√©di√© √† Envoy Proposition de structure de configuration Flux avec une gestion des d√©pendances et une factorisation que je trouve efficaces. Crossplane et composition IRSA qui simplifie la gestion des permissions IAM pour les composants plateforme Gestion des noms de domaine ainsi que des certificats automatis√©s avec External-DNS et Let's Encrypt L'id√©e √©tant d'avoir l'ensemble configur√© au bout de quelques minutes, en une seule ligne de commande ü§©.\n‚ò∏ Introduction √† Gateway API Comme √©voqu√© pr√©c√©demment, il y a de nombreuses options qui font office d' Ingress controller et chacune a ses propres sp√©cificit√©s et des fonctionnalit√©s particuli√®res, rendant leur utilisation parfois complexe. Par ailleurs, l'API Ingress, utilis√©e historiquement dans Kubernetes poss√®de tr√®s peu d'options. Certaines solutions ont d'ailleurs cr√©√© des CRDs (Ressources personalis√©es) quand d'autres font usage des annotations pour lever ces limites.\nC'est dans ce contexte que Gateway API fait son apparition. Il s'agit d'un standard qui permet de d√©finir des fonctionnalit√©s r√©seau avanc√©es sans n√©cessiter d'extensions sp√©cifiques au contr√¥leur sous-jacent. De plus √©tant donn√© que tous les contr√¥leurs utilisent la m√™me API , il est possible de passer d'une solution √† une autre sans changer de configuration (les ressources qui g√®rent le trafic entrant restent les m√™mes).\nParmi les concepts que nous allons explorer la GAPI introduit un sch√©ma de r√©partition des responsabilit√©s. Elle d√©finit des roles explicites avec des permissions bien distinctes. (Plus d'informations sur le mod√®le de s√©curit√© GAPI ici).\nEnfin il est important de noter que ce projet est dirig√© par le groupe de travail sig-network-kubernetes et un canal slack vous permettra de les solliciter si n√©cessaire.\nVoyons comment cela s'utilise concr√®tement üöÄ!\n\u0026#x2611;\u0026#xfe0f; Pr√©requis Pour le reste de cet article nous consid√©rons qu'un cluster EKS a √©t√© d√©ploy√©. Si vous n'utilisez pas la m√©thode propos√©e dans le repo de d√©mo servant de socle √† cet article, il y a certains points √† valider pour que GAPI puisse √™tre utilis√©.\n‚ÑπÔ∏è La m√©thode d'installation decrite ici se base sur Helm, l'ensemble des values peuvent √™tre consult√©es ici.\nInstaller les CRDs (resources personnalis√©s) disponibles dans le repository Gateway API Note Si Cilium est configur√© avec le support GAPI (voir ci-dessous) et que les CRDs sont absentes, il ne d√©marrera pas.\nDans le repo de demo les CRDs GAPI sont install√©es une premi√®re fois lors de la cr√©ation du cluster afin que Cilium puisse d√©marrer puis elles sont ensuite g√©r√©es par Flux.\nRemplacer kube-proxy par les fonctionnalit√©s de transfert r√©seau apport√©es par Cilium et eBPF.\n1kubeProxyReplacement: true Activer le support de Gateway API 1gatewayAPI: 2 enabled: true V√©rifier L'installation Il faut pour cela installer le client en ligne de commande cilium. J'utilise personnellement asdf pour cela:\n1asdf plugin-add cilium-cli 2asdf install cilium-cli 0.15.7 3asdf global cilium 0.15.7 La commande suivante permet de s'assurer que tous les composants sont d√©marr√©s et op√©rationnels\n1cilium status --wait 2 /¬Ø¬Ø\\ 3/¬Ø¬Ø\\__/¬Ø¬Ø\\ Cilium: OK 4\\__/¬Ø¬Ø\\__/ Operator: OK 5/¬Ø¬Ø\\__/¬Ø¬Ø\\ Envoy DaemonSet: OK 6\\__/¬Ø¬Ø\\__/ Hubble Relay: disabled 7 \\__/ ClusterMesh: disabled 8 9Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 10DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 11DaemonSet cilium-envoy Desired: 2, Ready: 2/2, Available: 2/2 12Containers: cilium Running: 2 13 cilium-operator Running: 2 14 cilium-envoy Running: 2 15Cluster Pods: 33/33 managed by Cilium 16Helm chart version: 1.14.2 17Image versions cilium quay.io/cilium/cilium:v1.14.2@sha256:6263f3a3d5d63b267b538298dbeb5ae87da3efacf09a2c620446c873ba807d35: 2 18 cilium-operator quay.io/cilium/operator-aws:v1.14.2@sha256:8d514a9eaa06b7a704d1ccead8c7e663334975e6584a815efe2b8c15244493f1: 2 19 cilium-envoy quay.io/cilium/cilium-envoy:v1.25.9-e198a2824d309024cb91fb6a984445e73033291d@sha256:52541e1726041b050c5d475b3c527ca4b8da487a0bbb0309f72247e8127af0ec: 2 Enfin le support de GAPI peut √™tre v√©rifi√© comme suit\n1cilium config view | grep -w \u0026#34;enable-gateway-api\u0026#34; 2enable-gateway-api true 3enable-gateway-api-secrets-sync true Il est aussi possible de lancer des tests de connectivit√© pour s'assurer qu'il n'y a pas de probl√®mes avec la configuration r√©seau du cluster:\n1cilium connectivity test ‚ö†Ô∏è Cette commande (connectivity test) provoque actuellement des erreurs lors de l'activation d'Envoy en tant que DaemonSet. (Issue Github).\nInfo as DaemonSet\nPar d√©faut l'agent cilium int√©gre Envoy et lui d√©legue les op√©rations r√©seau de niveau 7. Depuis la version v1.14, il est possible de d√©ployer Envoy s√©par√©ment ce qui apporte certains avantages:\nSi l'on modifie/red√©marre un composant (que ce soit Cilium ou Envoy), cela n'affecte pas l'autre. Mieux attribuer les ressources √† chacun des composants afin d'optimiser les perfs. Limite la surface d'attaque en cas de compromission d'un des pods. Les logs Envoy et de l'agent Cilium ne sont pas m√©lang√©s Il est possible d'utiliser la commande suivante pour v√©rifier que cette fonctionnalit√© est bien active:\n1cilium status 2 /¬Ø¬Ø\\ 3 /¬Ø¬Ø\\__/¬Ø¬Ø\\ Cilium: OK 4 \\__/¬Ø¬Ø\\__/ Operator: OK 5 /¬Ø¬Ø\\__/¬Ø¬Ø\\ Envoy DaemonSet: OK 6 \\__/¬Ø¬Ø\\__/ Hubble Relay: disabled 7 \\__/ ClusterMesh: disabled Plus d'information.\nüö™ La porte d'entr√©e: GatewayClass et Gateway Une fois les conditions n√©cessaires remplies, nous avons acc√®s √† plusieurs √©l√©ments. Nous pouvons notamment utiliser les ressources de la Gateway API gr√¢ce aux CRDs. D'ailleurs, d√®s l'installation de Cilium, une GatewayClass est directement disponible.\n1kubectl get gatewayclasses.gateway.networking.k8s.io 2NAME CONTROLLER ACCEPTED AGE 3cilium io.cilium/gateway-controller True 7m59s Sur un cluster il est possible de configurer plusieurs GatewayClass et donc d'avoir la possibilit√© de faire usage de diff√©rentes impl√©mentations. Nous pouvons par exemple utiliser Linkerd en r√©f√©rencant la GatewayClass dans la configuration de la Gateway.\nLa Gateway est la ressource qui permet d√©clencher la cr√©ation de composants de r√©partition de charge chez le Clouder.\nVoici un exemple simple: apps/base/echo/gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: echo-gateway 5 namespace: echo 6spec: 7 gatewayClassName: cilium 8 listeners: 9 - protocol: HTTP 10 port: 80 11 name: echo-1-echo-server 12 allowedRoutes: 13 namespaces: 14 from: Same Sur AWS (EKS), quand on configure une Gateway, Cilium cr√©e un Service de type LoadBalancer. Ce service est alors interpr√©t√© par un autre contr√¥leur, l'(AWS Load Balancer Controler), qui produit un NLB.\n1kubectl get svc -n echo cilium-gateway-echo 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3cilium-gateway-echo LoadBalancer 172.20.19.82 k8s-echo-ciliumga-64708ec85c-fcb7661f1ae4e4a4.elb.eu-west-3.amazonaws.com 80:30395/TCP 2m58s Il est int√©ressant de noter que l'adresse en question est aussi associ√©e √† la resource Gateway.\n1kubectl get gateway -n echo echo 2NAME CLASS ADDRESS PROGRAMMED AGE 3echo cilium k8s-echo-ciliumga-64708ec85c-fcb7661f1ae4e4a4.elb.eu-west-3.amazonaws.com True 16m \u0026#x21aa;\u0026#xfe0f; Les r√®gles de routage: HTTPRoute Un routage simple Pour r√©sumer le sch√©ma ci-dessus en quelques mots: Une ressource HTTPRoute permet de configurer le routage vers le service en r√©f√©rencant la gateway et en d√©finissant le les param√®tres de routage souhait√©s.\nNote workaround\n√Ä ce jour, il n'est pas possible de configurer les annotations des services g√©n√©r√©s par les Gateways (Issue Github). Une solution de contournement a √©t√© propos√© afin de modifier le service g√©n√©r√© par la Gateway d√®s lors qu'il est cr√©√©.\nKyverno est un outil qui permet de garantir la conformit√© des configurations par rapport aux bonnes pratiques et aux exigences de s√©curit√©. Nous utilisons ici uniquement sa capacit√© √† d√©crire une r√®gle de mutation facilement.\nsecurity/mycluster-0/echo-gw-clusterpolicy.yaml\n1spec: 2 rules: 3 - name: mutate-svc-annotations 4 match: 5 any: 6 - resources: 7 kinds: 8 - Service 9 namespaces: 10 - echo 11 name: cilium-gateway-echo 12 mutate: 13 patchStrategicMerge: 14 metadata: 15 annotations: 16 external-dns.alpha.kubernetes.io/hostname: echo.${domain_name} 17 service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; 18 service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp 19 spec: 20 loadBalancerClass: service.k8s.aws/nlb Le service cilium-gateway-echo se verra donc ajouter les annotations du contr√¥leur AWS ainsi qu'une annotation permettant de configurer une entr√©e DNS automatiquement.\napps/base/echo/httproute.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7 parentRefs: 8 - name: echo 9 namespace: echo 10 rules: 11 - matches: 12 - path: 13 type: PathPrefix 14 value: / 15 backendRefs: 16 - name: echo-1-echo-server 17 port: 80 L'exemple utilis√© ci-dessus est simpliste: toutes les requ√™tes sont transf√©r√©es au service echo-1-echo-server.\nparentRefs permet d'indiquer la Gateway √† utiliser puis les r√®gles de routage sont d√©finies dans rules.\nLes r√®gles de routages pourraient aussi √™tre bas√©es sur le path.\n1... 2spec: 3 hostnames: 4 - foo.bar.com 5 rules: 6 - matches: 7 - path: 8 type: PathPrefix 9 value: /login Ou une ent√™te HTTP\n1... 2spec: 3 rules: 4 - matches: 5 headers: 6 - name: \u0026#34;version\u0026#34; 7 value: \u0026#34;2\u0026#34; 8... V√©rifions que le service est joignable:\n1curl -s http://echo.cloud.ogenki.io | jq -rc \u0026#39;.environment.HOSTNAME\u0026#39; 2echo-1-echo-server-fd88497d-w6sgn Comme vous pouvez le voir le service est expos√© en HTTP sans certificat. Essayons de corriger cela üòâ\nExposer un service en utilisant un certificat TLS Il existe plusieurs m√©thodes pour configurer du TLS avec GAPI. Ici nous allons utiliser le cas le plus commun: protocole HTTPS et terminaison TLS sur la Gateway.\nSupposons que nous souhaitons configurer le nom de domaine echo.cloud.ogenki.io utilis√© pr√©c√©demment. La configuration se fait principalement au niveau de la Gateway\napps/base/echo/tls-gateway.yaml\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: Gateway 3metadata: 4 name: echo 5 namespace: echo 6 annotations: 7 cert-manager.io/cluster-issuer: letsencrypt-prod 8spec: 9 gatewayClassName: cilium 10 listeners: 11 - name: http 12 hostname: \u0026#34;echo.${domain_name}\u0026#34; 13 port: 443 14 protocol: HTTPS 15 allowedRoutes: 16 namespaces: 17 from: Same 18 tls: 19 mode: Terminate 20 certificateRefs: 21 - name: echo-tls Le point essentiel ici est la r√©f√©rence √† un secret contenant le certificat echo-tls. Ce certificat peut √™tre cr√©√© manuellement mais j'ai d√©cid√© pour cet article d'automatiser cela avec Let's Encrypt et cert-manager.\nInfo cert-manager\nAvec cert-manager il est tr√®s simple d'automatiser la cr√©ation et la mise √† jour des certificats expos√©s par la Gateway. Pour cela, il faut permettre au contr√¥lleur d'acc√©der √† route53 afin de r√©soudre un challenge DNS01 (M√©canisme qui permet de s'assurer que les clients peuvent seulement demander des certificats pour des domaines qu'ils poss√®dent).\nUne ressource ClusterIssuer d√©crit la configuration n√©cessaire pour g√©n√©rer des certificats gr√¢ce √† cert-manager.\nEnsuite il suffit d'ajouter une annotation cert-manager.io/cluster-issuer et indiquer le secret Kubernetes o√π sera stock√© le certificat.\n‚ÑπÔ∏è Dans le repo de demo les permissions sont attribu√©es en utilisant Crossplane qui se charge de configurer cela au niveau du Cloud AWS.\nPlus d'informations\nPour que le routage se fasse correctement il faut aussi bien entendu r√©f√©rencer la bonne Gateway mais aussi indiquer le nom de domaine dans la ressource HTTPRoute.\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7 parentRefs: 8 - name: echo 9 namespace: echo 10 hostnames: 11 - \u0026#34;echo.${domain_name}\u0026#34; 12... Il faut patienter quelques minutes le temps que le certificat soit cr√©√©.\n1kubectl get cert -n echo 2NAME READY SECRET AGE 3echo-tls True echo-tls 43m Nous pouvons enfin v√©rifier que le certificat est bien issue de Let's Encrypt comme suit:\n1curl https://echo.cloud.ogenki.io -v 2\u0026gt;\u0026amp;1 | grep -A 6 \u0026#39;Server certificate\u0026#39; 2* Server certificate: 3* subject: CN=echo.cloud.ogenki.io 4* start date: Sep 15 14:43:00 2023 GMT 5* expire date: Dec 14 14:42:59 2023 GMT 6* subjectAltName: host \u0026#34;echo.cloud.ogenki.io\u0026#34; matched cert\u0026#39;s \u0026#34;echo.cloud.ogenki.io\u0026#34; 7* issuer: C=US; O=Let\u0026#39;s Encrypt; CN=R3 8* SSL certificate verify ok. Info GAPI permet aussi de configurer le TLS de bout en bout, jusqu'au conteneur. Cela se fait en configurant la Gateway en Passthrough et en utilisant une ressource TLSRoute. Il faut aussi que le certificat soit port√© par le pod qui fait terminaison TLS.\nUne Gateway partag√©e par plusieurs namespaces Avec GAPI il est possible de router le trafic √† travers les Namespaces. Cela est rendu possible gr√¢ce √† des ressources distinctes pour chaque fonction: Une Gateway qui permet de configurer l'infrastructure et notamment de provisionner une adresse IP, et les *Routes. Ces routes peuvent r√©f√©rencer une Gateway situ√©e dans un autre namespace. Il est ainsi possible pour diff√©rent(e)s √©quipes/projets de partager les m√™mes √©l√©ments d'infrastructure.\nIl est cependant requis de sp√©cifier quelle route est autoris√©e √† r√©f√©rencer la Gateway. Ici nous supposons que nous avons une Gateway d√©di√©e aux outils internes qui s'appelle platform. En utilisant le param√®tre allowedRoutes, nous sp√©cifions explicitement quelles sont les namespaces autoris√©s √† r√©f√©rencer cette Gateway.\ninfrastructure/base/gapi/platform-gateway.yaml\n1... 2 allowedRoutes: 3 namespaces: 4 from: Selector 5 selector: 6 matchExpressions: 7 - key: kubernetes.io/metadata.name 8 operator: In 9 values: 10 - observability 11 - flux-system 12 tls: 13 mode: Terminate 14 certificateRefs: 15 - name: platform-tls Les HTTPRoutes situ√©es dans les namespaces observability et flux-system r√©fer√©ncent cette m√™me Gateway.\n1... 2spec: 3 parentRefs: 4 - name: platform 5 namespace: infrastructure Et utilisent le m√™me r√©partiteur de charge du Clouder\n1NLB_DOMAIN=$(kubectl get svc -n infrastructure cilium-gateway-platform -o jsonpath={.status.loadBalancer.ingress[0].hostname}) 2 3dig +short ${NLB_DOMAIN} 413.36.89.108 5 6dig +short grafana-mycluster-0.cloud.ogenki.io 713.36.89.108 8 9dig +short gitops-mycluster-0.cloud.ogenki.io 1013.36.89.108 Note üîí Ces outils internes ne devraient pas √™tre expos√©s sur Internet mais vous comprendrez qu'il s'agit l√† d'une d√©mo üôè. On pourrait par exemple, utiliser et une Gateway interne (IP priv√©e) en jouant sur les annotations et un moyen de connexion priv√© (VPN, tunnels ...)\nTraffic splitting Il est souvent utile de tester une application sur une portion du trafic lorsqu'une nouvelle version est disponible (A/B testing ou Canary deployment). GAPI permet cela de fa√ßon tr√®s simple en utilisant des poids.\nVoici un exemple permettant de tester sur 5% du trafic vers le service echo-2-echo-server\napps/base/echo/httproute-split.yaml\n1... 2 hostnames: 3 - \u0026#34;split-echo.${domain_name}\u0026#34; 4 rules: 5 - matches: 6 - path: 7 type: PathPrefix 8 value: / 9 backendRefs: 10 - name: echo-1-echo-server 11 port: 80 12 weight: 95 13 - name: echo-2-echo-server 14 port: 80 15 weight: 5 V√©rifions que la r√©partition se fait bien comme attendu:\nscripts/check-split.sh\n1./scripts/check-split.sh https://split-echo.cloud.ogenki.io 2Number of requests for echo-1: 95 3Number of requests for echo-2: 5 Manipulation des ent√™tes HTTP (Headers) Il est aussi possible de jouer avec les ent√™tes HTTP (Headers): en ajouter, modifier ou supprimer. Ces modifications peuvent se faire sur les Headers de requ√™te ou de r√©ponse par le biais de filtres ajout√©s √† la ressource HTTPRoute.\nNous allons par exemple ajouter un Header √† la requ√™te\n1apiVersion: gateway.networking.k8s.io/v1beta1 2kind: HTTPRoute 3metadata: 4 name: echo-1 5 namespace: echo 6spec: 7... 8 rules: 9 - matches: 10 - path: 11 type: PathPrefix 12 value: /req-header-add 13 filters: 14 - type: RequestHeaderModifier 15 requestHeaderModifier: 16 add: 17 - name: foo 18 value: bar 19 backendRefs: 20 - name: echo-1-echo-server 21 port: 80 22... La commande suivante permet de v√©rifier que le header est bien pr√©sent.\n1curl -s https://echo.cloud.ogenki.io/req-header-add -sk | jq \u0026#39;.request.headers\u0026#39; 2{ 3 \u0026#34;host\u0026#34;: \u0026#34;echo.cloud.ogenki.io\u0026#34;, 4 \u0026#34;user-agent\u0026#34;: \u0026#34;curl/8.2.1\u0026#34;, 5 \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34;, 6 \u0026#34;x-forwarded-for\u0026#34;: \u0026#34;81.220.234.254\u0026#34;, 7 \u0026#34;x-forwarded-proto\u0026#34;: \u0026#34;https\u0026#34;, 8 \u0026#34;x-envoy-external-address\u0026#34;: \u0026#34;81.220.234.254\u0026#34;, 9 \u0026#34;x-request-id\u0026#34;: \u0026#34;320ba4d2-3bd6-4c2f-8a97-74296a9f3f26\u0026#34;, 10 \u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34; 11} ü™™ Les roles et permissions GAPI offre un mod√®le de partage des permissions claire entre l'infrastructure de routage du trafic (g√©r√©e par les administrateurs de cluster) et les applications (g√©r√©es par les d√©veloppeurs).\nLe fait de disposer de plusieurs ressources nous permet d'utiliser les ressources RBAC dans Kubernetes pour attribuer les droits de fa√ßon d√©clarative. J'ai ajout√© quelques exemples qui n'ont aucun effet dans mon cluster de d√©mo mais qui peuvent vous permettre de vous faire une id√©e.\nLa configuration suivante permet aux membres du groupe developers de g√©rer les HTTPRoutes dans le namespace echo. En revanche ils ne poss√©dent que des droits en lecture sur les Gateways.\n1--- 2apiVersion: rbac.authorization.k8s.io/v1 3kind: Role 4metadata: 5 namespace: echo 6 name: gapi-developer 7rules: 8 - apiGroups: [\u0026#34;gateway.networking.k8s.io\u0026#34;] 9 resources: [\u0026#34;httproutes\u0026#34;] 10 verbs: [\u0026#34;*\u0026#34;] 11 - apiGroups: [\u0026#34;gateway.networking.k8s.io\u0026#34;] 12 resources: [\u0026#34;gateways\u0026#34;] 13 verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] 14--- 15apiVersion: rbac.authorization.k8s.io/v1 16kind: RoleBinding 17metadata: 18 name: gapi-developer 19 namespace: echo 20subjects: 21 - kind: Group 22 name: \u0026#34;developers\u0026#34; 23 apiGroup: rbac.authorization.k8s.io 24roleRef: 25 kind: Role 26 name: gapi-developer 27 apiGroup: rbac.authorization.k8s.io ü§î Un p√©rim√®tre pas √©vident Il ne faut pas confondre GAPI avec ce que l'on nomme couramment une API Gateway. Une section de la FAQ a d'ailleurs √©t√© cr√©√© pour √©claircir ce point. Bien que GAPI offre des fonctionnalit√©s typiquement pr√©sentes dans une API Gateway, il s'agit avant tout d'une impl√©mentation sp√©cifique pour Kubernetes. Cependant, ce choix de d√©nomination peut pr√™ter √† confusion.\nIl est essentiel de mentionner que cet article se concentre uniquement sur le trafic entrant, appel√© north-south, traditionnellement g√©r√© par les Ingress Controllers. Ce trafic repr√©sente le p√©rim√®tre initial de GAPI. Une initiative r√©cente nomm√©e GAMMA vise √† √©galement g√©rer le routage east-west, ce qui permettra de standardiser certaines fonctionnalit√©s des solutions de Service Mesh √† l'avenir. (voir cet article pour plus d'informations).\nüí≠ Derni√®res remarques Pour √™tre honn√™te, j'ai entendu parl√© de Gateway API depuis un petit moment. J'ai lu quelques articles mais jusqu'ici je n'avais pas pris le temps d'approfondir le sujet. Je me disais \u0026quot;Pourquoi? J'arrive √† faire ce que je veux avec mon Ingress Controller ? et puis il faut apprendre √† utiliser de nouvelles ressources\u0026quot;.\nGAPI gagne en maturit√© et nous sommes proche d'une version GA. De nombreux projets l'ont d√©j√† adopt√©, Istio et Linkerd par exemple sont totalement compatibles avec la version 0.8.0 et cette fa√ßon de g√©rer le trafic au sein de Kubernetes deviendra rapidement la norme.\nToujours est-il que j'ai beaucoup aim√© la d√©claration des diff√©rentes configurations que je trouve tr√®s intuitive et explicite ‚ù§Ô∏è. D'autre part le mod√®le de s√©curit√© permet de donner le pouvoir aux developpeurs sans sacrifier la s√©curit√©. Enfin la gestion de l'infrastructure se fait de fa√ßon transparente, nous pouvons rapidement passer d'une impl√©mentation (contr√¥leur sous-jacent) √† une autre sans toucher aux *Routes.\nAlors suis-je pr√™t √† changer mon Ingress Controller pour Cilium aujourd'hui? La r√©ponse courte est Non mais bient√¥t!.\nTout d'abord j'aimerais mettre en √©vidence sur l'√©tendue des possiblit√©s offertes par Cilium: De nombreuses personnes se sentent noy√©es sous les nombreux outils qui gravitent autour de Kubernetes. Cilium permettrait de remplir les fonctionnalit√©s de nombre d'entre eux (metrics, tracing, service-mesh, s√©curit√© et ... Ingress Controller avec GAPI).\nCependant, bien que nous puissions faire du routage HTTP de base, il y √† certains points d'attention:\nLe support de TCP et UDP Le support de GRPC Devoir passer par une r√®gle de mutation pour pouvoir configurer les composants cloud. (Issue Github) De nombreuses fonctionnalit√©s explor√©es sont toujours au stade exp√©rimental. On peut citer les fonctions √©tendues qui support√©s depuis quelques jours: J'ai par exemple tent√© de configurer une redirection HTTP\u0026gt;HTTPS simple mais je suis tomb√© sur ce probl√®me. Je m'attends donc √† ce qu'il y ait des changements dans l'API tr√®s prochainement. Je n'ai pas abord√© toutes les fonctionnalit√©s de l'impl√©mentation Cilium de GAPI (Honn√™tement, cet article est d√©j√† bien fourni üòú). N√©anmoins, je suis vraiment convaincu de son potentiel. J'ai bon espoir qu'on pourra bient√¥t envisager son utilisation en production. Si vous n'avez pas encore envisag√© cette transition, c'est le moment de s'y pencher üòâ ! Toutefois, compte tenu des aspects √©voqu√©s pr√©c√©demment, je conseillerais de patienter un peu.\nüîñ References https://gateway-api.sigs.k8s.io/ https://docs.cilium.io/en/latest/network/servicemesh/gateway-api/gateway-api/#gs-gateway-api https://isovalent.com/blog/post/cilium-gateway-api/ https://isovalent.com/blog/post/tutorial-getting-started-with-the-cilium-gateway-api/ Les labs d'Isovalent permettent de rapidement de tester GAPI et vous pourrez ajouter des badges √† votre collection üòÑ ","link":"https://blog.ogenki.io/fr/post/cilium-gateway-api/","section":"post","tags":["kubernetes","infrastructure","network"],"title":"`Gateway API`: Remplacer mon Ingress Controller avec `Cilium`?"},{"body":"","link":"https://blog.ogenki.io/fr/tags/infrastructure/","section":"tags","tags":null,"title":"infrastructure"},{"body":"","link":"https://blog.ogenki.io/fr/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"Terraform est probablement l'outil \u0026quot;Infrastructure As Code\u0026quot; le plus utilis√© pour construire, modifier et versionner les changements d'infrastructure Cloud. Il s'agit d'un projet Open Source d√©velopp√© par Hashicorp et qui utilise le langage HCL pour d√©clarer l'√©tat souhait√© de ressources Cloud. L'√©tat des ressources cr√©√©es est stock√© dans un fichier d'√©tat (terraform state).\nOn peut consid√©rer que Terraform est un outil \u0026quot;semi-d√©claratif\u0026quot; car il n'y a pas de fonctionnalit√© de r√©conciliation automatique int√©gr√©e. Il existe diff√©rentes approches pour r√©pondre √† cette probl√©matique, mais en r√®gle g√©n√©rale, une modification sera appliqu√©e en utilisant terraform apply. Le code est bien d√©crit dans des fichiers de configuration HCL (d√©claratif) mais l'ex√©cution est faite de mani√®re imp√©rative. De ce fait, il peut y avoir de la d√©rive entre l'√©tat d√©clar√© et le r√©el (par exemple, un coll√®gue qui serait pass√© sur la console pour changer un param√®tre üòâ).\n‚ùì‚ùì Alors, comment m'assurer que ce qui est commit dans mon repo git est vraiment appliqu√©. Comment √™tre alert√© s'il y a un changement par rapport √† l'√©tat d√©sir√© et comment appliquer automatiquement ce qui est dans mon code (GitOps) ?\nC'est la promesse de tf-controller, un operateur Kubernetes Open Source de Weaveworks, √©troitement li√© √† Flux (un moteur GitOps de la m√™me soci√©t√©). Flux est l'une des solutions que je pl√©biscite, et je vous invite donc √† lire un pr√©c√©dent article.\nInfo L'ensemble des √©tapes d√©crites ci-dessous sont faites avec ce repo Git\nüéØ Notre objectif En suivant les √©tapes de cet article nous visons les objectifs suivant:\nD√©ployer un cluster Kubernetes qui servira de \u0026quot;Control plane\u0026quot;. Pour r√©sumer il h√©bergera le controlleur Terraform qui nous permettra de d√©clarer tous les √©l√©ments d'infrastructure souhait√©s. Utiliser Flux comme moteur GitOps pour toutes les ressources Kubernetes. Concernant le controleur Terraform, nous allons voir:\nQuelle est le moyen de d√©finir des d√©pendances entre modules Cr√©ation de plusieurs ressources AWS: Zone route53, Certificat ACM, r√©seau, cluster EKS. Les diff√©rentes options de reconciliation (automatique, n√©cessitant une confirmation) Comment sauvegarder et restaurer un fichier d'√©tat (tfstate) üõ†Ô∏è Installer le controleur Terraform ‚ò∏ Le cluster \u0026quot;Control Plane\u0026quot; Afin de pouvoir utiliser le controleur Kubernetes tf-controller, il nous faut d'abord un cluster Kubernetes üòÜ. Nous allons donc cr√©er un cluster control plane en utilisant la ligne de commande terraform et les bonnes pratiques EKS.\nWarning Il est primordial que ce cluster soit r√©siliant, s√©curis√© et supervis√© car il sera responsable de la gestion de l'ensemble des ressources AWS cr√©√©es par la suite.\nSans entrer dans le d√©tail, le cluster \u0026quot;control plane\u0026quot; a √©t√© cr√©√© un utilisant ce code. Cel√†-dit, il est important de noter que toutes les op√©rations de d√©ploiement d'application se font en utilisant Flux.\nInfo En suivant les instructions du README, un cluster EKS sera cr√©√© mais pas uniquement! Il faut en effet donner les permissions au controlleur Terraform pour appliquer les changements d'infrastructure. De plus, Flux doit √™tre install√© et configur√© afin d'appliquer la configuration d√©finie ici.\nAu final on se retrouve donc avec plusieurs √©l√©ments install√©s et configur√©s:\nles addons quasi indispensables que sont aws-loadbalancer-controller et external-dns les roles IRSA pour ces m√™mes composants sont install√©s en utilisant tf-controller La stack de supervision Prometheus / Grafana. external-secrets pour pouvoir r√©cup√©rer des √©l√©ments sensibles depuis AWS secretsmanager. Afin de d√©montrer tout cela au bout de quelques minutes l'interface web pour Flux est accessible via l'URL gitops-\u0026lt;cluster_name\u0026gt;.\u0026lt;domain_name\u0026gt; V√©rifier toute de m√™me que le cluster est accessible et que Flux fonctionne correctement\n1aws eks update-kubeconfig --name controlplane-0 --alias controlplane-0 2Updated context controlplane-0 in /home/smana/.kube/config 1flux check 2... 3‚úî all checks passed 4 5flux get kustomizations 6NAME REVISION SUSPENDED READY MESSAGE 7flux-config main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 8flux-system main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 9infrastructure main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 10security main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 11tf-controller main@sha1:e2cdaced False True Applied revision: main@sha1:e2cdaced 12... üì¶ Le chart Helm et Flux Maintenant que notre cluster \u0026quot;controlplane\u0026quot; est op√©rationnel, l'ajout le contr√¥leur Terraform consiste √† utiliser le chart Helm.\nIl faut tout d'abord d√©clarer la source:\nsource.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: tf-controller 5spec: 6 interval: 30m 7 url: https://weaveworks.github.io/tf-controller Et d√©finir la HelmRelease:\nrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: tf-controller 5spec: 6 releaseName: tf-controller 7 chart: 8 spec: 9 chart: tf-controller 10 sourceRef: 11 kind: HelmRepository 12 name: tf-controller 13 namespace: flux-system 14 version: \u0026#34;0.12.0\u0026#34; 15 interval: 10m0s 16 install: 17 remediation: 18 retries: 3 19 values: 20 resources: 21 limits: 22 memory: 1Gi 23 requests: 24 cpu: 200m 25 memory: 500Mi 26 runner: 27 serviceAccount: 28 annotations: 29 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/tfcontroller_${cluster_name}\u0026#34; Lorsque ce changement est √©crit dans le repo Git, la HelmRelease sera d√©ploy√©e et le contr√¥lleur tf-controller d√©marera\n1kubectl get hr -n flux-system 2NAME AGE READY STATUS 3tf-controller 67m True Release reconciliation succeeded 4 5kubectl get po -n flux-system -l app.kubernetes.io/instance=tf-controller 6NAME READY STATUS RESTARTS AGE 7tf-controller-7ffdc69b54-c2brg 1/1 Running 0 2m6s Dans le repo de demo il y a d√©j√† un certain nombre de ressources AWS d√©clar√©es. Par cons√©quent, au bout de quelques minutes, le cluster se charge de la cr√©ation de celles-cis: Info Bien que la majorit√© des t√¢ches puisse √™tre r√©alis√©e de mani√®re d√©clarative ou via les utilitaires de ligne de commande tels que kubectl et flux, un autre outil existe qui offre la possibilit√© d'interagir avec les ressources terraform : tfctl\nüöÄ Appliquer un changement Parmis les bonnes pratiques avec Terraform, il y a l'usage de modules. Un module est un ensemble de ressources Terraform li√©es logigement afin d'obtenir une seule unit√© r√©utilisable. Cela permet d'abstraire la complexit√©, de prendre des entr√©es, effectuer des actions sp√©cifiques et produire des sorties.\nIl est possible de cr√©er ses propres modules et de les mettre √† disposition dans des Sources ou d'utiliser les nombreux modules partag√©s et maintenus par les communaut√©s. Il suffit alors d'indiquer quelques variables afin de l'adapter au contexte.\nAvec tf-controller, la premi√®re √©tape consiste donc √† indiquer la Source du module. Ici nous allons configurer le socle r√©seau sur AWS (vpc, subnets...) avec le module terraform-aws-vpc.\nsources/terraform-aws-vpc.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1 2kind: GitRepository 3metadata: 4 name: terraform-aws-vpc 5 namespace: flux-system 6spec: 7 interval: 30s 8 ref: 9 tag: v5.0.0 10 url: https://github.com/terraform-aws-modules/terraform-aws-vpc Nous pouvons ensuite cr√©er la ressource Terraform qui en fait usage:\nvpc/dev.yaml\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6 interval: 8m 7 path: . 8 destroyResourcesOnDeletion: true # You wouldn\u0026#39;t do that on a prod env ;) 9 storeReadablePlan: human 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-vpc 13 namespace: flux-system 14 vars: 15 - name: name 16 value: vpc-dev 17 - name: cidr 18 value: \u0026#34;10.42.0.0/16\u0026#34; 19 - name: azs 20 value: 21 - \u0026#34;eu-west-3a\u0026#34; 22 - \u0026#34;eu-west-3b\u0026#34; 23 - \u0026#34;eu-west-3c\u0026#34; 24 - name: private_subnets 25 value: 26 - \u0026#34;10.42.0.0/19\u0026#34; 27 - \u0026#34;10.42.32.0/19\u0026#34; 28 - \u0026#34;10.42.64.0/19\u0026#34; 29 - name: public_subnets 30 value: 31 - \u0026#34;10.42.96.0/24\u0026#34; 32 - \u0026#34;10.42.97.0/24\u0026#34; 33 - \u0026#34;10.42.98.0/24\u0026#34; 34 - name: enable_nat_gateway 35 value: true 36 - name: single_nat_gateway 37 value: true 38 - name: private_subnet_tags 39 value: 40 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 41 \u0026#34;karpenter.sh/discovery\u0026#34;: dev 42 - name: public_subnet_tags 43 value: 44 \u0026#34;kubernetes.io/role/elb\u0026#34;: 1 45 writeOutputsToSecret: 46 name: vpc-dev Si l'on devait r√©sumer grossi√®rement: le code terraform provenant de la source terraform-aws-vpc est utilis√© avec les variables vars.\nIl y a ensuite plusieurs param√®tres qui influent sur le fonctionnement de tf-controller. Les principaux param√®tres qui permettent de contr√¥ler la fa√ßon dont sont appliqu√©es les modifications sont .spec.approvePlan et .spec.autoApprove\nüö® D√©tection de la d√©rive D√©finir spec.approvePlan avec une valeur √† disable permet uniquement de notifier que l'√©tat actuel des ressources a d√©riv√© par rapport au code Terraform. Cela permet notamment de choisir le moment et la mani√®re dont l'application des changements sera effectu√©e.\nNote De mon point de vue il manque une section sur les notifications: La d√©rive, les plans en attentes, les probl√®mese de r√©concilation. J'essaye d'identifier les m√©thodes possibles (de pr√©f√©rence avec Prometheus) et de mettre √† jour cet article d√®s que possible.\nüîß Application manuelle L'exemple donn√© pr√©c√©demment (vpc-dev) ne contient pas le param√®tre .spec.approvePlan et h√©rite donc de la valeur par d√©faut qui est false. Par cons√©quent, l'application concr√®te des modifications (apply), n'est pas faite automatiquement.\nUn plan est ex√©cut√© et sera en attente d'une validation:\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system vpc-dev Unknown Plan generated: set approvePlan: \u0026#34;plan-v5.0.0-26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. true 2 minutes Je conseille d'ailleurs de configurer le param√®tre storeReadablePlan √† human. Cela permet de visualiser simplement les modifications en attente en utilisant tfctl:\n1tfctl show plan vpc-dev 2 3Terraform used the selected providers to generate the following execution 4plan. ressource actions are indicated with the following symbols: 5 + create 6 7Terraform will perform the following actions: 8 9 # aws_default_network_acl.this[0] will be created 10 + ressource \u0026#34;aws_default_network_acl\u0026#34; \u0026#34;this\u0026#34; { 11 + arn = (known after apply) 12 + default_network_acl_id = (known after apply) 13 + id = (known after apply) 14 + owner_id = (known after apply) 15 + tags = { 16 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 17 } 18 + tags_all = { 19 + \u0026#34;Name\u0026#34; = \u0026#34;vpc-dev-default\u0026#34; 20 } 21 + vpc_id = (known after apply) 22 23 + egress { 24 + action = \u0026#34;allow\u0026#34; 25 + from_port = 0 26 + ipv6_cidr_block = \u0026#34;::/0\u0026#34; 27 + protocol = \u0026#34;-1\u0026#34; 28 + rule_no = 101 29 + to_port = 0 30 } 31 + egress { 32... 33Plan generated: set approvePlan: \u0026#34;plan-v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671\u0026#34; to approve this plan. 34To set the field, you can also run: 35 36 tfctl approve vpc-dev -f filename.yaml Apr√®s revue des modifications ci-dessus, il suffit donc d'ajouter l'identifiant du plan √† valider et de pousser le changement sur git comme suit:\n1apiVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: vpc-dev 5spec: 6... 7 approvePlan: plan-v5.0.0-26c38a66f1 8... En quelques instants un runner sera lanc√© qui se chargera d'appliquer les changements:\n1kubectl logs -f -n flux-system vpc-dev-tf-runner 22023/07/01 15:33:36 Starting the runner... version sha 3... 4aws_vpc.this[0]: Creating... 5aws_vpc.this[0]: Still creating... [10s elapsed] 6... 7aws_route_table_association.private[1]: Creation complete after 0s [id=rtbassoc-01b7347a7e9960a13] 8aws_nat_gateway.this[0]: Still creating... [10s elapsed] La r√©conciliation √©ffectu√©e, la ressource passe √† l'√©tat READY: True\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m ü§ñ Application automatique Nous pouvons aussi activer la r√©conciliation automatique. Pour ce faire il faut d√©clarer le param√®tre .spec.autoApprove √† true.\nToutes les ressources IRSA sont configur√©es de la sorte:\nexternal-secrets.yaml\n1piVersion: infra.contrib.fluxcd.io/v1alpha2 2kind: Terraform 3metadata: 4 name: irsa-external-secrets 5spec: 6 approvePlan: auto 7 destroyResourcesOnDeletion: true 8 interval: 8m 9 path: ./modules/iam-role-for-service-accounts-eks 10 sourceRef: 11 kind: GitRepository 12 name: terraform-aws-iam 13 namespace: flux-system 14 vars: 15 - name: role_name 16 value: ${cluster_name}-external-secrets 17 - name: attach_external_secrets_policy 18 value: true 19 - name: oidc_providers 20 value: 21 main: 22 provider_arn: ${oidc_provider_arn} 23 namespace_service_accounts: [\u0026#34;security:external-secrets\u0026#34;] Donc si je fais le moindre changement sur la console AWS par exemple, celui-ci sera rapidement √©cras√© par celui g√©r√© par tf-controller.\nInfo La politique de suppression d'une ressource Terraform est d√©finie par le param√®tre destroyResourcesOnDeletion. Par d√©faut elles sont conserv√©es et il faut donc que ce param√®tre ait pour valeur true afin de d√©truire les √©l√©ments cr√©es lorsque l'objet Kubernetes est supprim√©.\nIci nous voulons la possibilit√© de supprimer les r√¥les IRSA. Ils sont en effet √©troitement li√©s aux clusters.\nüîÑ Entr√©es et sorties: d√©pendances entre modules Lorsque qu'on utilise Terraform, on a souvent besoin de passer des donn√©es d'un module √† l'autre. G√©n√©ralement ce sont les outputs du module qui exportent ces informations. Il faut donc un moyen de les importer dans un autre module.\nReprenons encore l'exemple donn√© ci-dessus (vpc-dev). Nous notons en bas du YAML la directive suivante:\n1... 2 writeOutputsToSecret: 3 name: vpc-dev Lorsque cette ressource est appliqu√©e nous aurons un message qui confirme que les outputs sont disponibles (\u0026quot;Outputs written\u0026quot;):\n1kubectl get tf -n flux-system vpc-dev 2NAME READY STATUS AGE 3vpc-dev True Outputs written: v5.0.0@sha1:26c38a66f12e7c6c93b6a2ba127ad68981a48671 17m En effet ce module exporte de nombreuses informations (126):\n1kubectl get secrets -n flux-system vpc-dev 2NAME TYPE DATA AGE 3vpc-dev Opaque 126 15s 4 5kubectl get secret -n flux-system vpc-dev --template=\u0026#39;{{.data.vpc_id}}\u0026#39; | base64 -d 6vpc-0c06a6d153b8cc4db Certains de ces √©l√©ments d'informations sont ensuite utilis√©s pour cr√©er un cluster EKS de dev:\nvpc/dev.yaml\n1... 2 varsFrom: 3 - kind: Secret 4 name: vpc-dev 5 varsKeys: 6 - vpc_id 7 - private_subnets 8... üíæ Sauvegarder et restaurer un tfstate Dans mon cas je ne souhaite pas recr√©er la zone et le certificat √† chaque destruction du controlplane. Voici un exemple des √©tapes √† mener pour que je puisse restaurer l'√©tat de ces ressources lorsque j'utilise cette demo.\nNote Il s'agit l√† d'une proc√©dure manuelle afin de d√©montrer le comportement de tf-controller par rapport aux fichiers d'√©tat. Par d√©faut ces tfstates sont stock√©s dans des secrets mais on pr√©ferera configurer un backend GCS ou S3\nLa cr√©ation initiale de l'environnement de d√©mo m'a permis de sauvegarder les fichiers d'√©tat (tfstate) de cette fa√ßon.\n1WORKSPACE=\u0026#34;default\u0026#34; 2STACK=\u0026#34;route53-cloud-hostedzone\u0026#34; 3BACKUPDIR=\u0026#34;${HOME}/tf-controller-backup\u0026#34; 4 5mkdir -p ${BACKUPDIR} 6 7kubectl get secrets -n flux-system tfstate-${WORKSPACE}-${STACK} -o jsonpath=\u0026#39;{.data.tfstate}\u0026#39; | \\ 8base64 -d | gzip -d \u0026gt; ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate Lorsque le cluster est cr√©√© √† nouveau, tf-controller essaye de cr√©er la zone car le fichier d'√©tat est vide.\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3... 4flux-system route53-cloud-hostedzone Unknown Plan generated: set approvePlan: \u0026#34;plan-main@sha1:345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. true 16 minutes 5 6tfctl show plan route53-cloud-hostedzone 7 8Terraform used the selected providers to generate the following execution 9plan. resource actions are indicated with the following symbols: 10 + create 11 12Terraform will perform the following actions: 13 14 # aws_route53_zone.this will be created 15 + resource \u0026#34;aws_route53_zone\u0026#34; \u0026#34;this\u0026#34; { 16 + arn = (known after apply) 17 + comment = \u0026#34;Experimentations for blog.ogenki.io\u0026#34; 18 + force_destroy = false 19 + id = (known after apply) 20 + name = \u0026#34;cloud.ogenki.io\u0026#34; 21 + name_servers = (known after apply) 22 + primary_name_server = (known after apply) 23 + tags = { 24 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 25 } 26 + tags_all = { 27 + \u0026#34;Name\u0026#34; = \u0026#34;cloud.ogenki.io\u0026#34; 28 } 29 + zone_id = (known after apply) 30 } 31 32Plan: 1 to add, 0 to change, 0 to destroy. 33 34Changes to Outputs: 35 + domain_name = \u0026#34;cloud.ogenki.io\u0026#34; 36 + nameservers = (known after apply) 37 + zone_arn = (known after apply) 38 + zone_id = (known after apply) 39 40Plan generated: set approvePlan: \u0026#34;plan-main@345394fb4a82b9b258014332ddd556dde87f73ab\u0026#34; to approve this plan. 41To set the field, you can also run: 42 43 tfctl approve route53-cloud-hostedzone -f filename.yaml La proc√©dure de restauration consiste donc √† cr√©er le secret √† nouveau:\n1gzip ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate 2 3cat \u0026lt;\u0026lt;EOF | kubectl apply -f - 4apiVersion: v1 5kind: Secret 6metadata: 7 name: tfstate-${WORKSPACE}-${STACK} 8 namespace: flux-system 9 annotations: 10 encoding: gzip 11type: Opaque 12data: 13 tfstate: $(cat ${BACKUPDIR}/${WORKSPACE}-${STACK}.tfstate.gz | base64 -w 0) 14EOF Il faudra aussi relancer un plan de fa√ßon explicite pour mettre √† jour l'√©tat de la ressource en question\n1tfctl replan route53-cloud-hostedzone 2Ôò´ Replan requested for flux-system/route53-cloud-hostedzone 3Error: timed out waiting for the condition Nous pouvons alors v√©rifier que le fichier d'√©tat a bien √©t√© mis √† jour\n1tfctl get 2NAMESPACE NAME READY MESSAGE PLAN PENDING AGE 3flux-system route53-cloud-hostedzone True Outputs written: main@sha1:d0934f979d832feb870a8741ec01a927e9ee6644 false 19 minutes üîç Focus sur certaines fonctionnalit√©s de Flux Oui j'ai un peu menti sur l'agenda üòù. Il me semblait n√©cessaire de mettre en lumi√®re 2 fonctionnalit√©s que je n'avais pas exploit√© jusque l√† et qui sont fort utiles!\nSubstition de variables Lorsque Flux est initilias√© un certain nombre de Kustomization sp√©cifique √† ce cluster sont cr√©√©s. Il est possible d'y indiquer des variables de substitution qui pourront √™tre utilis√©es dans l'ensemble des ressources d√©ploy√©es par cette Kustomization. Cela permet d'√©viter un maximum la d√©duplication de code.\nJ'ai d√©couvert l'efficacit√© de cette fonctionnalit√© tr√®s r√©cemment. Je vais d√©crire ici la fa√ßon dont je l'utilise:\nLe code terraform qui cr√©e un cluster EKS, g√©n√®re aussi une ConfigMap qui contient les variables propres au cluster. On y retrouvera, bien s√ªr, le nom du cluster, mais aussi tous les param√®tres qui varient entre les clusters et qui sont utilis√©s dans les manifests Kubernetes.\nflux.tf\n1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;flux_clusters_vars\u0026#34; { 2 metadata { 3 name = \u0026#34;eks-${var.cluster_name}-vars\u0026#34; 4 namespace = \u0026#34;flux-system\u0026#34; 5 } 6 7 data = { 8 cluster_name = var.cluster_name 9 oidc_provider_arn = module.eks.oidc_provider_arn 10 aws_account_id = data.aws_caller_identity.this.account_id 11 region = var.region 12 environment = var.env 13 vpc_id = module.vpc.vpc_id 14 } 15 depends_on = [flux_bootstrap_git.this] 16} Comme sp√©cifi√© pr√©cedemment, les variables de substition sont d√©finies dans les Kustomization. Prenons un exemple concret. Ci-dessous on d√©finie la Kustomization qui d√©ploie toutes les ressources qui sont consomm√©es par tf-controller On d√©clare ici la ConfigMap eks-controlplane-0-vars qui avait √©t√© g√©n√©r√© √† la cr√©ation du cluster EKS.\ninfrastructure.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1 2kind: Kustomization 3metadata: 4 name: tf-custom-resources 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 path: ./infrastructure/controlplane-0/terraform/custom-resources 10 postBuild: 11 substitute: 12 domain_name: \u0026#34;cloud.ogenki.io\u0026#34; 13 substituteFrom: 14 - kind: ConfigMap 15 name: eks-controlplane-0-vars 16 - kind: Secret 17 name: eks-controlplane-0-vars 18 optional: true 19 sourceRef: 20 kind: GitRepository 21 name: flux-system 22 dependsOn: 23 - name: tf-controller Enfin voici un exemple de ressource Kubernetes qui en fait usage. Cet unique manifest peut √™tre utilis√© par tous les clusters!.\nexternal-dns/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: external-dns 5spec: 6... 7 values: 8 global: 9 imageRegistry: public.ecr.aws 10 fullnameOverride: external-dns 11 aws: 12 region: ${region} 13 zoneType: \u0026#34;public\u0026#34; 14 batchChangeSize: 1000 15 domainFilters: [\u0026#34;${domain_name}\u0026#34;] 16 logFormat: json 17 txtOwnerId: \u0026#34;${cluster_name}\u0026#34; 18 serviceAccount: 19 annotations: 20 eks.amazonaws.com/role-arn: \u0026#34;arn:aws:iam::${aws_account_id}:role/${cluster_name}-external-dns\u0026#34; Cela √©limine totalement les overlays qui consistaient √† ajouter les param√®tres sp√©cifiques au cluster.\nWeb UI (Weave GitOps) Dans mon pr√©c√©dent article sur Flux, je mentionnais le fait que l'un des inconv√©nients (si l'on compare avec son principale concurrent: ArgoCD) est le manque d'une interface Web. Bien que je sois un adepte de la ligne de commande, c'est parfois bien utile d'avoir une vue synth√©tique et de pouvoir effectuer certaines op√©ration en quelques clicks \u0026#x1f5b1;\u0026#xfe0f;\nC'est d√©sormais possible avec Weave Gitops! Bien entendu ce n'est pas comparable avec l'UI d'ArgoCD, mais l'essentiel est l√†: Mettre en pause la r√©concilation, visualiser les manifests, les d√©pendances, les √©v√©nements...\nIl existe aussi le plugin VSCode comme alternative.\nüí≠ Remarques Et voil√†, nous arrivons au bout de notre exploration de cet autre outil de gestion d'infrastructure sur Kubernetes. Malgr√© quelques petits soucis rencontr√©s en cours de route, que j'ai partag√© sur le repo Git du projet, l'exp√©rience m'a beaucoup plu. tf-controller offre une r√©ponse concr√®te √† une question fr√©quente : comment g√©rer notre infra comme on g√®re notre code ?\nJ'aime beaucoup l'approche GitOps appliqu√©e √† l'infrastructure, j'avais d'ailleurs √©crit un article sur Crossplane. tf-controller aborde la probl√©matique sous un angle diff√©rent: utiliser du Terraform directement. Cela signifie qu'on peut utiliser nos connaissances actuelles et notre code existant. Pas besoin d'apprendre une nouvelle fa√ßon de d√©clarer nos ressources. C'est un crit√®re √† prendre en compte car migrer vers un nouvel outil lorsque l'on a un existant repr√©sente un √©ffort non n√©gligeable. Cependant j'ajouterais aussi que tf-controller s'adresse aux utilisateurs de Flux uniquement et, de ce fait, restreint le publique cible.\nAujourd'hui, j'utilise une combinaison de Terraform, Terragrunt et RunAtlantis. tf-controller pourrait devenir une alternative viable: Nous avons en effet √©voqu√© l'int√©r√™t de Kustomize associ√© aux substitions de variables pour la factorisation de code. Dans la roadmap du projet il y a aussi l'objectif d'afficher les plans dans les pull-requests. Autre probl√©matique fr√©quente: la n√©cessit√© de passer des √©l√©ments sensibles aux modules. En utilisant une ressource Terraform, on peut injecter des variables depuis des secrets Kubernetes. Ce qui permet d'utiliser certains outils, tels que external-secrets, sealed-secrets ...\nJe vous encourage donc √† essayer tf-controller vous-m√™me, et peut-√™tre m√™me d'y apporter votre contribution üôÇ.\nNote La d√©mo que j'ai faite ici utilise pas mal de ressources, dont certaines assez cruciales (comme le r√©seau). Donc, gardez en t√™te que c'est juste pour la d√©mo ! Je sugg√®re une approche progressive si vous envisagez de le mettre en ouvre: commencez par utiliser la d√©tection de d√©rives, puis cr√©ez des ressources simples. J'ai aussi pris quelques raccourcis en terme de s√©curit√© √† √©viter absolument, notamment le fait de donner les droits admin au contr√¥leur. ","link":"https://blog.ogenki.io/fr/post/terraform-controller/","section":"post","tags":["infrastructure"],"title":"Appliquer les principes de GitOps √† l'infrastructure: Introduction √† `tf-controller`"},{"body":"Kubernetes est d√©sormais la plate-forme privil√©gi√©e pour orchestrer les applications \u0026quot;sans √©tat\u0026quot; aussi appel√© \u0026quot;stateless\u0026quot;. Les conteneurs qui ne stockent pas de donn√©es peuvent √™tre d√©truits et recr√©√©s ailleurs sans impact. En revanche, la gestion d'applications \u0026quot;stateful\u0026quot; dans un environnement dynamique tel que Kubernetes peut √™tre un v√©ritable d√©fi. Malgr√© le fait qu'il existe un nombre croissant de solutions de base de donn√©es \u0026quot;Cloud Native\u0026quot; (comme CockroachDB, TiDB, K8ssandra, Strimzi ...) et il y a de nombreux √©l√©ments √† consid√©rer lors de leur √©valuation:\nQuelle est la maturit√© de l'op√©rateur? (Dynamisme et contributeurs, gouvernance du projet) Quels sont les resources personalis√©es disponibles (\u0026quot;custom resources\u0026quot;), quelles op√©rations permettent t-elles de r√©aliser? Quels sont les type de stockage disponibles: HDD / SSD, stockage local / distant? Que se passe-t-il lorsque quelque chose se passe mal: Quelle est le niveau de r√©silience de la solution? Sauvegarde et restauration: est-il facile d'effectuer et de planifier des sauvegardes? Quelles options de r√©plication et de mise √† l'√©chelle sont disponibles? Qu'en est-il des limites de connexion et de concurrence, les pools de connexion? A propos de la supervision, quelles sont les m√©triques expos√©es et comment les exploiter? J'√©tais √† la recherche d'une solution permettant de g√©rer un serveur PostgreSQL. La base de donn√©es qui y serait h√©berg√©e est n√©cessaire pour un logiciel de r√©servation de billets nomm√© Alf.io. Nous sommes en effet en train d'organiser les Kubernetes Community Days France vous √™tes tous convi√©s! üëê.\nJe cherchais sp√©cifiquement une solution ind√©pendante d'un clouder (cloud agnostic) et l'un des principaux crit√®res √©tait la simplicit√© d'utilisation. Je connaissais d√©j√† plusieurs op√©rateurs Kubernetes, et j'ai fini par √©valuer une solution relativement r√©cente: CloudNativePG.\nCloudNativepg est l'op√©rateur de Kubernetes qui couvre le cycle de vie complet d'un cluster de base de donn√©es PostgreSQL hautement disponible avec une architecture de r√©plication native en streaming.\nCe projet √©t√© cr√©√© par l'entreprise EnterpriseDB et a √©t√© soumis √† la CNCF afin de rejoindre les projets Sandbox.\nüéØ Notre objectif Je vais donner ici une introduction aux principales fonctionnalit√©s de CloudNativePG.\nL'objectif est de:\nCr√©er une base de donn√©es PostgreSQL sur un cluster GKE, Ajouter une instance secondaire (r√©plication) Ex√©cuter quelques tests de r√©silience. Nous verrons √©galement comment tout cela se comporte en terme de performances et quels sont les outils de supervision disponibles. Enfin, nous allons jeter un ≈ìil aux m√©thodes de sauvegarde/restauration.\nInfo Dans cet article, nous allons tout cr√©er et tout mettre √† jour manuellement. Mais dans un environnement de production, il est conseill√© d'utiliser un moteur GitOps, par exemple Flux (sujet couvert dans un article pr√©c√©dent).\nSi vous souhaitez voir un exemple complet, vous pouvez consulter le d√©p√¥t git KCD France infrastructure.\nToutes les resources de cet article sont dans ce d√©p√¥t.\n\u0026#x2611;\u0026#xfe0f; Pr√©requis \u0026#x1f4e5; Outils gcloud SDK: Nous allons d√©ployer sur Google Cloud (en particulier sur GKE) et, pour ce faire, nous devrons cr√©er quelques ressources dans notre projet GCP. Nous aurons donc besoin du SDK et de la CLI Google Cloud. Il est donc n√©cessaire de l'installer en suivant cette documentation.\nkubectl plugin: Pour faciliter la gestion des clusters, il existe un plugin kubectl qui donne des informations synth√©tiques sur l'instance PostgreSQL et permet aussi d'effectuer certaines op√©rations. Ce plugin peut √™tre install√© en utilisant krew:\n1kubectl krew install cnpg ‚òÅÔ∏è Cr√©er les resources Google Cloud Avant de cr√©er notre instance PostgreSQL, nous devons configurer certaines choses:\nNous avons besoin d'un cluster Kubernetes. (Cet article suppose que vous avez d√©j√† pris soin de provisionner un cluster GKE) Nous allons cr√©er un bucket (Google Cloud Storage) pour stocker les sauvegardes et Fichiers WAL. Nous configurerons les permissions pour nos pods afin qu'ils puissent √©crire dans ce bucket. Cr√©er le bucket √† l'aide de CLI gcloud\n1gcloud storage buckets create --location=eu --default-storage-class=coldline gs://cnpg-ogenki 2Creating gs://cnpg-ogenki/... 3 4gcloud storage buckets describe gs://cnpg-ogenki 5[...] 6name: cnpg-ogenki 7owner: 8 entity: project-owners-xxxx0008 9projectNumber: \u0026#39;xxx00008\u0026#39; 10rpo: DEFAULT 11selfLink: https://www.googleapis.com/storage/v1/b/cnpg-ogenki 12storageClass: STANDARD 13timeCreated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; 14updated: \u0026#39;2022-10-15T19:27:54.364000+00:00\u0026#39; Nous allons maintenant configurer les permissions afin que les pods (PostgreSQL Server) puissent permettant √©crire/lire √† partir du bucket gr√¢ce √† Workload Identity.\nNote Workload Identity doit √™tre activ√© au niveau du cluster GKE. Afin de v√©rifier que le cluster est bien configur√©, vous pouvez lancer la commande suivante:\n1gcloud container clusters describe \u0026lt;cluster_name\u0026gt; --format json --zone \u0026lt;zone\u0026gt; | jq .workloadIdentityConfig 2{ 3 \u0026#34;workloadPool\u0026#34;: \u0026#34;{{ gcp_project }}.svc.id.goog\u0026#34; 4} Cr√©er un compte de service Google Cloud\n1gcloud iam service-accounts create cloudnative-pg --project={{ gcp_project }} 2Created service account [cloudnative-pg]. Attribuer au compte de service la permission storage.admin\n1gcloud projects add-iam-policy-binding {{ gcp_project }} \\ 2--member \u0026#34;serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com\u0026#34; \\ 3--role \u0026#34;roles/storage.admin\u0026#34; 4[...] 5- members: 6 - serviceAccount:cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 7 role: roles/storage.admin 8etag: BwXrGA_VRd4= 9version: 1 Autoriser le compte de service (Attention il s'agit l√† du compte de service au niveau Kubernetes) afin d'usurper le compte de service IAM. \u0026#x2139;\u0026#xfe0f; Assurez-vous d'utiliser le format appropri√© serviceAccount:{{ gcp_project }}.svc.id.goog[{{ kubernetes_namespace }}/{{ kubernetes_serviceaccount }}]\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 7 role: roles/iam.workloadIdentityUser 8etag: BwXrGBjt5kQ= 9version: 1 Nous sommes pr√™ts √† cr√©er les ressources Kubernetes \u0026#x1f4aa;\n\u0026#x1f511; Cr√©er les secrets pour les utilisateurs PostgreSQL Nous devons cr√©er les param√®tres d'authentification des utilisateurs qui seront cr√©√©s pendant la phase de \u0026quot;bootstrap\u0026quot; (nous y reviendrons par la suite): le superutilisateur et le propri√©taire de base de donn√©es nouvellement cr√©√©.\n1kubectl create secret generic cnpg-mydb-superuser --from-literal=username=postgres --from-literal=password=foobar --namespace demo 2secret/cnpg-mydb-superuser created 1kubectl create secret generic cnpg-mydb-user --from-literal=username=smana --from-literal=password=barbaz --namespace demo 2secret/cnpg-mydb-user created \u0026#x1f6e0;\u0026#xfe0f; D√©ployer l'op√©rateur CloudNativePG avec Helm Ici nous utiliserons le chart Helm pour d√©ployer CloudNativePG:\n1helm repo add cnpg https://cloudnative-pg.github.io/charts 2 3helm upgrade --install cnpg --namespace cnpg-system \\ 4--create-namespace charts/cloudnative-pg 5 6kubectl get po -n cnpg-system 7NAME READY STATUS RESTARTS AGE 8cnpg-74488f5849-8lhjr 1/1 Running 0 6h17m Cela installe aussi quelques resources personnalis√©es (Custom Resources Definitions)\n1kubectl get crds | grep cnpg.io 2backups.postgresql.cnpg.io 2022-10-08T16:15:14Z 3clusters.postgresql.cnpg.io 2022-10-08T16:15:14Z 4poolers.postgresql.cnpg.io 2022-10-08T16:15:14Z 5scheduledbackups.postgresql.cnpg.io 2022-10-08T16:15:14Z Pour une liste compl√®te des param√®tres possibles, veuillez vous r√©f√©rer √† la doc de l'API.\n\u0026#x1f680; Cr√©er un serveur PostgreSQL Nous pouvons d√©sormais cr√©er notre premi√®re instance en utilisant une resource personnalis√©e Cluster. La d√©finition suivante est assez simple: Nous souhaitons d√©marrer un serveur PostgreSQL, cr√©er automatiquement une base de donn√©es nomm√©e mydb et configurer les informations d'authentification en utilisant les secrets cr√©√©s pr√©c√©demment.\n1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki 5 namespace: demo 6spec: 7 description: \u0026#34;PostgreSQL Demo Ogenki\u0026#34; 8 imageName: ghcr.io/cloudnative-pg/postgresql:14.5 9 instances: 1 10 11 bootstrap: 12 initdb: 13 database: mydb 14 owner: smana 15 secret: 16 name: cnpg-mydb-user 17 18 serviceAccountTemplate: 19 metadata: 20 annotations: 21 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 22 23 superuserSecret: 24 name: cnpg-mydb-superuser 25 26 storage: 27 storageClass: standard 28 size: 10Gi 29 30 backup: 31 barmanObjectStore: 32 destinationPath: \u0026#34;gs://cnpg-ogenki\u0026#34; 33 googleCredentials: 34 gkeEnvironment: true 35 retentionPolicy: \u0026#34;30d\u0026#34; 36 37 resources: 38 requests: 39 memory: \u0026#34;1Gi\u0026#34; 40 cpu: \u0026#34;500m\u0026#34; 41 limits: 42 memory: \u0026#34;1Gi\u0026#34; Cr√©er le namespace o√π notre instance postgresql sera d√©ploy√©e\n1kubectl create ns demo 2namespace/demo created Adapdez le fichier YAML ci-dessus vos besoins et appliquez comme suit:\n1kubectl apply -f cluster.yaml 2cluster.postgresql.cnpg.io/ogenki created Vous remarquerez que le cluster sera en phase Initializing. Nous allons utiliser le plugin CNPG pour la premi√®re fois afin de v√©rifier son √©tat. Cet outil deviendra par la suite notre meilleur ami pour afficher une vue synth√©tique de l'√©tat du cluster.\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Primary server is initializing 4Name: ogenki 5Namespace: demo 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: (switching to ogenki-1) 8Status: Setting up primary Creating primary instance ogenki-1 9Instances: 1 10Ready instances: 0 11 12Certificates Status 13Certificate Name Expiration Date Days Left Until Expiration 14---------------- --------------- -------------------------- 15ogenki-ca 2023-01-13 20:02:40 +0000 UTC 90.00 16ogenki-replication 2023-01-13 20:02:40 +0000 UTC 90.00 17ogenki-server 2023-01-13 20:02:40 +0000 UTC 90.00 18 19Continuous Backup status 20First Point of Recoverability: Not Available 21No Primary instance found 22Streaming Replication status 23Not configured 24 25Instances status 26Name Database Size Current LSN Replication role Status QoS Manager Version Node 27---- ------------- ----------- ---------------- ------ --- --------------- ---- imm√©diatement apr√®s la d√©claration de notre nouveau Cluster, une action de bootstrap est lanc√©e. Dans notre exemple, nous cr√©ons une toute nouvelle base de donn√©es nomm√©e mydb avec un propri√©taire smana dont les informations d'authentification viennent du secret cr√©√© pr√©c√©demment.\n1[...] 2 bootstrap: 3 initdb: 4 database: mydb 5 owner: smana 6 secret: 7 name: cnpg-mydb-user 8[...] 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 0/1 Running 0 55s 4ogenki-1-initdb-q75cz 0/1 Completed 0 2m32s Apr√®s quelques secondes, le cluster change de statut et devient Ready (configur√© et pr√™t √† l'usage) \u0026#x1f44f;\n1kubectl cnpg status ogenki -n demo 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7154833472216277012 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 1 10Ready instances: 1 11 12[...] 13 14Instances status 15Name Database Size Current LSN Replication role Status QoS Manager Version Node 16---- ------------- ----------- ---------------- ------ --- --------------- ---- 17ogenki-1 33 MB 0/17079F8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xczh Info Il existe de nombreuses fa√ßons de bootstrap un cluster. Par exemple, la restauration d'une sauvegarde dans une toute nouvelle instance ou en ex√©cutant du code SQL ... Plus d'infos ici.\nü©π Instance de secours et r√©silience Info Dans les architectures postgresql traditionnelles, nous trouvons g√©n√©ralement un composant suppl√©mentaire pour g√©rer la haute disponibilit√© (ex: Patroni). Un particularit√© de l'op√©rateur CloudNativePG est qu'il b√©n√©ficie des fonctionnalit√©s de base de Kubernetes et s'appuie sur un composant nomm√© Postgres instance manager.\nAjoutez une instance de secours (\u0026quot;standby\u0026quot;) en d√©finissant le nombre de r√©pliques sur 2.\n1kubectl edit cluster -n demo ogenki 2cluster.postgresql.cnpg.io/ogenki edited 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3[...] 4spec: 5 instances: 2 6[...] L'op√©rateur remarque imm√©diatement le changement, ajoute une instance de secours et d√©marre le processus de r√©plication.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Creating a new replica Creating replica ogenki-2-join 9Instances: 2 10Ready instances: 1 11Current Write LSN: 0/1707A30 (Timeline: 1 - WAL File: 000000010000000000000001) 1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 0 3m16s 4ogenki-2-join-xxrwx 0/1 Pending 0 82s Apr√®s un certain temps (qui d√©pend de la quantit√© de donn√©es √† r√©pliquer), l'instance de secours devient op√©rationnelle et nous pouvons voir les statistiques de r√©plication.\n1kubectl cnpg status -n demo ogenki 2Cluster Summary 3Name: ogenki 4Namespace: demo 5System ID: 7155095145869606932 6PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 7Primary instance: ogenki-1 8Status: Cluster in healthy state 9Instances: 2 10Ready instances: 2 11Current Write LSN: 0/3000060 (Timeline: 1 - WAL File: 000000010000000000000003) 12 13[...] 14 15Streaming Replication status 16Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 17---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 18ogenki-2 0/3000060 0/3000060 0/3000060 0/3000060 00:00:00 00:00:00 00:00:00 streaming async 0 19 20Instances status 21Name Database Size Current LSN Replication role Status QoS Manager Version Node 22---- ------------- ----------- ---------------- ------ --- --------------- ---- 23ogenki-1 33 MB 0/3000060 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 24ogenki-2 33 MB 0/3000060 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc Nous allons d√©sormais √©ffectuer ce que l'on appelle un \u0026quot;Switchover\u0026quot;: Nous allons promouvoir l'instance de secours en instance primaire.\nLe plugin cnpg permet de le faire de fa√ßon imp√©rative, en utilisant la ligne de commande suivante:\n1kubectl cnpg promote ogenki ogenki-2 -n demo 2Node ogenki-2 in cluster ogenki will be promoted Dans mon cas, le basculement √©tait vraiment rapide. Nous pouvons v√©rifier que l'instance ogenki-2 est devenu primaire et que la r√©plication est effectu√©e dans l'autre sens.\n1kubectl cnpg status -n demo ogenki 2[...] 3Status: Switchover in progress Switching over to ogenki-2 4Instances: 2 5Ready instances: 1 6[...] 7Streaming Replication status 8Name Sent LSN Write LSN Flush LSN Replay LSN Write Lag Flush Lag Replay Lag State Sync State Sync Priority 9---- -------- --------- --------- ---------- --------- --------- ---------- ----- ---------- ------------- 10ogenki-1 0/4004CA0 0/4004CA0 0/4004CA0 0/4004CA0 00:00:00 00:00:00 00:00:00 streaming async 0 11 12Instances status 13Name Database Size Current LSN Replication role Status QoS Manager Version Node 14---- ------------- ----------- ---------------- ------ --- --------------- ---- 15ogenki-2 33 MB 0/4004CA0 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-xszc 16ogenki-1 33 MB 0/4004CA0 Standby (async) OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 Maintenant, provoquons un Failover en supprimant le pod principal\n1kubectl delete po -n demo --grace-period 0 --force ogenki-2 2Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. 3pod \u0026#34;ogenki-2\u0026#34; force deleted 1Cluster Summary 2Name: ogenki 3Namespace: demo 4System ID: 7155095145869606932 5PostgreSQL Image: ghcr.io/cloudnative-pg/postgresql:14.5 6Primary instance: ogenki-1 7Status: Failing over Failing over from ogenki-2 to ogenki-1 8Instances: 2 9Ready instances: 1 10Current Write LSN: 0/4005D98 (Timeline: 3 - WAL File: 000000030000000000000004) 11 12[...] 13Instances status 14Name Database Size Current LSN Replication role Status QoS Manager Version Node 15---- ------------- ----------- ---------------- ------ --- --------------- ---- 16ogenki-1 33 MB 0/40078D8 Primary OK Burstable 1.18.0 gke-kcdfrance-main-np-0e87115b-76k7 17ogenki-2 - - - pod not available Burstable - gke-kcdfrance-main-np-0e87115b-xszc Quelques secondes plus tard le cluster devient op√©rationnel √† nouveau.\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 13m 2 2 Cluster in healthy state ogenki-1 Jusqu'ici tout va bien, nous avons pu faire quelques tests de la haute disponibilit√© et c'√©tait assez probant üòé.\nüëÅÔ∏è Supervision Nous allons utiliser la Stack Prometheus. Nous ne couvrirons pas son installation dans cet article. Si vous voulez voir comment l'installer avec Flux, vous pouvez jeter un oeil √† cet exemple.\nPour r√©cup√©rer les m√©triques de notre instance, nous devons cr√©er un PodMonitor.\n1apiVersion: monitoring.coreos.com/v1 2kind: PodMonitor 3metadata: 4 labels: 5 prometheus-instance: main 6 name: cnpg-ogenki 7 namespace: demo 8spec: 9 namespaceSelector: 10 matchNames: 11 - demo 12 podMetricsEndpoints: 13 - port: metrics 14 selector: 15 matchLabels: 16 postgresql: ogenki Nous pouvons ensuite ajouter le tableau de bord Grafana disponible ici.\nEnfin, vous souhaiterez peut-√™tre configurer des alertes et vous pouvez cr√©er un PrometheusRule en utilisant ces r√®gles.\n\u0026#x1f525; Performances and benchmark Info Mise √† jour: Il est d√©sormais possible de faire un test de performance avec le plugin cnpg\nAfin de connaitre les limites de votre serveur, vous devriez faire un test de performances et de conserver une base de r√©f√©rence pour de futures am√©liorations.\nNote Au sujet des performances, il existe de nombreux domaines d'am√©lioration sur lesquels nous pouvons travailler.Cela d√©pend principalement de l'objectif que nous voulons atteindre. En effet, nous ne voulons pas perdre du temps et de l'argent pour les performances dont nous n'aurons probablement jamais besoin.\nVoici les principaux √©l√©ments √† analyser:\nTuning de la configuration PostgreSQL Resources syst√®mes (cpu et m√©moire) Types de Disque : IOPS, stockage locale (local-volume-provisioner), Disques d√©di√©es pour les WAL et les donn√©es PG_DATA \u0026quot;Pooling\u0026quot; de connexions PGBouncer. CloudNativePG fourni une resource personnalis√©e Pooler qui permet de configurer cela facilement. Optimisation de la base de donn√©es, analyser les plans d'ex√©cution gr√¢ce √† explain, utiliser l'extension pg_stat_statement ... Tout d'abord, nous ajouterons des \u0026quot;labels\u0026quot; aux n≈ìuds afin d'ex√©cuter la commande pgbench sur diff√©rentes machines de celles h√©bergeant la base de donn√©es.\n1PG_NODE=$(kubectl get po -n demo -l postgresql=ogenki,role=primary -o jsonpath={.items[0].spec.nodeName}) 2kubectl label node ${PG_NODE} workload=postgresql 3node/gke-kcdfrance-main-np-0e87115b-vlzm labeled 4 5 6# Choose any other node different than the ${PG_NODE} 7kubectl label node gke-kcdfrance-main-np-0e87115b-p5d7 workload=pgbench 8node/gke-kcdfrance-main-np-0e87115b-p5d7 labeled Et nous d√©ploierons le chart Helm comme suit\n1git clone git@github.com:EnterpriseDB/cnp-bench.git 2cd cnp-bench 3 4cat \u0026gt; pgbench-benchmark/myvalues.yaml \u0026lt;\u0026lt;EOF 5cnp: 6 existingCluster: true 7 existingHost: ogenki-rw 8 existingCredentials: cnpg-mydb-superuser 9 existingDatabase: mydb 10 11pgbench: 12 # Node where to run pgbench 13 nodeSelector: 14 workload: pgbench 15 initialize: true 16 scaleFactor: 1 17 time: 600 18 clients: 10 19 jobs: 1 20 skipVacuum: false 21 reportLatencies: false 22EOF 23 24helm upgrade --install -n demo pgbench -f pgbench-benchmark/myvalues.yaml pgbench-benchmark/ Info Il existe diff√©rents services selon que vous souhaitez lire et √©crire ou de la lecture seule.\n1kubectl get ep -n demo 2NAME ENDPOINTS AGE 3ogenki-any 10.64.1.136:5432,10.64.1.3:5432 15d 4ogenki-r 10.64.1.136:5432,10.64.1.3:5432 15d 5ogenki-ro 10.64.1.136:5432 15d 6ogenki-rw 10.64.1.3:5432 15d 1kubectl logs -n demo job/pgbench-pgbench-benchmark -f 2Defaulted container \u0026#34;pgbench\u0026#34; out of: pgbench, wait-for-cnp (init), pgbench-init (init) 3pgbench (14.1, server 14.5 (Debian 14.5-2.pgdg110+2)) 4starting vacuum...end. 5transaction type: \u0026lt;builtin: TPC-B (sort of)\u0026gt; 6scaling factor: 1 7query mode: simple 8number of clients: 10 9number of threads: 1 10duration: 600 s 11number of transactions actually processed: 545187 12latency average = 11.004 ms 13initial connection time = 111.585 ms 14tps = 908.782896 (without initial connection time) üíΩ Sauvegarde and restaurations Note Le fait de pouvoir stocker des sauvegarde et fichiers WAL dans le bucket GCP est possible car nous avons attribu√© les autorisations en utilisant une annotation pr√©sente dans le ServiceAccount utilis√© par le cluster\n1serviceAccountTemplate: 2 metadata: 3 annotations: 4 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com Nous pouvons d'abord d√©clencher une sauvegarde on demand √† l'aide de la ressource personnalis√©e Backup\n1apiVersion: postgresql.cnpg.io/v1 2kind: Backup 3metadata: 4 name: ogenki-now 5 namespace: demo 6spec: 7 cluster: 8 name: ogenki 1kubectl apply -f backup.yaml 2backup.postgresql.cnpg.io/ogenki-now created 3 4kubectl get backup -n demo 5NAME AGE CLUSTER PHASE ERROR 6ogenki-now 36s ogenki completed Si vous jetez un ≈ìil au contenu du bucket GCS, vous verrez un nouveau r√©pertoire qui stocke les sauvegardes de base (\u0026quot;base backups\u0026quot;).\n1gcloud storage ls gs://cnpg-ogenki/ogenki/base 2gs://cnpg-ogenki/ogenki/base/20221023T130327/ Mais la plupart du temps, nous pr√©fererons configurer une sauvegarde planifi√©e (\u0026quot;scheduled\u0026quot;). Ci-dessous un exemple pour une sauvegarde quotidienne:\n1apiVersion: postgresql.cnpg.io/v1 2kind: ScheduledBackup 3metadata: 4 name: ogenki-daily 5 namespace: demo 6spec: 7 backupOwnerReference: self 8 cluster: 9 name: ogenki 10 schedule: 0 0 0 * * * Les restaurations ne peuvent √™tre effectu√©es que sur de nouvelles instances. Ici, nous utiliserons la sauvegarde que nous avions cr√©√©e pr√©c√©demment afin d'initialiser une toute nouvelle instance.\n1gcloud iam service-accounts add-iam-policy-binding cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com \\ 2--role roles/iam.workloadIdentityUser --member \u0026#34;serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore]\u0026#34; 3Updated IAM policy for serviceAccount [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 4bindings: 5- members: 6 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki-restore] 7 - serviceAccount:{{ gcp_project }}.svc.id.goog[demo/ogenki] 8 role: roles/iam.workloadIdentityUser 9etag: BwXrs755FPA= 10version: 1 1apiVersion: postgresql.cnpg.io/v1 2kind: Cluster 3metadata: 4 name: ogenki-restore 5 namespace: demo 6spec: 7 instances: 1 8 9 serviceAccountTemplate: 10 metadata: 11 annotations: 12 iam.gke.io/gcp-service-account: cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 13 14 storage: 15 storageClass: standard 16 size: 10Gi 17 18 resources: 19 requests: 20 memory: \u0026#34;1Gi\u0026#34; 21 cpu: \u0026#34;500m\u0026#34; 22 limits: 23 memory: \u0026#34;1Gi\u0026#34; 24 25 superuserSecret: 26 name: cnpg-mydb-superuser 27 28 bootstrap: 29 recovery: 30 backup: 31 name: ogenki-now Nous notons qu'un pod se charge imm√©diatement de la restauration compl√®te (\u0026quot;full recovery\u0026quot;).\n1kubectl get po -n demo 2NAME READY STATUS RESTARTS AGE 3ogenki-1 1/1 Running 1 (18h ago) 18h 4ogenki-2 1/1 Running 0 18h 5ogenki-restore-1 0/1 Init:0/1 0 0s 6ogenki-restore-1-full-recovery-5p4ct 0/1 Completed 0 51s Le nouveau cluster devient alors op√©rationnel (\u0026quot;Ready\u0026quot;).\n1kubectl get cluster -n demo 2NAME AGE INSTANCES READY STATUS PRIMARY 3ogenki 18h 2 2 Cluster in healthy state ogenki-1 4ogenki-restore 80s 1 1 Cluster in healthy state ogenki-restore-1 \u0026#x1f9f9; Nettoyage Suppression du cluster\n1kubectl delete cluster -n demo ogenki ogenki-restore 2cluster.postgresql.cnpg.io \u0026#34;ogenki\u0026#34; deleted 3cluster.postgresql.cnpg.io \u0026#34;ogenki-restore\u0026#34; deleted Supprimer le service IAM\n1gcloud iam service-accounts delete cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com 2You are about to delete service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com]. 3 4Do you want to continue (Y/n)? y 5 6deleted service account [cloudnative-pg@{{ gcp_project }}.iam.gserviceaccount.com] üí≠ Conclusion Je viens tout juste de d√©couvrir CloudNativePG et je n'ai fait qu'en percevoir la surface, mais une chose est s√ªre: la gestion d'une instance PostgreSQL est vraiment facilit√©e. Cependant, le choix d'une solution de base de donn√©es est une d√©cision complexe. Il faut prendre en compte le cas d'usage, les contraintes de l'entreprise, la criticit√© de l'application et les comp√©tences des √©quipes op√©rationnelles. Il existe de nombreuses options: bases de donn√©es g√©r√©es par le fournisseur Cloud, installation traditionnelle sur serveur baremetal, solutions distribu√©es ...\nNous pouvons √©galement envisager d'utiliser Crossplane et une Composition pour fournir un niveau d'abstraction suppl√©mentaire afin de d√©clarer des bases de donn√©es des fournisseurs Cloud, mais cela n√©cessite plus de configuration.\nCloudNativePG sort du lot par sa simplicit√©: Super facile √† ex√©cuter et √† comprendre. De plus, la documentation est excellente (l'une des meilleures que j'aie jamais vues!), Surtout pour un si jeune projet open source (cela aidera peut √™tre pour √™tre accept√© en tant que projet \u0026quot;Sandbox\u0026quot; CNCF ü§û).\nSi vous voulez en savoir plus, il y avait une pr√©sentation √† ce sujet √† KubeCon NA 2022.\n","link":"https://blog.ogenki.io/fr/post/cnpg/","section":"post","tags":["data"],"title":"`CloudNativePG`: et PostgreSQL devient facile sur Kubernetes"},{"body":"","link":"https://blog.ogenki.io/fr/tags/data/","section":"tags","tags":null,"title":"data"},{"body":"In a previous article, we've seen how to use Crossplane so that we can manage cloud resources the same way as our applications. \u0026#x2764;\u0026#xfe0f; Declarative approach! There were several steps and command lines in order to get everything working and reach our target to provision a dev Kubernetes cluster.\nHere we'll achieve exactly the same thing but we'll do that in the GitOps way. According to the OpenGitOps working group there are 4 GitOps principles:\nThe desired state of our system must be expressed declaratively. This state must be stored in a versioning system. Changes are pulled and applied automatically in the target platform whenever the desired state changes. If, for any reason, the current state is modified, it will be automatically reconciled with the desired state. There are several GitOps engine options. The most famous ones are ArgoCD and Flux. We won't compare them here. I chose Flux because I like its composable architecture with different controllers, each one handling a core Flux feature (GitOps toolkit).\nLearn more about GitOps toolkit components here.\nüéØ Our target Here we want to declare our desired infrastructure components only by adding git changes. By the end of this article you'll get a GKE cluster provisioned using a local Crossplane instance. We'll discover Flux basics and how to use it in order to build a complete GitOps CD workflow.\n\u0026#x2611;\u0026#xfe0f; Requirements \u0026#x1f4e5; Install required tools First of all we need to install a few tools using asdf\nCreate a local file .tool-versions\n1cd ~/sources/devflux/ 2 3cat \u0026gt; .tool-versions \u0026lt;\u0026lt;EOF 4flux2 0.31.3 5kubectl 1.24.3 6kubeseal 0.18.1 7kustomize 4.5.5 8EOF 1for PLUGIN in $(cat .tool-versions | awk \u0026#39;{print $1}\u0026#39;); do asdf plugin-add $PLUGIN; done 2 3asdf install 4Downloading ... 100.0% 5Copying Binary 6... Check that all the required tools are actually installed.\n1asdf current 2flux2 0.31.3 /home/smana/sources/devflux/.tool-versions 3kubectl 1.24.3 /home/smana/sources/devflux/.tool-versions 4kubeseal 0.18.1 /home/smana/sources/devflux/.tool-versions 5kustomize 4.5.5 /home/smana/sources/devflux/.tool-versions \u0026#x1f511; Create a Github personal access token In this article the git repository is hosted in Github. In order to be able to use the flux bootstrap a personnal access token is required.\nPlease follow this procedure.\nWarning Store the Github token in a safe place for later use\n\u0026#x1f9d1;\u0026zwj;\u0026#x1f4bb; Clone the devflux repository All the files used for the upcoming steps can be retrieved from this repository. You should clone it, that will be easier to copy them into your own repository.\n1git clone https://github.com/Smana/devflux.git \u0026#x1f680; Bootstrap flux in the Crossplane cluster As we will often be using the flux CLI you may want to configure the bash|zsh completion\n1source \u0026lt;(flux completion bash) Warning Here we consider that you already have a local k3d instance. If not you may want to either go through the whole previous article or just run the local cluster creation.\nEnsure that you're working in the right context\n1kubectl config current-context 2k3d-crossplane Run the bootstrap command that will basically deploy all Flux's components in the namespace flux-system. Here I'll create a repository named devflux using my personal Github account.\n1export GITHUB_USER=\u0026lt;YOUR_ACCOUNT\u0026gt; 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/k3d-crossplane 6‚ñ∫ cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git\u0026#34; 7... 8‚úî configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/k3d-crossplane\u0026#34; for \u0026#34;https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux\u0026#34; 9... 10‚úî all components are healthy Check that all the pods are running properly and that the kustomization flux-system has been successfully reconciled.\n1kubectl get po -n flux-system 2NAME READY STATUS RESTARTS AGE 3helm-controller-5985c795f8-gs2pc 1/1 Running 0 86s 4notification-controller-6b7d7485fc-lzlpg 1/1 Running 0 86s 5kustomize-controller-6d4669f847-9x844 1/1 Running 0 86s 6source-controller-5fb4888d8f-wgcqv 1/1 Running 0 86s 7 8flux get kustomizations 9NAME REVISION SUSPENDED READY MESSAGE 10flux-system main/33ebef1 False True Applied revision: main/33ebef1 Running the bootstap command actually creates a github repository if it doesn't exist yet. Clone it now for our upcoming changes. You'll notice that the first commit has been made by Flux.\n1git clone https://github.com/\u0026lt;YOUR_ACCOUNT\u0026gt;/devflux.git 2Cloning into \u0026#39;devflux\u0026#39;... 3 4cd devflux 5 6git log -1 7commit 2beb6aafea67f3386b50cbc706fb34575844040d (HEAD -\u0026gt; main, origin/main, origin/HEAD) 8Author: Flux \u0026lt;\u0026gt; 9Date: Thu Jul 14 17:13:27 2022 +0200 10 11 Add Flux sync manifests 12 13ls clusters/k3d-crossplane/flux-system/ 14gotk-components.yaml gotk-sync.yaml kustomization.yaml \u0026#x1f4c2; Flux repository structure There are several options for organizing your resources in the Flux configuration repository. Here is a proposition for the sake of this article.\n1tree -d -L 2 2. 3‚îú‚îÄ‚îÄ apps 4‚îÇ¬†‚îú‚îÄ‚îÄ base 5‚îÇ¬†‚îî‚îÄ‚îÄ dev-cluster 6‚îú‚îÄ‚îÄ clusters 7‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 8‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 9‚îú‚îÄ‚îÄ infrastructure 10‚îÇ¬†‚îú‚îÄ‚îÄ base 11‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 12‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 13‚îú‚îÄ‚îÄ observability 14‚îÇ¬†‚îú‚îÄ‚îÄ base 15‚îÇ¬†‚îú‚îÄ‚îÄ dev-cluster 16‚îÇ¬†‚îî‚îÄ‚îÄ k3d-crossplane 17‚îî‚îÄ‚îÄ security 18 ‚îú‚îÄ‚îÄ base 19 ‚îú‚îÄ‚îÄ dev-cluster 20 ‚îî‚îÄ‚îÄ k3d-crossplane Directory Description Example /apps our applications Here we'll deploy a demo application \u0026quot;online-boutique\u0026quot; /infrastructure base infrastructure/network components Crossplane as it will be used to provision cloud resources but we can also find CSI/CNI/EBS drivers... /observability All metrics/apm/logging tools Prometheus of course, Opentelemetry ... /security Any component that enhance our security level SealedSecrets (see below) Info For the upcoming steps please refer to the demo repository here\nLet's use this structure and begin to deploy applications \u0026#x1f680;.\n\u0026#x1f510; SealedSecrets There are plenty of alternatives when it comes to secrets management in Kubernetes. In order to securely store secrets in a git repository the GitOps way we'll make use of SealedSecrets. It uses a custom resource definition named SealedSecrets in order to encrypt the Kubernetes secret at the client side then the controller is in charge of decrypting and generating the expected secret in the cluster.\n\u0026#x1f6e0;\u0026#xfe0f; Deploy the controller using Helm The first thing to do is to declare the kustomization that handles all the security tools.\nclusters/k3d-crossplane/security.yaml\n1apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 2kind: Kustomization 3metadata: 4 name: security 5 namespace: flux-system 6spec: 7 prune: true 8 interval: 4m0s 9 sourceRef: 10 kind: GitRepository 11 name: flux-system 12 path: ./security/k3d-crossplane 13 healthChecks: 14 - apiVersion: helm.toolkit.fluxcd.io/v1beta1 15 kind: HelmRelease 16 name: sealed-secrets 17 namespace: kube-system Info A Kustomization is a custom resource that comes with Flux. It basically points to a set of Kubernetes resources managed with kustomize The above security kustomization points to a local directory where the kustomize resources are.\n1... 2spec: 3 path: ./security/k3d-crossplane 4... Note This is worth noting that there are two types on kustomizations. That can be confusing when you start playing with Flux.\nOne managed by flux's kustomize controller. Its API is kustomization.kustomize.toolkit.fluxcd.io The other kustomization.kustomize.config.k8s.io is for the kustomize overlay The kustomization.yaml file is always used for the kustomize overlay. Flux itself doesn't need this overlay in all cases, but if you want to use features of a Kustomize overlay you will occasionally need to create it in order to access them. It provides instructions to the Kustomize CLI.\nWe will deploy SealedSecrets using the Helm chart. So we need to declare the source of this chart. Using the kustomize overlay system, we'll first create the base files that will be inherited at the cluster level.\nsecurity/base/sealed-secrets/source.yaml\n1apiVersion: source.toolkit.fluxcd.io/v1beta2 2kind: HelmRepository 3metadata: 4 name: sealed-secrets 5 namespace: flux-system 6spec: 7 interval: 30m 8 url: https://bitnami-labs.github.io/sealed-secrets Then we'll define the HelmRelease which references the above source. Put the values you want to apply to the Helm chart under spec.values\nsecurity/base/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 releaseName: sealed-secrets 8 chart: 9 spec: 10 chart: sealed-secrets 11 sourceRef: 12 kind: HelmRepository 13 name: sealed-secrets 14 namespace: flux-system 15 version: \u0026#34;2.4.0\u0026#34; 16 interval: 10m0s 17 install: 18 remediation: 19 retries: 3 20 values: 21 fullnameOverride: sealed-secrets-controller 22 resources: 23 requests: 24 cpu: 80m 25 memory: 100Mi If you're starting your repository from scratch you'll need to generate the kustomization.yaml file (kustomize overlay).\n1kustomize create --autodetect security/base/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4- helmrelease.yaml 5- source.yaml Now we declare the sealed-secret kustomization at the cluster level. Just for the example we'll overwrite a value at the cluster level using kustomize's overlay system.\nsecurity/k3d-crossplane/sealed-secrets/helmrelease.yaml\n1apiVersion: helm.toolkit.fluxcd.io/v2beta1 2kind: HelmRelease 3metadata: 4 name: sealed-secrets 5 namespace: kube-system 6spec: 7 values: 8 resources: 9 requests: 10 cpu: 100m security/k3d-crossplane/sealed-secrets/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3bases: 4 - ../../base 5patches: 6 - helmrelease.yaml Pushing our changes is the only thing to do in order to get sealed-secrets deployed in the target cluster.\n1git commit -m \u0026#34;security: deploy sealed-secrets in k3d-crossplane\u0026#34; 2[security/sealed-secrets 283648e] security: deploy sealed-secrets in k3d-crossplane 3 6 files changed, 66 insertions(+) 4 create mode 100644 clusters/k3d-crossplane/security.yaml 5 create mode 100644 security/base/sealed-secrets/helmrelease.yaml 6 create mode 100644 security/base/sealed-secrets/kustomization.yaml 7 create mode 100644 security/base/sealed-secrets/source.yaml 8 create mode 100644 security/k3d-crossplane/sealed-secrets/helmrelease.yaml 9 create mode 100644 security/k3d-crossplane/sealed-secrets/kustomization.yaml After a few seconds (1 minutes by default) a new kustomization will appear.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3flux-system main/d36a33c False True Applied revision: main/d36a33c 4security main/d36a33c False True Applied revision: main/d36a33c And all the resources that we declared in the flux repository should be available and READY.\n1flux get sources helm 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee False True stored artifact for revision \u0026#39;4c0aa1980e3ec9055dea70abd2b259aad1a2c235325ecf51a25a92a39ac4eeee\u0026#39; 1flux get helmrelease -n kube-system 2NAME REVISION SUSPENDED READY MESSAGE 3sealed-secrets 2.2.0 False True Release reconciliation succeeded üß™ A first test SealedSecret Let's use the CLI kubeseal to test it out. We'll create a SealedSecret that will be decrypted by the sealed-secrets controller in the cluster and create the expected secret foobar\n1kubectl create secret generic foobar -n default --dry-run=client -o yaml --from-literal=foo=bar \\ 2| kubeseal --namespace default --format yaml | kubectl apply -f - 3sealedsecret.bitnami.com/foobar created 4 5kubectl get secret -n default foobar 6NAME TYPE DATA AGE 7foobar Opaque 1 3m13s 8 9kubectl delete sealedsecrets.bitnami.com foobar 10sealedsecret.bitnami.com \u0026#34;foobar\u0026#34; deleted \u0026#x2601;\u0026#xfe0f; Deploy and configure Crossplane \u0026#x1f511; Create the Google service account secret The first thing we need to do in order to get Crossplane working is to create the GCP serviceaccount. The steps have been covered here in the previous article. We'll create a SealedSecret gcp-creds that contains the serviceaccount file crossplane.json.\ninfrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml\n1kubectl create secret generic gcp-creds --context k3d-crossplane -n crossplane-system --from-file=creds=./crossplane.json --dry-run=client -o yaml \\ 2| kubeseal --format yaml --namespace crossplane-system - \u0026gt; infrastructure/k3d-crossplane/crossplane/configuration/sealedsecrets.yaml üîÑ Crossplane dependencies Now we will deploy Crossplane with Flux. I won't put the manifests here you'll find all of them in this repository. However it's important to understand that, in order to deploy and configure Crossplane properly we need to do that in a specific order. Indeed several CRD's (custom resource definitions) are required:\nFirst of all we'll install the crossplane controller. Then we'll configure the provider because the custom resource is now available thanks to the crossplane controller installation. Finally a provider installation deploys several CRDs that can be used to configure the provider itself and cloud resources. The dependencies between kustomizations can be controlled using the parameters dependsOn. Looking at the file clusters/k3d-crossplane/infrastructure.yaml, we can see for example that the kustomization infrastructure-custom-resources depends on the kustomization crossplane_provider which itself depends on crossplane-configuration....\n1--- 2apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 3kind: Kustomization 4metadata: 5 name: crossplane-provider 6spec: 7... 8 dependsOn: 9 - name: crossplane-core 10--- 11apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 12kind: Kustomization 13metadata: 14 name: crossplane-configuration 15spec: 16... 17 dependsOn: 18 - name: crossplane-provider 19--- 20apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 21kind: Kustomization 22metadata: 23 name: infrastructure-custom-resources 24spec: 25... 26 dependsOn: 27 - name: crossplane-configuration Commit and push the changes for the kustomisations to appear. Note that they'll be reconciled in the defined order.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3infrastructure-custom-resources False False dependency \u0026#39;flux-system/crossplane-configuration\u0026#39; is not ready 4crossplane-configuration False False dependency \u0026#39;flux-system/crossplane-provider\u0026#39; is not ready 5security main/666f85a False True Applied revision: main/666f85a 6flux-system main/666f85a False True Applied revision: main/666f85a 7crossplane-core main/666f85a False True Applied revision: main/666f85a 8crossplane-provider main/666f85a False True Applied revision: main/666f85a Then all Crossplane components will be deployed, we can have a look to the HelmRelease status for instance.\n1kubectl describe helmrelease -n crossplane-system crossplane 2... 3Status: 4 Conditions: 5 Last Transition Time: 2022-07-15T19:12:04Z 6 Message: Release reconciliation succeeded 7 Reason: ReconciliationSucceeded 8 Status: True 9 Type: Ready 10 Last Transition Time: 2022-07-15T19:12:04Z 11 Message: Helm upgrade succeeded 12 Reason: UpgradeSucceeded 13 Status: True 14 Type: Released 15 Helm Chart: crossplane-system/crossplane-system-crossplane 16 Last Applied Revision: 1.9.0 17 Last Attempted Revision: 1.9.0 18 Last Attempted Values Checksum: 056dc1c6029b3a644adc7d6a69a93620afd25b65 19 Last Release Revision: 2 20 Observed Generation: 1 21Events: 22 Type Reason Age From Message 23 ---- ------ ---- ---- ------- 24 Normal info 20m helm-controller HelmChart \u0026#39;crossplane-system/crossplane-system-crossplane\u0026#39; is not ready 25 Normal info 20m helm-controller Helm upgrade has started 26 Normal info 19m helm-controller Helm upgrade succeeded And our GKE cluster should also be created because we defined a bunch of crossplane custom resources in infrastructure/k3d-crossplane/custom-resources/crossplane\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RUNNING 34.x.x.190 europe-west9-a 22m \u0026#x1f680; Bootstrap flux in the dev cluster Our local Crossplane cluster is now ready and it created our dev cluster and we also want it to be managed with Flux. So let's configure Flux for this dev cluster using the same bootstrap command.\nAuthenticate to the newly created cluster. The following command will automatically change your current context.\n1gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project \u0026lt;your_project\u0026gt; 2Fetching cluster endpoint and auth data. 3kubeconfig entry generated for dev-cluster. 4 5kubectl config current-context 6gke_\u0026lt;your_project\u0026gt;_europe-west9-a_dev-cluster Run the bootstrap command for the dev-cluster.\n1export GITHUB_USER=Smana 2export GITHUB_TOKEN=ghp_\u0026lt;REDACTED\u0026gt; # your personal access token 3export GITHUB_REPO=devflux 4 5flux bootstrap github --owner=\u0026#34;${GITHUB_USER}\u0026#34; --repository=\u0026#34;${GITHUB_REPO}\u0026#34; --personal --path=clusters/dev-cluster 6‚ñ∫ cloning branch \u0026#34;main\u0026#34; from Git repository \u0026#34;https://github.com/Smana/devflux.git\u0026#34; 7... 8‚úî configured deploy key \u0026#34;flux-system-main-flux-system-./clusters/dev-cluster\u0026#34; for \u0026#34;https://github.com/Smana/devflux\u0026#34; 9... 10‚úî all components are healthy Note It's worth noting that each Kubernetes cluster generates its own sealing keys. That means that if you recreate the dev-cluster, you must regenerate all the sealedsecrets. In our example we declared a secret in order to set the Grafana credentials. Here's the command you need to run in order to create a new version of the sealedsecret and don't forget to use the proper context \u0026#x1f609;.\n1kubectl create secret generic kube-prometheus-stack-grafana \\ 2--from-literal=admin-user=admin --from-literal=admin-password=\u0026lt;yourpassword\u0026gt; --namespace observability --dry-run=client -o yaml \\ 3| kubeseal --namespace observability --format yaml \u0026gt; observability/dev-cluster/kube-prometheus-stack/sealedsecrets.yaml After a few seconds we'll get the following kustomizations deployed.\n1flux get kustomizations 2NAME REVISION SUSPENDED READY MESSAGE 3apps main/1380eaa False True Applied revision: main/1380eaa 4flux-system main/1380eaa False True Applied revision: main/1380eaa 5observability main/1380eaa False True Applied revision: main/1380eaa 6security main/1380eaa False True Applied revision: main/1380eaa Here we configured the prometheus stack and deployed a demo microservices stack named \u0026quot;online-boutique\u0026quot; This demo application exposes the frontend through a service of type LoadBalancer.\n1kubectl get svc -n demo frontend-external 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3frontend-external LoadBalancer 10.140.174.201 34.155.121.2 80:31943/TCP 7m44s Use the EXTERNAL_IP\n\u0026#x1f575;\u0026#xfe0f; Troubleshooting The cheatsheet in Flux's documentation contains many ways for troubleshooting when something goes wrong. Here I'll just give a sample of my favorite command lines.\nObjects that aren't ready\n1flux get all -A --status-selector ready=false Checking the logs of a given kustomization\n1flux logs --kind kustomization --name infrastructure-custom-resources 22022-07-15T19:38:52.996Z info Kustomization/infrastructure-custom-resources.flux-system - server-side apply completed 32022-07-15T19:38:53.016Z info Kustomization/infrastructure-custom-resources.flux-system - Reconciliation finished in 66.12266ms, next run in 4m0s 42022-07-15T19:11:34.697Z info Kustomization/infrastructure-custom-resources.flux-system - Discarding event, no alerts found for the involved object Show how a given pod is managed by Flux.\n1flux trace -n crossplane-system pod/crossplane-5dc8d888d7-g95qx 2 3Object: Pod/crossplane-5dc8d888d7-g95qx 4Namespace: crossplane-system 5Status: Managed by Flux 6--- 7HelmRelease: crossplane 8Namespace: crossplane-system 9Revision: 1.9.0 10Status: Last reconciled at 2022-07-15 21:12:04 +0200 CEST 11Message: Release reconciliation succeeded 12--- 13HelmChart: crossplane-system-crossplane 14Namespace: crossplane-system 15Chart: crossplane 16Version: 1.9.0 17Revision: 1.9.0 18Status: Last reconciled at 2022-07-15 21:11:36 +0200 CEST 19Message: pulled \u0026#39;crossplane\u0026#39; chart with version \u0026#39;1.9.0\u0026#39; 20--- 21HelmRepository: crossplane 22Namespace: crossplane-system 23URL: https://charts.crossplane.io/stable 24Revision: 362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d 25Status: Last reconciled at 2022-07-15 21:11:35 +0200 CEST 26Message: stored artifact for revision \u0026#39;362022f8c7ce215a0bb276887115cb5324b35a3169723900c84456adc3538a8d\u0026#39; If you want to check what would be the changes before pushing your commit. In thi given example I just increased the cpu requests for the sealed-secrets controller.\n1flux diff kustomization security --path security/k3d-crossplane 2‚úì Kustomization diffing... 3‚ñ∫ HelmRelease/kube-system/sealed-secrets drifted 4 5metadata.generation 6 ¬± value change 7 - 6 8 + 7 9 10spec.values.resources.requests.cpu 11 ¬± value change 12 - 100m 13 + 120m 14 15‚ö†Ô∏è identified at least one change, exiting with non-zero exit code \u0026#x1f9f9; Cleanup Don't forget to delete the Cloud resources if you don't want to have a bad suprise \u0026#x1f4b5;! Just comment the file infrastructure/k3d-crossplane/custom-resources/crossplane/kustomization.yaml\n1apiVersion: kustomize.config.k8s.io/v1beta1 2kind: Kustomization 3resources: 4 # - cluster.yaml 5 - network.yaml \u0026#x1f44f; Achievements With our current setup everything is configured using the GitOps approach:\nWe can manage infrastructure resources using Crossplane. Our secrets are securely stored in our git repository. We have a dev-cluster that we can enable or disable just but commenting a yaml file. Our demo application can be deployed from scratch in seconds. üí≠ final thoughts Flux is probably the tool I'm using the most on a daily basis. It's really amazing!\nWhen you get familiar with its concepts and the command line it becomes really easy to use and troubleshoot. You can use either Helm when a chart is available or Kustomize.\nHowever we faced a few issues:\nIt's not straightforward to find an efficient structure depending on the company needs. Especially when you have several Kubernetes controllers that depend on other CRDs. The Helm controller doesn't maintain a state of the Kubernetes resources deployed by the Helm chart. That means that if you delete a resource which has been deployed through a Helm chart, it won't be reconciled (It will change soon. Being discussed here) Flux doesn't provide itself a web UI and switching between CLIs (kubectl, flux ...) can be annoying from a developer perspective. (I'm going to test weave-gitops ) I've been using Flux in production for more than a year and we configured it with the image automation so that the only thing a developer has to do is to merge a pull request and the new version of the application is automatically deployed in the target cluster.\nI should probably give another try to ArgoCD in order to be able to compare these precisely ü§î.\n","link":"https://blog.ogenki.io/fr/post/devflux/","section":"post","tags":["gitops","devxp"],"title":"100% `GitOps` using Flux"},{"body":"Qui suis-je? Je suis un ing√©nieur syst√®me / SRE senior. Je porte un int√©r√™t particulier aux conteneurs Linux et les technologies d√Ætes \u0026quot;Cloud Native\u0026quot;. Au fil de ma carri√®re, j'ai eu l'occasion de collaborer avec une vari√©t√© d'entreprises, des startups dynamiques aux grandes structures, o√π j'ai contribu√© √† optimiser la fiabilit√© et la disponibilit√© des plateformes, tout en am√©liorant l'exp√©rience des d√©veloppeurs. Mon parcours inclut l'accompagnement de plusieurs soci√©t√©s dans leur migration vers des solutions Cloud. J'ai √©galement pu diriger des √©quipes SRE/DevOps compos√©es de profils vari√©s, incluant d√©veloppeurs et ing√©nieurs SRE, avec pour objectif un engagement collectif vers des objectifs communs.\nEn parall√®le de mon parcours professionnel, je m'investis dans l'organisation du meetup Cloud Native Computing √† Paris ainsi que les Kubernetes Community Days France , refl√©tant mon engagement continu dans l'√©cosyst√®me du Cloud.\nAutres centre d'int√©ret : Lecture de romans de science-fiction, Kickboxing, Surf, Longboard et Rollers..\n","link":"https://blog.ogenki.io/fr/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://blog.ogenki.io/fr/tags/devxp/","section":"tags","tags":null,"title":"devxp"},{"body":"","link":"https://blog.ogenki.io/fr/tags/gitops/","section":"tags","tags":null,"title":"gitops"},{"body":"La cible de cette documentation est de pouvoir cr√©er et g√©rer un cluster GKE en utilisant Crossplane.\nCrossplane exploite les principes de base de Kubernetes afin de fournir des ressources cloud et bien plus encore: une ** approche d√©clarative ** avec ** Drift Detections ** et ** r√©conciliations ** Utilisation de boucles de contr√¥le: Exploseding_head:.En d'autres termes, nous d√©clarons les ressources cloud que nous voulons et Crossplane garantit que l'√©tat cible correspond √† celui appliqu√© via l'API Kubernetes.\nVoici les √©tapes que nous suivrons afin d'obtenir un cluster Kubernetes pour le d√©veloppement et les cas d'utilisation des exp√©rimentations.\n\u0026#x1f433; Create the local k3d cluster for Crossplane's control plane k3d est un cluster Kubernetes l√©ger qui exploite K3S qui s'ex√©cute dans notre ordinateur portable local. Il existe plusieurs mod√®les de d√©ploiement pour Crossplane, nous pourrions par exemple d√©ployer le plan de contr√¥le sur un cluster de gestion sur Kubernetes ou un plan de contr√¥le par cluster Kubernetes. Ici, j'ai choisi une m√©thode simple qui est bien pour un cas d'utilisation personnelle: une instance Kubernetes locale ** dans laquelle je vais d√©ployer Crossplane.\nLet's install k3d using asdf.\n1asdf plugin-add k3d 2 3asdf install k3d $(asdf latest k3d) 4* Downloading k3d release 5.4.1... 5k3d 5.4.1 installation was successful! Create a single node Kubernetes cluster.\n1k3d cluster create crossplane 2... 3INFO[0043] You can now use it like this: 4kubectl cluster-info 5 6k3d cluster list 7crossplane 1/1 0/0 true Check that the cluster is reachable using the kubectl CLI.\n1kubectl cluster-info 2Kubernetes control plane is running at https://0.0.0.0:40643 3CoreDNS is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 4Metrics-server is running at https://0.0.0.0:40643/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy We only need a single node for our Crossplane use case.\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3k3d-crossplane-server-0 Ready control-plane,master 26h v1.22.7+k3s1 \u0026#x2601;\u0026#xfe0f; Generate the Google Cloud service account Warning Store the downloaded crossplane.json credentials file in a safe place.\nCreate a service account\n1GCP_PROJECT=\u0026lt;your_project\u0026gt; 2gcloud iam service-accounts create crossplane --display-name \u0026#34;Crossplane\u0026#34; --project=${GCP_PROJECT} 3Created service account [crossplane]. Assign the proper permissions to the service account.\nCompute Network Admin Kubernetes Engine Admin Service Account User 1SA_EMAIL=$(gcloud iam service-accounts list --filter=\u0026#34;email ~ ^crossplane\u0026#34; --format=\u0026#39;value(email)\u0026#39;) 2 3gcloud projects add-iam-policy-binding \u0026#34;${GCP_PROJECT}\u0026#34; --member=serviceAccount:\u0026#34;${SA_EMAIL}\u0026#34; \\ 4--role=roles/container.admin --role=roles/compute.networkAdmin --role=roles/iam.serviceAccountUser 5Updated IAM policy for project [\u0026lt;project\u0026gt;]. 6bindings: 7- members: 8 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 9 role: roles/compute.networkAdmin 10- members: 11 - serviceAccount:crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com 12... 13version: 1 Download the service account key (json format)\n1gcloud iam service-accounts keys create crossplane.json --iam-account ${SA_EMAIL} 2created key [ea2eb9ce2939127xxxxxxxxxx] of type [json] as [crossplane.json] for [crossplane@\u0026lt;project\u0026gt;.iam.gserviceaccount.com] \u0026#x1f6a7; Deploy and configure Crossplane Now that we have a credentials file for Google Cloud, we can deploy the Crossplane operator and configure the provider-gcp provider.\nInfo Most of the following steps are issued from the official documentation\nWe'll first use Helm in order to install the operator\n1helm repo add crossplane-master https://charts.crossplane.io/master/ 2\u0026#34;crossplane-master\u0026#34; has been added to your repositories 3 4helm repo update 5...Successfully got an update from the \u0026#34;crossplane-master\u0026#34; chart repository 6 7helm install crossplane --namespace crossplane-system --create-namespace \\ 8--version 1.18.1 crossplane-stable/crossplane 9 10NAME: crossplane 11LAST DEPLOYED: Mon Jun 6 22:00:02 2022 12NAMESPACE: crossplane-system 13STATUS: deployed 14REVISION: 1 15TEST SUITE: None 16NOTES: 17Release: crossplane 18... Check that the operator is running properly.\n1kubectl get po -n crossplane-system 2NAME READY STATUS RESTARTS AGE 3crossplane-rbac-manager-54d96cd559-222hc 1/1 Running 0 3m37s 4crossplane-688c575476-lgklq 1/1 Running 0 3m37s Info All the files used for the upcoming steps are stored within this blog repository. So you should clone and change the current directory:\n1git clone https://github.com/Smana/smana.github.io.git 2 3cd smana.github.io/content/resources/crossplane_k3d Now we'll configure Crossplane so that it will be able to create and manage GCP resources. This is done by configuring the provider provider-gcp as follows.\nprovider.yaml\n1apiVersion: pkg.crossplane.io/v1 2kind: Provider 3metadata: 4 name: crossplane-provider-gcp 5spec: 6 package: crossplane/provider-gcp:v0.21.0 1kubectl apply -f provider.yaml 2provider.pkg.crossplane.io/crossplane-provider-gcp created 3 4kubectl get providers 5NAME INSTALLED HEALTHY PACKAGE AGE 6crossplane-provider-gcp True True crossplane/provider-gcp:v0.21.0 10s Create the Kubernetes secret that holds the GCP credentials file created above\n1kubectl create secret generic gcp-creds -n crossplane-system --from-file=creds=./crossplane.json 2secret/gcp-creds created Then we need to create a resource named ProviderConfig and reference the newly created secret.\nprovider-config.yaml\n1apiVersion: gcp.crossplane.io/v1beta1 2kind: ProviderConfig 3metadata: 4 name: default 5spec: 6 projectID: ${GCP_PROJECT} 7 credentials: 8 source: Secret 9 secretRef: 10 namespace: crossplane-system 11 name: gcp-creds 12 key: creds 1kubectl apply -f provider-config.yaml 2providerconfig.gcp.crossplane.io/default created Info If the serviceaccount has the proper permissions we can create resources in GCP. In order to learn about all the available resources and parameters we can have a look to the provider's API reference.\nThe first resource we'll create is the network that will host our Kubernetes cluster.\nnetwork.yaml\n1apiVersion: compute.gcp.crossplane.io/v1beta1 2kind: Network 3metadata: 4 name: dev-network 5 labels: 6 service: vpc 7 creation: crossplane 8spec: 9 forProvider: 10 autoCreateSubnetworks: false 11 description: \u0026#34;Network used for experimentations and POCs\u0026#34; 12 routingConfig: 13 routingMode: REGIONAL 1kubectl get network 2NAME READY SYNCED 3dev-network True True You can even get more details by describing this resource. For instance if something fails you would see the message returned by the Cloud provider in the events.\n1kubectl describe network dev-network | grep -A 20 \u0026#39;^Status:\u0026#39; 2Status: 3 At Provider: 4 Creation Timestamp: 2022-06-28T09:45:30.703-07:00 5 Id: 3005424280727359173 6 Self Link: https://www.googleapis.com/compute/v1/projects/${GCP_PROJECT}/global/networks/dev-network 7 Conditions: 8 Last Transition Time: 2022-06-28T16:45:31Z 9 Reason: Available 10 Status: True 11 Type: Ready 12 Last Transition Time: 2022-06-30T16:36:59Z 13 Reason: ReconcileSuccess 14 Status: True 15 Type: Synced \u0026#x1f680; Create a GKE cluster Everything is ready so that we can create our GKE cluster. Applying the file cluster.yaml will create a cluster and attach a node group to it.\ncluster.yaml\n1--- 2apiVersion: container.gcp.crossplane.io/v1beta2 3kind: Cluster 4metadata: 5 name: dev-cluster 6spec: 7 forProvider: 8 description: \u0026#34;Kubernetes cluster for experimentations and POCs\u0026#34; 9 initialClusterVersion: \u0026#34;1.24\u0026#34; 10 releaseChannel: 11 channel: \u0026#34;RAPID\u0026#34; 12 location: europe-west9-a 13 addonsConfig: 14 gcePersistentDiskCsiDriverConfig: 15 enabled: true 16 networkPolicyConfig: 17 disabled: false 18 networkRef: 19 name: dev-network 20 ipAllocationPolicy: 21 createSubnetwork: true 22 useIpAliases: true 23 defaultMaxPodsConstraint: 24 maxPodsPerNode: 110 25 networkPolicy: 26 enabled: false 27 writeConnectionSecretToRef: 28 namespace: default 29 name: gke-conn 30--- 31apiVersion: container.gcp.crossplane.io/v1beta1 32kind: NodePool 33metadata: 34 name: main-np 35spec: 36 forProvider: 37 initialNodeCount: 1 38 autoscaling: 39 autoprovisioned: false 40 enabled: true 41 maxNodeCount: 4 42 minNodeCount: 1 43 clusterRef: 44 name: dev-cluster 45 config: 46 machineType: n2-standard-2 47 diskSizeGb: 120 48 diskType: pd-standard 49 imageType: cos_containerd 50 preemptible: true 51 labels: 52 environment: dev 53 managed-by: crossplane 54 oauthScopes: 55 - \u0026#34;https://www.googleapis.com/auth/devstorage.read_only\u0026#34; 56 - \u0026#34;https://www.googleapis.com/auth/logging.write\u0026#34; 57 - \u0026#34;https://www.googleapis.com/auth/monitoring\u0026#34; 58 - \u0026#34;https://www.googleapis.com/auth/servicecontrol\u0026#34; 59 - \u0026#34;https://www.googleapis.com/auth/service.management.readonly\u0026#34; 60 - \u0026#34;https://www.googleapis.com/auth/trace.append\u0026#34; 61 metadata: 62 disable-legacy-endpoints: \u0026#34;true\u0026#34; 63 shieldedInstanceConfig: 64 enableIntegrityMonitoring: true 65 enableSecureBoot: true 66 management: 67 autoRepair: true 68 autoUpgrade: true 69 maxPodsConstraint: 70 maxPodsPerNode: 60 71 locations: 72 - \u0026#34;europe-west9-a\u0026#34; 1kubectl apply -f cluster.yaml 2cluster.container.gcp.crossplane.io/dev-cluster created 3nodepool.container.gcp.crossplane.io/main-np created Note that it takes around 10 minutes for the Kubernetes API and the nodes to be available. The STATE will transition from PROVISIONING to RUNNING and when a change is being applied the cluster status is RECONCILING\n1watch \u0026#39;kubectl get cluster,nodepool\u0026#39; 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3cluster.container.gcp.crossplane.io/dev-cluster False True PROVISIONING 34.155.122.6 europe-west9-a 3m15s 4 5NAME READY SYNCED STATE CLUSTER-REF AGE 6nodepool.container.gcp.crossplane.io/main-np False False dev-cluster 3m15s When the column READY switches to True you can download the cluster's credentials.\n1kubectl get cluster 2NAME READY SYNCED STATE ENDPOINT LOCATION AGE 3dev-cluster True True RECONCILING 34.42.42.42 europe-west9-a 6m23s 4 5gcloud container clusters get-credentials dev-cluster --zone europe-west9-a --project ${GCP_PROJECT} 6Fetching cluster endpoint and auth data. 7kubeconfig entry generated for dev-cluster. For better readability you may want to rename the context id for the newly created cluster\n1kubectl config rename-context gke_${GCP_PROJECT}_europe-west9-a_dev-cluster dev-cluster 2Context \u0026#34;gke_${GCP_PROJECT}_europe-west9-a_dev-cluster\u0026#34; renamed to \u0026#34;dev-cluster\u0026#34;. 3 4kubectl config get-contexts 5CURRENT NAME CLUSTER AUTHINFO NAMESPACE 6* dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster gke_cloud-native-computing-paris_europe-west9-a_dev-cluster 7 k3d-crossplane k3d-crossplane admin@k3d-crossplane Check that you can call our brand new GKE API\n1kubectl get nodes 2NAME STATUS ROLES AGE VERSION 3gke-dev-cluster-main-np-d0d978f9-5fc0 Ready \u0026lt;none\u0026gt; 10m v1.24.1-gke.1400 That's great \u0026#x1f389; we know have a GKE cluster up and running.\nüí≠ final thoughts I've been using Crossplane for a few months now in a production environment.\nEven if I'm conviced about the declarative approach using the Kubernetes API, we decided to move with caution with it. It clearly doesn't have Terraform's community and maturity. We're still declaring our resources using the deletionPolicy: Orphan so that even if something goes wrong on the controller side the resource won't be deleted.\nFurthermore we limited to a specific list of usual AWS resources requested by our developers. Nevertheless our target has always been to empower developers and we had really positive feedback from them. That's the best indicator for us. As the project matures, we'll move more and more resources from Terraform to Crossplane.\nIMHO the key success of Crossplane depends on the providers maintenance and evolution. The Cloud providers interest and involvement is really important.\nIn our next article we'll see how to use a GitOps engine to run all the above steps.\n","link":"https://blog.ogenki.io/fr/post/crossplane_k3d/","section":"post","tags":["kubernetes","infrastructure"],"title":"Mon cluster Kubernetes (GKE) avec `Crossplane`"},{"body":"Afin d'installer des binaires et de pouvoir passer d'une version √† une autre, j'aime utiliser asdf.\n\u0026#x1f4e5; Installation L'installation recommand√©e consiste √† utiliser Git comme suit\n1git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.10.0 Il y a quelques √©tapes suppl√©mentaires qui d√©pendent de votre shell. Voici celles que j'utilise pour bash:\n1. $HOME/.asdf/asdf.sh Vous voudrez probablement configurer la completion du shell comme suit\n1. $HOME/.asdf/completions/asdf.bash \u0026#x1f680; Prenons un exemple Listons tous les plugins disponibles pour trouver k3d\n1asdf plugin-list-all | grep k3d 2k3d https://github.com/spencergilbert/asdf-k3d.git Installons k3d\n1asdf plugin-add k3d V√©rifier les versions disponibles\n1asdf list-all k3d| tail -n 3 25.4.0-dev.3 35.4.0 45.4.1 Nous installerons la derni√®re version\n1asdf install k3d latest 2* Downloading k3d release 5.4.1... 3k3d 5.4.1 installation was successful! Enfin, nous pouvons passer d'une version √† une autre. Nous pouvons d√©finir une version \u0026quot;globale\u0026quot; qui serait utilis√©e sur tous les r√©pertoires.\n1asdf global k3d 5.4.1 ou utilisez une version locale en fonction du r√©pertoire actuel\n1cd /tmp 2asdf local k3d 5.4.1 3 4asdf current k3d 5k3d 5.4.1 /tmp/.tool-versions \u0026#x1f9f9; Faire le m√©nage D√©sinstaller une version donn√©e\n1asdf uninstall k3d 5.4.1 Retirer un plugin\n1asdf plugin remove k3d ","link":"https://blog.ogenki.io/fr/post/asdf/asdf/","section":"post","tags":["tooling","local"],"title":"G√©rer les versions d'outils avec `asdf`"},{"body":"","link":"https://blog.ogenki.io/fr/tags/local/","section":"tags","tags":null,"title":"local"},{"body":"","link":"https://blog.ogenki.io/fr/tags/tooling/","section":"tags","tags":null,"title":"tooling"},{"body":"","link":"https://blog.ogenki.io/fr/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://blog.ogenki.io/fr/tags/index/","section":"tags","tags":null,"title":"index"},{"body":"","link":"https://blog.ogenki.io/fr/series/","section":"series","tags":null,"title":"Series"}]